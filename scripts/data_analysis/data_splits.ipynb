{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_folder = \"../../data/en/acc_split\"\n",
    "train_folder = \"../../data/en/train_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_results = dict()\n",
    "for file in os.listdir(acc_folder):\n",
    "    acc = os.path.splitext(file)[0]\n",
    "    data = json.load(open(os.path.join(acc_folder, file)))\n",
    "    n_samples, dur = 0, 0\n",
    "    for obj in data:\n",
    "        n_samples += 1\n",
    "        dur += obj[\"duration\"]\n",
    "    acc_results[acc] = {\n",
    "        \"n_samples\": n_samples,\n",
    "        \"dur\": round(dur / 3600, 2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'au': {'dur': 71.39, 'n_samples': 51108},\n",
      " 'ca': {'dur': 85.63, 'n_samples': 59342},\n",
      " 'de': {'dur': 79.02, 'n_samples': 40897},\n",
      " 'hk': {'dur': 5.78, 'n_samples': 4260},\n",
      " 'ie': {'dur': 12.98, 'n_samples': 9461},\n",
      " 'in': {'dur': 148.06, 'n_samples': 99613},\n",
      " 'ni': {'dur': 7.96, 'n_samples': 5968},\n",
      " 'nz': {'dur': 15.75, 'n_samples': 11877},\n",
      " 'ph': {'dur': 7.36, 'n_samples': 5105},\n",
      " 'sc': {'dur': 24.46, 'n_samples': 15474},\n",
      " 'sg': {'dur': 4.71, 'n_samples': 3365},\n",
      " 'uk': {'dur': 178.34, 'n_samples': 134126},\n",
      " 'us': {'dur': 520.61, 'n_samples': 382626},\n",
      " 'za': {'dur': 11.58, 'n_samples': 8374}}\n"
     ]
    }
   ],
   "source": [
    "pprint(acc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = dict()\n",
    "for file in os.listdir(train_folder):\n",
    "    prepend, acc = os.path.splitext(file)[0].split(\"_\")\n",
    "    if acc not in train_results:\n",
    "        train_results[acc] = dict()\n",
    "    dur, n_samples = 0, 0\n",
    "    fname = os.path.join(train_folder, file)\n",
    "    try:\n",
    "        for line in open(fname):\n",
    "            n_samples += 1\n",
    "            dur += json.loads(line)[\"duration\"]\n",
    "        train_results\n",
    "        train_results[acc][prepend] = {\n",
    "            \"n_samples\": n_samples,\n",
    "            \"dur\": round(dur / 3600, 2)\n",
    "        }\n",
    "    except Exception:  # file does not exist\n",
    "        print(f\"{fname} does not exist\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'None': {'test': {'dur': 0.0, 'n_samples': 0}},\n",
      " 'au': {'test': {'dur': 14.36, 'n_samples': 10222},\n",
      "        'train': {'dur': 57.03, 'n_samples': 40886}},\n",
      " 'ca': {'test': {'dur': 17.1, 'n_samples': 11868},\n",
      "        'train': {'dur': 68.53, 'n_samples': 47474}},\n",
      " 'de': {'test': {'dur': 15.83, 'n_samples': 8179},\n",
      "        'train': {'dur': 63.19, 'n_samples': 32718}},\n",
      " 'hk': {'test': {'dur': 5.78, 'n_samples': 4260}},\n",
      " 'ie': {'test': {'dur': 12.98, 'n_samples': 9461}},\n",
      " 'in': {'test': {'dur': 29.57, 'n_samples': 19923},\n",
      "        'train': {'dur': 118.49, 'n_samples': 79690}},\n",
      " 'ni': {'test': {'dur': 7.96, 'n_samples': 5968}},\n",
      " 'nz': {'test': {'dur': 15.75, 'n_samples': 11877}},\n",
      " 'ph': {'test': {'dur': 7.36, 'n_samples': 5105}},\n",
      " 'sc': {'test': {'dur': 24.46, 'n_samples': 15474}},\n",
      " 'sg': {'test': {'dur': 4.71, 'n_samples': 3365}},\n",
      " 'uk': {'test': {'dur': 35.62, 'n_samples': 26825},\n",
      "        'train': {'dur': 142.72, 'n_samples': 107301}},\n",
      " 'us': {'test': {'dur': 104.17, 'n_samples': 76525},\n",
      "        'train': {'dur': 416.45, 'n_samples': 306101}},\n",
      " 'za': {'test': {'dur': 11.58, 'n_samples': 8374}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uk 178.3 0.15\n",
      "None 0.0 0.0\n",
      "ca 85.6 0.07\n",
      "au 71.4 0.06\n",
      "in 148.1 0.13\n",
      "sg 4.7 0.0\n",
      "sc 24.5 0.02\n",
      "ni 8.0 0.01\n",
      "us 520.6 0.44\n",
      "ph 7.4 0.01\n",
      "za 11.6 0.01\n",
      "nz 15.8 0.01\n",
      "ie 13.0 0.01\n",
      "de 79.0 0.07\n",
      "hk 5.8 0.0\n"
     ]
    }
   ],
   "source": [
    "# compute the distribution of data across accents\n",
    "hours = [0 for _ in range(len(train_results))]\n",
    "for i, acc in enumerate(train_results):\n",
    "    for file in train_results[acc].values():\n",
    "        hours[i] += file[\"dur\"]\n",
    "\n",
    "for i, acc in enumerate(train_results):\n",
    "    print(acc, round(hours[i], 1), round(hours[i] / sum(hours), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accented: 449.96\n",
      "US: 416.45\n"
     ]
    }
   ],
   "source": [
    "# sum all train data that is not US, and compare to the US data\n",
    "hours = 0\n",
    "for acc in train_results:\n",
    "    if \"train\" in train_results[acc] and acc != \"us\":\n",
    "        hours += train_results[acc][\"train\"][\"dur\"]\n",
    "print(\"accented: \" + str(hours))\n",
    "print(\"US: \" + str(train_results[\"us\"][\"train\"][\"dur\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accented: 203.06000000000003\n",
      "US: 104.17\n"
     ]
    }
   ],
   "source": [
    "# sum all train data that is not US, and compare to the US data\n",
    "hours = 0\n",
    "for acc in train_results:\n",
    "    if \"test\" in train_results[acc] and acc != \"us\":\n",
    "        hours += train_results[acc][\"test\"][\"dur\"]\n",
    "print(\"accented: \" + str(hours))\n",
    "print(\"US: \" + str(train_results[\"us\"][\"test\"][\"dur\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57485ebec7cc786341bb1b2b19abc916613018ee2fed0a584a0df93216af615b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
