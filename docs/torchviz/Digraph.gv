digraph {
	graph [size="1432.95,1432.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140597480547008 [label="
 ()" fillcolor=darkolivegreen1]
	140597480504432 [label="AddBackward0
------------
alpha: 1"]
	140597480504096 -> 140597480504432
	140597480504096 -> 140597367774080 [dir=none]
	140597367774080 [label="other
 ()" fillcolor=orange]
	140597480504096 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480502656 -> 140597480504096
	140597480502656 [label="DivBackward1
------------
other: 1"]
	140597480502944 -> 140597480502656
	140597480502944 [label=NegBackward0]
	140597480503664 -> 140597480502944
	140597480503664 [label="SumBackward0
------------------
self_sizes: (1, 3)"]
	140597480503712 -> 140597480503664
	140597480503712 -> 140597480031872 [dir=none]
	140597480031872 [label="other
 (1, 3)" fillcolor=orange]
	140597480503712 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480503520 -> 140597480503712
	140597480503520 -> 140597367774400 [dir=none]
	140597367774400 [label="result
 (1, 3)" fillcolor=orange]
	140597480503520 [label="LogSoftmaxBackward0
----------------------
dim   :              1
result: [saved tensor]"]
	140597480504000 -> 140597480503520
	140597480504000 [label="MeanBackward1
-----------------------
dim       :        (1,)
keepdim   :       False
self_sizes: (1, 126, 3)"]
	140597480504960 -> 140597480504000
	140597480504960 -> 140597367774320 [dir=none]
	140597367774320 [label="result
 (1, 126, 3)" fillcolor=orange]
	140597480504960 [label="SoftmaxBackward0
----------------------
dim   :              1
result: [saved tensor]"]
	140597480505056 -> 140597480504960
	140597480505056 [label="ViewBackward0
--------------------
self_sizes: (126, 3)"]
	140597480505008 -> 140597480505056
	140597480505008 -> 140597367774240 [dir=none]
	140597367774240 [label="mat1
 (126, 256)" fillcolor=orange]
	140597480505008 -> 140597367774560 [dir=none]
	140597367774560 [label="mat2
 (256, 3)" fillcolor=orange]
	140597480505008 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :       (256, 3)
mat2_strides:       (1, 256)"]
	140597480504480 -> 140597480505008
	140597776162064 [label="acc_classifier.fc3.bias
 (3)" fillcolor=lightblue]
	140597776162064 -> 140597480504480
	140597480504480 [label=AccumulateGrad]
	140597480504576 -> 140597480505008
	140597480504576 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 256)"]
	140597480504624 -> 140597480504576
	140597480504624 -> 140597367774160 [dir=none]
	140597367774160 [label="result
 (1, 126, 256)" fillcolor=orange]
	140597480504624 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140597480504144 -> 140597480504624
	140597480504144 [label="ViewBackward0
----------------------
self_sizes: (126, 256)"]
	140597480503280 -> 140597480504144
	140597480503280 -> 140597367774720 [dir=none]
	140597367774720 [label="mat1
 (126, 1024)" fillcolor=orange]
	140597480503280 -> 140597367774880 [dir=none]
	140597367774880 [label="mat2
 (1024, 256)" fillcolor=orange]
	140597480503280 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 1024)
mat1_strides:      (1024, 1)
mat2        : [saved tensor]
mat2_sizes  :    (1024, 256)
mat2_strides:      (1, 1024)"]
	140597480504336 -> 140597480503280
	140597776162304 [label="acc_classifier.fc2.bias
 (256)" fillcolor=lightblue]
	140597776162304 -> 140597480504336
	140597480504336 [label=AccumulateGrad]
	140597480504192 -> 140597480503280
	140597480504192 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 1024)"]
	140597480503472 -> 140597480504192
	140597480503472 -> 140597367775120 [dir=none]
	140597367775120 [label="result
 (1, 126, 1024)" fillcolor=orange]
	140597480503472 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140597480503904 -> 140597480503472
	140597480503904 [label="ViewBackward0
-----------------------
self_sizes: (126, 1024)"]
	140597480503952 -> 140597480503904
	140597480503952 -> 140597367774640 [dir=none]
	140597367774640 [label="mat1
 (126, 512)" fillcolor=orange]
	140597480503952 -> 140597367774480 [dir=none]
	140597367774480 [label="mat2
 (512, 1024)" fillcolor=orange]
	140597480503952 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 1024)
mat2_strides:       (1, 512)"]
	140597480374720 -> 140597480503952
	140597776162144 [label="acc_classifier.fc1.bias
 (1024)" fillcolor=lightblue]
	140597776162144 -> 140597480374720
	140597480374720 [label=AccumulateGrad]
	140597480374672 -> 140597480503952
	140597480374672 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597480376832 -> 140597480374672
	140597480376832 -> 140597520168496 [dir=none]
	140597520168496 [label="bias
 (512)" fillcolor=orange]
	140597480376832 -> 140597480544112 [dir=none]
	140597480544112 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597480376832 -> 140597367774960 [dir=none]
	140597367774960 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597480376832 -> 140597367774800 [dir=none]
	140597367774800 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597480376832 -> 140597520168416 [dir=none]
	140597520168416 [label="weight
 (512)" fillcolor=orange]
	140597480376832 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480376592 -> 140597480376832
	140597480376592 [label="AddBackward0
------------
alpha: 1"]
	140597480376400 -> 140597480376592
	140597480376400 [label="AddBackward0
------------
alpha: 1"]
	140597480377840 -> 140597480376400
	140597480377840 [label="AddBackward0
------------
alpha: 1"]
	140597480377888 -> 140597480377840
	140597480377888 [label="AddBackward0
------------
alpha: 1"]
	140597480377984 -> 140597480377888
	140597480377984 -> 140597566765392 [dir=none]
	140597566765392 [label="bias
 (512)" fillcolor=orange]
	140597480377984 -> 140597480541936 [dir=none]
	140597480541936 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597480377984 -> 140597367775040 [dir=none]
	140597367775040 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597480377984 -> 140597367832640 [dir=none]
	140597367832640 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597480377984 -> 140597566765312 [dir=none]
	140597566765312 [label="weight
 (512)" fillcolor=orange]
	140597480377984 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480377648 -> 140597480377984
	140597480377648 [label="AddBackward0
------------
alpha: 1"]
	140597480026608 -> 140597480377648
	140597480026608 [label="AddBackward0
------------
alpha: 1"]
	140597480026368 -> 140597480026608
	140597480026368 [label="AddBackward0
------------
alpha: 1"]
	140597480026320 -> 140597480026368
	140597480026320 [label="AddBackward0
------------
alpha: 1"]
	140597480028528 -> 140597480026320
	140597480028528 -> 140597566639216 [dir=none]
	140597566639216 [label="bias
 (512)" fillcolor=orange]
	140597480028528 -> 140597480539856 [dir=none]
	140597480539856 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597480028528 -> 140597367832880 [dir=none]
	140597367832880 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597480028528 -> 140597367832720 [dir=none]
	140597367832720 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597480028528 -> 140597566639136 [dir=none]
	140597566639136 [label="weight
 (512)" fillcolor=orange]
	140597480028528 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480026560 -> 140597480028528
	140597480026560 [label="AddBackward0
------------
alpha: 1"]
	140597480029488 -> 140597480026560
	140597480029488 [label="AddBackward0
------------
alpha: 1"]
	140597480028192 -> 140597480029488
	140597480028192 [label="AddBackward0
------------
alpha: 1"]
	140597480028240 -> 140597480028192
	140597480028240 [label="AddBackward0
------------
alpha: 1"]
	140597480029104 -> 140597480028240
	140597480029104 -> 140597854179216 [dir=none]
	140597854179216 [label="bias
 (512)" fillcolor=orange]
	140597480029104 -> 140597480537680 [dir=none]
	140597480537680 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597480029104 -> 140597367833040 [dir=none]
	140597367833040 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597480029104 -> 140597367832800 [dir=none]
	140597367832800 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597480029104 -> 140597854179136 [dir=none]
	140597854179136 [label="weight
 (512)" fillcolor=orange]
	140597480029104 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480028720 -> 140597480029104
	140597480028720 [label="AddBackward0
------------
alpha: 1"]
	140597480028960 -> 140597480028720
	140597480028960 [label="AddBackward0
------------
alpha: 1"]
	140597480028672 -> 140597480028960
	140597480028672 [label="AddBackward0
------------
alpha: 1"]
	140597480029632 -> 140597480028672
	140597480029632 [label="AddBackward0
------------
alpha: 1"]
	140597480054592 -> 140597480029632
	140597480054592 -> 140597854176016 [dir=none]
	140597854176016 [label="bias
 (512)" fillcolor=orange]
	140597480054592 -> 140597480535600 [dir=none]
	140597480535600 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597480054592 -> 140597367833200 [dir=none]
	140597367833200 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597480054592 -> 140597367832960 [dir=none]
	140597367832960 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597480054592 -> 140597854175936 [dir=none]
	140597854175936 [label="weight
 (512)" fillcolor=orange]
	140597480054592 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480054256 -> 140597480054592
	140597480054256 [label="AddBackward0
------------
alpha: 1"]
	140597480054016 -> 140597480054256
	140597480054016 [label="AddBackward0
------------
alpha: 1"]
	140597480053728 -> 140597480054016
	140597480053728 [label="AddBackward0
------------
alpha: 1"]
	140597368046304 -> 140597480053728
	140597368046304 [label="AddBackward0
------------
alpha: 1"]
	140597368048608 -> 140597368046304
	140597368048608 -> 140597854053936 [dir=none]
	140597854053936 [label="bias
 (512)" fillcolor=orange]
	140597368048608 -> 140597480533424 [dir=none]
	140597480533424 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597368048608 -> 140597367833360 [dir=none]
	140597367833360 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597368048608 -> 140597367833120 [dir=none]
	140597367833120 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597368048608 -> 140597854053856 [dir=none]
	140597854053856 [label="weight
 (512)" fillcolor=orange]
	140597368048608 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597368049184 -> 140597368048608
	140597368049184 [label="AddBackward0
------------
alpha: 1"]
	140597368048800 -> 140597368049184
	140597368048800 [label="AddBackward0
------------
alpha: 1"]
	140597368048944 -> 140597368048800
	140597368048944 [label="AddBackward0
------------
alpha: 1"]
	140597368049328 -> 140597368048944
	140597368049328 [label="AddBackward0
------------
alpha: 1"]
	140597368049472 -> 140597368049328
	140597368049472 -> 140597855181056 [dir=none]
	140597855181056 [label="bias
 (512)" fillcolor=orange]
	140597368049472 -> 140597480531344 [dir=none]
	140597480531344 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597368049472 -> 140597367833520 [dir=none]
	140597367833520 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597368049472 -> 140597367833280 [dir=none]
	140597367833280 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597368049472 -> 140597855180976 [dir=none]
	140597855180976 [label="weight
 (512)" fillcolor=orange]
	140597368049472 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597368049616 -> 140597368049472
	140597368049616 [label="AddBackward0
------------
alpha: 1"]
	140597367853264 -> 140597368049616
	140597367853264 [label="AddBackward0
------------
alpha: 1"]
	140597367853408 -> 140597367853264
	140597367853408 [label="AddBackward0
------------
alpha: 1"]
	140597367853552 -> 140597367853408
	140597367853552 [label="AddBackward0
------------
alpha: 1"]
	140597367853696 -> 140597367853552
	140597367853696 -> 140597854972960 [dir=none]
	140597854972960 [label="bias
 (512)" fillcolor=orange]
	140597367853696 -> 140597480529168 [dir=none]
	140597480529168 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367853696 -> 140597367833680 [dir=none]
	140597367833680 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367853696 -> 140597367833440 [dir=none]
	140597367833440 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367853696 -> 140597854972880 [dir=none]
	140597854972880 [label="weight
 (512)" fillcolor=orange]
	140597367853696 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367853840 -> 140597367853696
	140597367853840 [label="AddBackward0
------------
alpha: 1"]
	140597367854032 -> 140597367853840
	140597367854032 [label="AddBackward0
------------
alpha: 1"]
	140597367854176 -> 140597367854032
	140597367854176 [label="AddBackward0
------------
alpha: 1"]
	140597367854320 -> 140597367854176
	140597367854320 [label="AddBackward0
------------
alpha: 1"]
	140597367854464 -> 140597367854320
	140597367854464 -> 140597854490432 [dir=none]
	140597854490432 [label="bias
 (512)" fillcolor=orange]
	140597367854464 -> 140597480527088 [dir=none]
	140597480527088 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367854464 -> 140597367833840 [dir=none]
	140597367833840 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367854464 -> 140597367833600 [dir=none]
	140597367833600 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367854464 -> 140597854490352 [dir=none]
	140597854490352 [label="weight
 (512)" fillcolor=orange]
	140597367854464 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367854608 -> 140597367854464
	140597367854608 [label="AddBackward0
------------
alpha: 1"]
	140597367854800 -> 140597367854608
	140597367854800 [label="AddBackward0
------------
alpha: 1"]
	140597367854944 -> 140597367854800
	140597367854944 [label="AddBackward0
------------
alpha: 1"]
	140597367855088 -> 140597367854944
	140597367855088 [label="AddBackward0
------------
alpha: 1"]
	140597367855232 -> 140597367855088
	140597367855232 -> 140597856970768 [dir=none]
	140597856970768 [label="bias
 (512)" fillcolor=orange]
	140597367855232 -> 140597480524912 [dir=none]
	140597480524912 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367855232 -> 140597367834000 [dir=none]
	140597367834000 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367855232 -> 140597367833760 [dir=none]
	140597367833760 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367855232 -> 140597856970688 [dir=none]
	140597856970688 [label="weight
 (512)" fillcolor=orange]
	140597367855232 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367855376 -> 140597367855232
	140597367855376 [label="AddBackward0
------------
alpha: 1"]
	140597367855568 -> 140597367855376
	140597367855568 [label="AddBackward0
------------
alpha: 1"]
	140597367855712 -> 140597367855568
	140597367855712 [label="AddBackward0
------------
alpha: 1"]
	140597367855856 -> 140597367855712
	140597367855856 [label="AddBackward0
------------
alpha: 1"]
	140597367856000 -> 140597367855856
	140597367856000 -> 140597854487552 [dir=none]
	140597854487552 [label="bias
 (512)" fillcolor=orange]
	140597367856000 -> 140597480522832 [dir=none]
	140597480522832 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367856000 -> 140597367834160 [dir=none]
	140597367834160 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367856000 -> 140597367833920 [dir=none]
	140597367833920 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367856000 -> 140597854487472 [dir=none]
	140597854487472 [label="weight
 (512)" fillcolor=orange]
	140597367856000 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367856144 -> 140597367856000
	140597367856144 [label="AddBackward0
------------
alpha: 1"]
	140597367856336 -> 140597367856144
	140597367856336 [label="AddBackward0
------------
alpha: 1"]
	140597367856480 -> 140597367856336
	140597367856480 [label="AddBackward0
------------
alpha: 1"]
	140597367856624 -> 140597367856480
	140597367856624 [label="AddBackward0
------------
alpha: 1"]
	140597367856768 -> 140597367856624
	140597367856768 -> 140597855618848 [dir=none]
	140597855618848 [label="bias
 (512)" fillcolor=orange]
	140597367856768 -> 140597480520656 [dir=none]
	140597480520656 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367856768 -> 140597367834320 [dir=none]
	140597367834320 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367856768 -> 140597367834080 [dir=none]
	140597367834080 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367856768 -> 140597855618768 [dir=none]
	140597855618768 [label="weight
 (512)" fillcolor=orange]
	140597367856768 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367856912 -> 140597367856768
	140597367856912 [label="AddBackward0
------------
alpha: 1"]
	140597367857104 -> 140597367856912
	140597367857104 [label="AddBackward0
------------
alpha: 1"]
	140597367865504 -> 140597367857104
	140597367865504 [label="AddBackward0
------------
alpha: 1"]
	140597367865648 -> 140597367865504
	140597367865648 [label="AddBackward0
------------
alpha: 1"]
	140597367865792 -> 140597367865648
	140597367865792 -> 140597855865408 [dir=none]
	140597855865408 [label="bias
 (512)" fillcolor=orange]
	140597367865792 -> 140597480518576 [dir=none]
	140597480518576 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367865792 -> 140597367834480 [dir=none]
	140597367834480 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367865792 -> 140597367834240 [dir=none]
	140597367834240 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367865792 -> 140597855865328 [dir=none]
	140597855865328 [label="weight
 (512)" fillcolor=orange]
	140597367865792 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367865936 -> 140597367865792
	140597367865936 [label="AddBackward0
------------
alpha: 1"]
	140597367866128 -> 140597367865936
	140597367866128 [label="AddBackward0
------------
alpha: 1"]
	140597367866272 -> 140597367866128
	140597367866272 [label="AddBackward0
------------
alpha: 1"]
	140597367866416 -> 140597367866272
	140597367866416 [label="AddBackward0
------------
alpha: 1"]
	140597367866560 -> 140597367866416
	140597367866560 -> 140597856787808 [dir=none]
	140597856787808 [label="bias
 (512)" fillcolor=orange]
	140597367866560 -> 140597480516400 [dir=none]
	140597480516400 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367866560 -> 140597367834640 [dir=none]
	140597367834640 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367866560 -> 140597367834400 [dir=none]
	140597367834400 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367866560 -> 140597856787728 [dir=none]
	140597856787728 [label="weight
 (512)" fillcolor=orange]
	140597367866560 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367866752 -> 140597367866560
	140597367866752 [label="AddBackward0
------------
alpha: 1"]
	140597367866944 -> 140597367866752
	140597367866944 [label="AddBackward0
------------
alpha: 1"]
	140597367867088 -> 140597367866944
	140597367867088 [label="AddBackward0
------------
alpha: 1"]
	140597367867232 -> 140597367867088
	140597367867232 [label="AddBackward0
------------
alpha: 1"]
	140597367867376 -> 140597367867232
	140597367867376 -> 140597856784608 [dir=none]
	140597856784608 [label="bias
 (512)" fillcolor=orange]
	140597367867376 -> 140597480514320 [dir=none]
	140597480514320 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367867376 -> 140597367834800 [dir=none]
	140597367834800 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367867376 -> 140597367834560 [dir=none]
	140597367834560 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367867376 -> 140597856784528 [dir=none]
	140597856784528 [label="weight
 (512)" fillcolor=orange]
	140597367867376 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367867520 -> 140597367867376
	140597367867520 [label="AddBackward0
------------
alpha: 1"]
	140597367867712 -> 140597367867520
	140597367867712 [label="AddBackward0
------------
alpha: 1"]
	140597367867856 -> 140597367867712
	140597367867856 [label="AddBackward0
------------
alpha: 1"]
	140597367868000 -> 140597367867856
	140597367868000 [label="AddBackward0
------------
alpha: 1"]
	140597367868144 -> 140597367868000
	140597367868144 -> 140597367834960 [dir=none]
	140597367834960 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367868144 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367868288 -> 140597367868144
	140597367868288 -> 140597367835040 [dir=none]
	140597367835040 [label="other
 ()" fillcolor=orange]
	140597367868288 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367868384 -> 140597367868288
	140597367868384 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367868480 -> 140597367868384
	140597367868480 -> 140597367834720 [dir=none]
	140597367834720 [label="mat1
 (126, 10240)" fillcolor=orange]
	140597367868480 -> 140597367834880 [dir=none]
	140597367834880 [label="mat2
 (10240, 512)" fillcolor=orange]
	140597367868480 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :   (126, 10240)
mat1_strides:     (10240, 1)
mat2        : [saved tensor]
mat2_sizes  :   (10240, 512)
mat2_strides:     (1, 10240)"]
	140597367868576 -> 140597367868480
	140597857543808 [label="ct.encoder.pre_encode.out.bias
 (512)" fillcolor=lightblue]
	140597857543808 -> 140597367868576
	140597367868576 [label=AccumulateGrad]
	140597367868528 -> 140597367868480
	140597367868528 [label="ViewBackward0
---------------------------
self_sizes: (1, 126, 10240)"]
	140597367868672 -> 140597367868528
	140597367868672 [label="UnsafeViewBackward0
-----------------------------
self_sizes: (1, 126, 512, 20)"]
	140597367868864 -> 140597367868672
	140597367868864 [label=CloneBackward0]
	140597367868960 -> 140597367868864
	140597367868960 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367869056 -> 140597367868960
	140597367869056 -> 140597367835520 [dir=none]
	140597367835520 [label="result
 (1, 512, 126, 20)" fillcolor=orange]
	140597367869056 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140597367869152 -> 140597367869056
	140597367869152 -> 140597480454640 [dir=none]
	140597480454640 [label="input
 (1, 512, 251, 40)" fillcolor=orange]
	140597367869152 -> 140597857543328 [dir=none]
	140597857543328 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	140597367869152 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	140597367869248 -> 140597367869152
	140597367869248 -> 140597367835200 [dir=none]
	140597367835200 [label="result
 (1, 512, 251, 40)" fillcolor=orange]
	140597367869248 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140597367869392 -> 140597367869248
	140597367869392 -> 140597480454240 [dir=none]
	140597480454240 [label="input
 (1, 1, 501, 80)" fillcolor=orange]
	140597367869392 -> 140597857543168 [dir=none]
	140597857543168 [label="weight
 (512, 1, 3, 3)" fillcolor=orange]
	140597367869392 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	140597367885984 -> 140597367869392
	140597857543168 [label="ct.encoder.pre_encode.conv.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140597857543168 -> 140597367885984
	140597367885984 [label=AccumulateGrad]
	140597367885936 -> 140597367869392
	140597857543248 [label="ct.encoder.pre_encode.conv.0.bias
 (512)" fillcolor=lightblue]
	140597857543248 -> 140597367885936
	140597367885936 [label=AccumulateGrad]
	140597367869200 -> 140597367869152
	140597857543328 [label="ct.encoder.pre_encode.conv.2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140597857543328 -> 140597367869200
	140597367869200 [label=AccumulateGrad]
	140597367868768 -> 140597367869152
	140597857543408 [label="ct.encoder.pre_encode.conv.2.bias
 (512)" fillcolor=lightblue]
	140597857543408 -> 140597367868768
	140597367868768 [label=AccumulateGrad]
	140597367868192 -> 140597367868480
	140597367868192 [label=TBackward0]
	140597367868912 -> 140597367868192
	140597857543728 [label="ct.encoder.pre_encode.out.weight
 (512, 10240)" fillcolor=lightblue]
	140597857543728 -> 140597367868912
	140597367868912 [label=AccumulateGrad]
	140597367868096 -> 140597367868000
	140597367868096 -> 140597367835360 [dir=none]
	140597367835360 [label="other
 ()" fillcolor=orange]
	140597367868096 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367868432 -> 140597367868096
	140597367868432 -> 140597367835680 [dir=none]
	140597367835680 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367868432 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367868816 -> 140597367868432
	140597367868816 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367869008 -> 140597367868816
	140597367869008 -> 140597367835280 [dir=none]
	140597367835280 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367869008 -> 140597367835600 [dir=none]
	140597367835600 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367869008 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367869296 -> 140597367869008
	140597857543488 [label="ct.encoder.layers.0.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597857543488 -> 140597367869296
	140597367869296 [label=AccumulateGrad]
	140597367868720 -> 140597367869008
	140597367868720 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367885888 -> 140597367868720
	140597367885888 -> 140597367836080 [dir=none]
	140597367836080 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367885888 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367886176 -> 140597367885888
	140597367886176 -> 140597480454960 [dir=none]
	140597480454960 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367886176 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367886272 -> 140597367886176
	140597367886272 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367886368 -> 140597367886272
	140597367886368 -> 140597367835120 [dir=none]
	140597367835120 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367886368 -> 140597367835920 [dir=none]
	140597367835920 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367886368 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367886464 -> 140597367886368
	140597876034672 [label="ct.encoder.layers.0.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597876034672 -> 140597367886464
	140597367886464 [label=AccumulateGrad]
	140597367886416 -> 140597367886368
	140597367886416 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367886560 -> 140597367886416
	140597367886560 -> 140597875703056 [dir=none]
	140597875703056 [label="bias
 (512)" fillcolor=orange]
	140597367886560 -> 140597480454880 [dir=none]
	140597480454880 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367886560 -> 140597367835440 [dir=none]
	140597367835440 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367886560 -> 140597367836160 [dir=none]
	140597367836160 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367886560 -> 140597875702336 [dir=none]
	140597875702336 [label="weight
 (512)" fillcolor=orange]
	140597367886560 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367868144 -> 140597367886560
	140597367886752 -> 140597367886560
	140597875702336 [label="ct.encoder.layers.0.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597875702336 -> 140597367886752
	140597367886752 [label=AccumulateGrad]
	140597367886704 -> 140597367886560
	140597875703056 [label="ct.encoder.layers.0.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597875703056 -> 140597367886704
	140597367886704 [label=AccumulateGrad]
	140597367886080 -> 140597367886368
	140597367886080 [label=TBackward0]
	140597367886800 -> 140597367886080
	140597861309232 [label="ct.encoder.layers.0.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597861309232 -> 140597367886800
	140597367886800 [label=AccumulateGrad]
	140597367868240 -> 140597367869008
	140597367868240 [label=TBackward0]
	140597367886224 -> 140597367868240
	140597857543568 [label="ct.encoder.layers.0.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597857543568 -> 140597367886224
	140597367886224 [label=AccumulateGrad]
	140597367867952 -> 140597367867856
	140597367867952 -> 140597367836320 [dir=none]
	140597367836320 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367867952 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367868624 -> 140597367867952
	140597367868624 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367869344 -> 140597367868624
	140597367869344 -> 140597367836400 [dir=none]
	140597367836400 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367869344 -> 140597367835840 [dir=none]
	140597367835840 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367869344 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367868048 -> 140597367869344
	140597857545488 [label="ct.encoder.layers.0.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597857545488 -> 140597367868048
	140597367868048 [label=AccumulateGrad]
	140597367886512 -> 140597367869344
	140597367886512 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367886032 -> 140597367886512
	140597367886032 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367886608 -> 140597367886032
	140597367886608 [label=CloneBackward0]
	140597367886992 -> 140597367886608
	140597367886992 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367887088 -> 140597367886992
	140597367887088 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367887184 -> 140597367887088
	140597367887184 -> 140597367836560 [dir=none]
	140597367836560 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367887184 -> 140597367836480 [dir=none]
	140597367836480 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367887184 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367887280 -> 140597367887184
	140597367887280 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367887424 -> 140597367887280
	140597367887424 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367887520 -> 140597367887424
	140597367887520 -> 140597367836000 [dir=none]
	140597367836000 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367887520 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367887616 -> 140597367887520
	140597367887616 -> 140597480513840 [dir=none]
	140597480513840 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367887616 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367887712 -> 140597367887616
	140597367887712 -> 140597367835760 [dir=none]
	140597367835760 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367887712 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367887856 -> 140597367887712
	140597367887856 -> 140597480513840 [dir=none]
	140597480513840 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367887856 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367887952 -> 140597367887856
	140597367887952 -> 140597367836240 [dir=none]
	140597367836240 [label="other
 ()" fillcolor=orange]
	140597367887952 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367888048 -> 140597367887952
	140597367888048 [label="AddBackward0
------------
alpha: 1"]
	140597367888144 -> 140597367888048
	140597367888144 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367888288 -> 140597367888144
	140597367888288 -> 140597367914640 [dir=none]
	140597367914640 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367888288 -> 140597367914560 [dir=none]
	140597367914560 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367888288 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367888384 -> 140597367888288
	140597367888384 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367888528 -> 140597367888384
	140597367888528 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367888624 -> 140597367888528
	140597367888624 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367888720 -> 140597367888624
	140597367888720 [label="AddBackward0
------------
alpha: 1"]
	140597367888816 -> 140597367888720
	140597367888816 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367888960 -> 140597367888816
	140597367888960 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367889056 -> 140597367888960
	140597367889056 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367889152 -> 140597367889056
	140597367889152 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367889248 -> 140597367889152
	140597367889248 -> 140597367914880 [dir=none]
	140597367914880 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367889248 -> 140597367915040 [dir=none]
	140597367915040 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367889248 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367889344 -> 140597367889248
	140597857545008 [label="ct.encoder.layers.0.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597857545008 -> 140597367889344
	140597367889344 [label=AccumulateGrad]
	140597367889296 -> 140597367889248
	140597367889296 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367889440 -> 140597367889296
	140597367889440 -> 140597857544848 [dir=none]
	140597857544848 [label="bias
 (512)" fillcolor=orange]
	140597367889440 -> 140597480455200 [dir=none]
	140597480455200 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367889440 -> 140597367915120 [dir=none]
	140597367915120 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367889440 -> 140597367914720 [dir=none]
	140597367914720 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367889440 -> 140597857544768 [dir=none]
	140597857544768 [label="weight
 (512)" fillcolor=orange]
	140597367889440 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367868000 -> 140597367889440
	140597367889632 -> 140597367889440
	140597857544768 [label="ct.encoder.layers.0.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597857544768 -> 140597367889632
	140597367889632 [label=AccumulateGrad]
	140597367889584 -> 140597367889440
	140597857544848 [label="ct.encoder.layers.0.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597857544848 -> 140597367889584
	140597367889584 [label=AccumulateGrad]
	140597367888864 -> 140597367889248
	140597367888864 [label=TBackward0]
	140597367889680 -> 140597367888864
	140597857544928 [label="ct.encoder.layers.0.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597857544928 -> 140597367889680
	140597367889680 [label=AccumulateGrad]
	140597367888768 -> 140597367888720
	140597857545648 [label="ct.encoder.layers.0.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597857545648 -> 140597367888768
	140597367888768 [label=AccumulateGrad]
	140597367888336 -> 140597367888288
	140597367888336 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367888672 -> 140597367888336
	140597367888672 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367889104 -> 140597367888672
	140597367889104 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367889392 -> 140597367889104
	140597367889392 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367889536 -> 140597367889392
	140597367889536 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367889488 -> 140597367889536
	140597367889488 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367889872 -> 140597367889488
	140597367889872 -> 140597367915280 [dir=none]
	140597367915280 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367889872 -> 140597367914800 [dir=none]
	140597367914800 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367889872 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367888480 -> 140597367889872
	140597857545168 [label="ct.encoder.layers.0.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597857545168 -> 140597367888480
	140597367888480 [label=AccumulateGrad]
	140597367922752 -> 140597367889872
	140597367922752 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367889440 -> 140597367922752
	140597367922800 -> 140597367889872
	140597367922800 [label=TBackward0]
	140597367922992 -> 140597367922800
	140597857545088 [label="ct.encoder.layers.0.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597857545088 -> 140597367922992
	140597367922992 [label=AccumulateGrad]
	140597367888096 -> 140597367888048
	140597367888096 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597367888576 -> 140597367888096
	140597367888576 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367889200 -> 140597367888576
	140597367889200 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367889776 -> 140597367889200
	140597367889776 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367889824 -> 140597367889776
	140597367889824 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367923088 -> 140597367889824
	140597367923088 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367922896 -> 140597367923088
	140597367922896 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367923184 -> 140597367922896
	140597367923184 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367923280 -> 140597367923184
	140597367923280 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367923376 -> 140597367923280
	140597367923376 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367923472 -> 140597367923376
	140597367923472 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367923568 -> 140597367923472
	140597367923568 -> 140597367915840 [dir=none]
	140597367915840 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367923568 -> 140597367914960 [dir=none]
	140597367914960 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367923568 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367923664 -> 140597367923568
	140597367923664 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367923808 -> 140597367923664
	140597367923808 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367923904 -> 140597367923808
	140597367923904 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367924000 -> 140597367923904
	140597367924000 [label="AddBackward0
------------
alpha: 1"]
	140597367888816 -> 140597367924000
	140597367924096 -> 140597367924000
	140597857545728 [label="ct.encoder.layers.0.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597857545728 -> 140597367924096
	140597367924096 [label=AccumulateGrad]
	140597367923616 -> 140597367923568
	140597367923616 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367923952 -> 140597367923616
	140597367923952 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367924192 -> 140597367923952
	140597367924192 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367924240 -> 140597367924192
	140597367924240 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367924288 -> 140597367924240
	140597367924288 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597367924432 -> 140597367924288
	140597367924432 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597367924528 -> 140597367924432
	140597367924528 -> 140597367915360 [dir=none]
	140597367915360 [label="self
 (251, 512)" fillcolor=orange]
	140597367924528 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597367924624 -> 140597367924528
	140597367924624 [label=TBackward0]
	140597367924720 -> 140597367924624
	140597857545568 [label="ct.encoder.layers.0.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597857545568 -> 140597367924720
	140597367924720 [label=AccumulateGrad]
	140597367887232 -> 140597367887184
	140597367887232 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367887568 -> 140597367887232
	140597367887568 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367887808 -> 140597367887568
	140597367887808 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367888000 -> 140597367887808
	140597367888000 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367888192 -> 140597367888000
	140597367888192 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367889728 -> 140597367888192
	140597367889728 -> 140597367915680 [dir=none]
	140597367915680 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367889728 -> 140597367915600 [dir=none]
	140597367915600 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367889728 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367888240 -> 140597367889728
	140597857545328 [label="ct.encoder.layers.0.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597857545328 -> 140597367888240
	140597367888240 [label=AccumulateGrad]
	140597367887376 -> 140597367889728
	140597367887376 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367889440 -> 140597367887376
	140597367922944 -> 140597367889728
	140597367922944 [label=TBackward0]
	140597367923328 -> 140597367922944
	140597857545248 [label="ct.encoder.layers.0.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597857545248 -> 140597367923328
	140597367923328 [label=AccumulateGrad]
	140597367886128 -> 140597367869344
	140597367886128 [label=TBackward0]
	140597367886944 -> 140597367886128
	140597857545408 [label="ct.encoder.layers.0.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597857545408 -> 140597367886944
	140597367886944 [label=AccumulateGrad]
	140597367867808 -> 140597367867712
	140597367867808 -> 140597367915520 [dir=none]
	140597367915520 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367867808 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367869104 -> 140597367867808
	140597367869104 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367867904 -> 140597367869104
	140597367867904 -> 140597480514080 [dir=none]
	140597480514080 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367867904 -> 140597857544608 [dir=none]
	140597857544608 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597367867904 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367887040 -> 140597367867904
	140597367887040 -> 140597480455520 [dir=none]
	140597480455520 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367887040 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367887664 -> 140597367887040
	140597367887664 -> 140597480455680 [dir=none]
	140597480455680 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367887664 -> 140597367915200 [dir=none]
	140597367915200 [label="result1
 (512)" fillcolor=orange]
	140597367887664 -> 140597367916000 [dir=none]
	140597367916000 [label="result2
 (512)" fillcolor=orange]
	140597367887664 -> 140597367915440 [dir=none]
	140597367915440 [label="result3
 (0)" fillcolor=orange]
	140597367887664 -> 140597390289872 [dir=none]
	140597390289872 [label="running_mean
 (512)" fillcolor=orange]
	140597367887664 -> 140597875284624 [dir=none]
	140597875284624 [label="running_var
 (512)" fillcolor=orange]
	140597367887664 -> 140597857544288 [dir=none]
	140597857544288 [label="weight
 (512)" fillcolor=orange]
	140597367887664 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367887328 -> 140597367887664
	140597367887328 -> 140597480455360 [dir=none]
	140597480455360 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367887328 -> 140597857544128 [dir=none]
	140597857544128 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367887328 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367923520 -> 140597367887328
	140597367923520 -> 140597480455440 [dir=none]
	140597480455440 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367923520 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367923856 -> 140597367923520
	140597367923856 -> 140597480455760 [dir=none]
	140597480455760 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367923856 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367923712 -> 140597367923856
	140597367923712 -> 140597480455920 [dir=none]
	140597480455920 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367923712 -> 140597857879920 [dir=none]
	140597857879920 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367923712 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367924384 -> 140597367923712
	140597367924384 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367924672 -> 140597367924384
	140597367924672 -> 140597875702176 [dir=none]
	140597875702176 [label="bias
 (512)" fillcolor=orange]
	140597367924672 -> 140597480456080 [dir=none]
	140597480456080 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367924672 -> 140597367916400 [dir=none]
	140597367916400 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367924672 -> 140597367916080 [dir=none]
	140597367916080 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367924672 -> 140597857880960 [dir=none]
	140597857880960 [label="weight
 (512)" fillcolor=orange]
	140597367924672 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367867856 -> 140597367924672
	140597367924768 -> 140597367924672
	140597857880960 [label="ct.encoder.layers.0.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597857880960 -> 140597367924768
	140597367924768 [label=AccumulateGrad]
	140597367924816 -> 140597367924672
	140597875702176 [label="ct.encoder.layers.0.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597875702176 -> 140597367924816
	140597367924816 [label=AccumulateGrad]
	140597367924144 -> 140597367923712
	140597857879920 [label="ct.encoder.layers.0.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597857879920 -> 140597367924144
	140597367924144 [label=AccumulateGrad]
	140597367923136 -> 140597367923712
	140597876276656 [label="ct.encoder.layers.0.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597876276656 -> 140597367923136
	140597367923136 [label=AccumulateGrad]
	140597367923232 -> 140597367887328
	140597857544128 [label="ct.encoder.layers.0.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597857544128 -> 140597367923232
	140597367923232 [label=AccumulateGrad]
	140597367923040 -> 140597367887328
	140597857544208 [label="ct.encoder.layers.0.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597857544208 -> 140597367923040
	140597367923040 [label=AccumulateGrad]
	140597367887904 -> 140597367887664
	140597857544288 [label="ct.encoder.layers.0.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597857544288 -> 140597367887904
	140597367887904 [label=AccumulateGrad]
	140597367886896 -> 140597367887664
	140597857544368 [label="ct.encoder.layers.0.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597857544368 -> 140597367886896
	140597367886896 [label=AccumulateGrad]
	140597367887136 -> 140597367867904
	140597857544608 [label="ct.encoder.layers.0.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597857544608 -> 140597367887136
	140597367887136 [label=AccumulateGrad]
	140597367886320 -> 140597367867904
	140597857544688 [label="ct.encoder.layers.0.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597857544688 -> 140597367886320
	140597367886320 [label=AccumulateGrad]
	140597367867664 -> 140597367867520
	140597367867664 -> 140597367916560 [dir=none]
	140597367916560 [label="other
 ()" fillcolor=orange]
	140597367867664 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367868336 -> 140597367867664
	140597367868336 -> 140597367916480 [dir=none]
	140597367916480 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367868336 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367887472 -> 140597367868336
	140597367887472 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367888912 -> 140597367887472
	140597367888912 -> 140597367916720 [dir=none]
	140597367916720 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367888912 -> 140597367915920 [dir=none]
	140597367915920 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367888912 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367924048 -> 140597367888912
	140597856784448 [label="ct.encoder.layers.0.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597856784448 -> 140597367924048
	140597367924048 [label=AccumulateGrad]
	140597367922848 -> 140597367888912
	140597367922848 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367924576 -> 140597367922848
	140597367924576 -> 140597367916320 [dir=none]
	140597367916320 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367924576 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367924912 -> 140597367924576
	140597367924912 -> 140597480513600 [dir=none]
	140597480513600 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367924912 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367925008 -> 140597367924912
	140597367925008 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367925104 -> 140597367925008
	140597367925104 -> 140597367916160 [dir=none]
	140597367916160 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367925104 -> 140597367916800 [dir=none]
	140597367916800 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367925104 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367925200 -> 140597367925104
	140597857546048 [label="ct.encoder.layers.0.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597857546048 -> 140597367925200
	140597367925200 [label=AccumulateGrad]
	140597367925152 -> 140597367925104
	140597367925152 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367925296 -> 140597367925152
	140597367925296 -> 140597857545888 [dir=none]
	140597857545888 [label="bias
 (512)" fillcolor=orange]
	140597367925296 -> 140597480513760 [dir=none]
	140597480513760 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367925296 -> 140597367916960 [dir=none]
	140597367916960 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367925296 -> 140597367916880 [dir=none]
	140597367916880 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367925296 -> 140597857545808 [dir=none]
	140597857545808 [label="weight
 (512)" fillcolor=orange]
	140597367925296 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367867712 -> 140597367925296
	140597367925488 -> 140597367925296
	140597857545808 [label="ct.encoder.layers.0.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597857545808 -> 140597367925488
	140597367925488 [label=AccumulateGrad]
	140597367925440 -> 140597367925296
	140597857545888 [label="ct.encoder.layers.0.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597857545888 -> 140597367925440
	140597367925440 [label=AccumulateGrad]
	140597367923760 -> 140597367925104
	140597367923760 [label=TBackward0]
	140597367925536 -> 140597367923760
	140597857545968 [label="ct.encoder.layers.0.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597857545968 -> 140597367925536
	140597367925536 [label=AccumulateGrad]
	140597367923424 -> 140597367888912
	140597367923424 [label=TBackward0]
	140597367924960 -> 140597367923424
	140597857546128 [label="ct.encoder.layers.0.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597857546128 -> 140597367924960
	140597367924960 [label=AccumulateGrad]
	140597367867472 -> 140597367867376
	140597856784528 [label="ct.encoder.layers.0.norm_out.weight
 (512)" fillcolor=lightblue]
	140597856784528 -> 140597367867472
	140597367867472 [label=AccumulateGrad]
	140597367867424 -> 140597367867376
	140597856784608 [label="ct.encoder.layers.0.norm_out.bias
 (512)" fillcolor=lightblue]
	140597856784608 -> 140597367867424
	140597367867424 [label=AccumulateGrad]
	140597367867328 -> 140597367867232
	140597367867328 -> 140597367917120 [dir=none]
	140597367917120 [label="other
 ()" fillcolor=orange]
	140597367867328 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367867760 -> 140597367867328
	140597367867760 -> 140597367917040 [dir=none]
	140597367917040 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367867760 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367886848 -> 140597367867760
	140597367886848 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367867568 -> 140597367886848
	140597367867568 -> 140597367917280 [dir=none]
	140597367917280 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367867568 -> 140597367916240 [dir=none]
	140597367916240 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367867568 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367925056 -> 140597367867568
	140597856785088 [label="ct.encoder.layers.1.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597856785088 -> 140597367925056
	140597367925056 [label=AccumulateGrad]
	140597367925248 -> 140597367867568
	140597367925248 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367925392 -> 140597367925248
	140597367925392 -> 140597367917440 [dir=none]
	140597367917440 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367925392 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367925680 -> 140597367925392
	140597367925680 -> 140597480514480 [dir=none]
	140597480514480 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367925680 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367925776 -> 140597367925680
	140597367925776 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367925872 -> 140597367925776
	140597367925872 -> 140597367917520 [dir=none]
	140597367917520 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367925872 -> 140597367917360 [dir=none]
	140597367917360 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367925872 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367925968 -> 140597367925872
	140597856784928 [label="ct.encoder.layers.1.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597856784928 -> 140597367925968
	140597367925968 [label=AccumulateGrad]
	140597367925920 -> 140597367925872
	140597367925920 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367926064 -> 140597367925920
	140597367926064 -> 140597856784768 [dir=none]
	140597856784768 [label="bias
 (512)" fillcolor=orange]
	140597367926064 -> 140597480514400 [dir=none]
	140597480514400 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367926064 -> 140597367917680 [dir=none]
	140597367917680 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367926064 -> 140597367917600 [dir=none]
	140597367917600 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367926064 -> 140597856784688 [dir=none]
	140597856784688 [label="weight
 (512)" fillcolor=orange]
	140597367926064 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367867376 -> 140597367926064
	140597367926304 -> 140597367926064
	140597856784688 [label="ct.encoder.layers.1.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597856784688 -> 140597367926304
	140597367926304 [label=AccumulateGrad]
	140597367926256 -> 140597367926064
	140597856784768 [label="ct.encoder.layers.1.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597856784768 -> 140597367926256
	140597367926256 [label=AccumulateGrad]
	140597367925584 -> 140597367925872
	140597367925584 [label=TBackward0]
	140597367926352 -> 140597367925584
	140597856784848 [label="ct.encoder.layers.1.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597856784848 -> 140597367926352
	140597367926352 [label=AccumulateGrad]
	140597367924336 -> 140597367867568
	140597367924336 [label=TBackward0]
	140597367925728 -> 140597367924336
	140597856785008 [label="ct.encoder.layers.1.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597856785008 -> 140597367925728
	140597367925728 [label=AccumulateGrad]
	140597367867184 -> 140597367867088
	140597367867184 -> 140597367917840 [dir=none]
	140597367917840 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367867184 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367886656 -> 140597367867184
	140597367886656 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367867280 -> 140597367886656
	140597367867280 -> 140597367917920 [dir=none]
	140597367917920 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367867280 -> 140597367917200 [dir=none]
	140597367917200 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367867280 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367926016 -> 140597367867280
	140597856786928 [label="ct.encoder.layers.1.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597856786928 -> 140597367926016
	140597367926016 [label=AccumulateGrad]
	140597367925344 -> 140597367867280
	140597367925344 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367925632 -> 140597367925344
	140597367925632 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367926112 -> 140597367925632
	140597367926112 [label=CloneBackward0]
	140597367926544 -> 140597367926112
	140597367926544 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367926640 -> 140597367926544
	140597367926640 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367926736 -> 140597367926640
	140597367926736 -> 140597367918080 [dir=none]
	140597367918080 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367926736 -> 140597367918000 [dir=none]
	140597367918000 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367926736 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367926448 -> 140597367926736
	140597367926448 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367451904 -> 140597367926448
	140597367451904 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367452000 -> 140597367451904
	140597367452000 -> 140597367916640 [dir=none]
	140597367916640 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367452000 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367452096 -> 140597367452000
	140597367452096 -> 140597480515920 [dir=none]
	140597480515920 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367452096 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367452192 -> 140597367452096
	140597367452192 -> 140597367918480 [dir=none]
	140597367918480 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367452192 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367452288 -> 140597367452192
	140597367452288 -> 140597480515920 [dir=none]
	140597480515920 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367452288 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367452384 -> 140597367452288
	140597367452384 -> 140597367918240 [dir=none]
	140597367918240 [label="other
 ()" fillcolor=orange]
	140597367452384 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367452480 -> 140597367452384
	140597367452480 [label="AddBackward0
------------
alpha: 1"]
	140597367452576 -> 140597367452480
	140597367452576 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367452720 -> 140597367452576
	140597367452720 -> 140597367918160 [dir=none]
	140597367918160 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367452720 -> 140597367917760 [dir=none]
	140597367917760 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367452720 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367452816 -> 140597367452720
	140597367452816 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367452960 -> 140597367452816
	140597367452960 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367453056 -> 140597367452960
	140597367453056 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367453152 -> 140597367453056
	140597367453152 [label="AddBackward0
------------
alpha: 1"]
	140597367453248 -> 140597367453152
	140597367453248 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367453392 -> 140597367453248
	140597367453392 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367453488 -> 140597367453392
	140597367453488 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367453584 -> 140597367453488
	140597367453584 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367453680 -> 140597367453584
	140597367453680 -> 140597367464000 [dir=none]
	140597367464000 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367453680 -> 140597367464080 [dir=none]
	140597367464080 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367453680 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367453776 -> 140597367453680
	140597856786448 [label="ct.encoder.layers.1.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597856786448 -> 140597367453776
	140597367453776 [label=AccumulateGrad]
	140597367453728 -> 140597367453680
	140597367453728 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367453872 -> 140597367453728
	140597367453872 -> 140597856786288 [dir=none]
	140597856786288 [label="bias
 (512)" fillcolor=orange]
	140597367453872 -> 140597480514720 [dir=none]
	140597480514720 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367453872 -> 140597367464400 [dir=none]
	140597367464400 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367453872 -> 140597367464240 [dir=none]
	140597367464240 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367453872 -> 140597856786208 [dir=none]
	140597856786208 [label="weight
 (512)" fillcolor=orange]
	140597367453872 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367867232 -> 140597367453872
	140597367454064 -> 140597367453872
	140597856786208 [label="ct.encoder.layers.1.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597856786208 -> 140597367454064
	140597367454064 [label=AccumulateGrad]
	140597367454016 -> 140597367453872
	140597856786288 [label="ct.encoder.layers.1.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597856786288 -> 140597367454016
	140597367454016 [label=AccumulateGrad]
	140597367453296 -> 140597367453680
	140597367453296 [label=TBackward0]
	140597367454112 -> 140597367453296
	140597856786368 [label="ct.encoder.layers.1.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597856786368 -> 140597367454112
	140597367454112 [label=AccumulateGrad]
	140597367453200 -> 140597367453152
	140597856787088 [label="ct.encoder.layers.1.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597856787088 -> 140597367453200
	140597367453200 [label=AccumulateGrad]
	140597367452768 -> 140597367452720
	140597367452768 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367453104 -> 140597367452768
	140597367453104 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367453536 -> 140597367453104
	140597367453536 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367453632 -> 140597367453536
	140597367453632 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367453824 -> 140597367453632
	140597367453824 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367454160 -> 140597367453824
	140597367454160 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367454256 -> 140597367454160
	140597367454256 -> 140597367464560 [dir=none]
	140597367464560 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367454256 -> 140597367464160 [dir=none]
	140597367464160 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367454256 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367454352 -> 140597367454256
	140597856786608 [label="ct.encoder.layers.1.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597856786608 -> 140597367454352
	140597367454352 [label=AccumulateGrad]
	140597367454304 -> 140597367454256
	140597367454304 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367453872 -> 140597367454304
	140597367452912 -> 140597367454256
	140597367452912 [label=TBackward0]
	140597367454544 -> 140597367452912
	140597856786528 [label="ct.encoder.layers.1.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597856786528 -> 140597367454544
	140597367454544 [label=AccumulateGrad]
	140597367452528 -> 140597367452480
	140597367452528 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597367453008 -> 140597367452528
	140597367453008 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367453344 -> 140597367453008
	140597367453344 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367454208 -> 140597367453344
	140597367454208 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367453920 -> 140597367454208
	140597367453920 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367454496 -> 140597367453920
	140597367454496 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367454592 -> 140597367454496
	140597367454592 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367454688 -> 140597367454592
	140597367454688 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367454784 -> 140597367454688
	140597367454784 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367454880 -> 140597367454784
	140597367454880 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367454976 -> 140597367454880
	140597367454976 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367455072 -> 140597367454976
	140597367455072 -> 140597367465120 [dir=none]
	140597367465120 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367455072 -> 140597367464320 [dir=none]
	140597367464320 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367455072 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367455168 -> 140597367455072
	140597367455168 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367455312 -> 140597367455168
	140597367455312 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367455408 -> 140597367455312
	140597367455408 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367455504 -> 140597367455408
	140597367455504 [label="AddBackward0
------------
alpha: 1"]
	140597367453248 -> 140597367455504
	140597367455600 -> 140597367455504
	140597856787168 [label="ct.encoder.layers.1.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597856787168 -> 140597367455600
	140597367455600 [label=AccumulateGrad]
	140597367455120 -> 140597367455072
	140597367455120 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367455456 -> 140597367455120
	140597367455456 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367455696 -> 140597367455456
	140597367455696 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367455648 -> 140597367455696
	140597367455648 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367484528 -> 140597367455648
	140597367484528 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597367484672 -> 140597367484528
	140597367484672 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597367484768 -> 140597367484672
	140597367484768 -> 140597367464720 [dir=none]
	140597367464720 [label="self
 (251, 512)" fillcolor=orange]
	140597367484768 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597367484864 -> 140597367484768
	140597367484864 [label=TBackward0]
	140597367484960 -> 140597367484864
	140597856787008 [label="ct.encoder.layers.1.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597856787008 -> 140597367484960
	140597367484960 [label=AccumulateGrad]
	140597367451712 -> 140597367926736
	140597367451712 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367452048 -> 140597367451712
	140597367452048 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367452240 -> 140597367452048
	140597367452240 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367452432 -> 140597367452240
	140597367452432 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367452624 -> 140597367452432
	140597367452624 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367453440 -> 140597367452624
	140597367453440 -> 140597367465200 [dir=none]
	140597367465200 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367453440 -> 140597367464640 [dir=none]
	140597367464640 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367453440 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367454400 -> 140597367453440
	140597856786768 [label="ct.encoder.layers.1.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597856786768 -> 140597367454400
	140597367454400 [label=AccumulateGrad]
	140597367453968 -> 140597367453440
	140597367453968 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367453872 -> 140597367453968
	140597367451856 -> 140597367453440
	140597367451856 [label=TBackward0]
	140597367454832 -> 140597367451856
	140597856786688 [label="ct.encoder.layers.1.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597856786688 -> 140597367454832
	140597367454832 [label=AccumulateGrad]
	140597367924864 -> 140597367867280
	140597367924864 [label=TBackward0]
	140597367926496 -> 140597367924864
	140597856786848 [label="ct.encoder.layers.1.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597856786848 -> 140597367926496
	140597367926496 [label=AccumulateGrad]
	140597367867040 -> 140597367866944
	140597367867040 -> 140597367465360 [dir=none]
	140597367465360 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367867040 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367867616 -> 140597367867040
	140597367867616 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367926400 -> 140597367867616
	140597367926400 -> 140597480514960 [dir=none]
	140597480514960 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367926400 -> 140597856786048 [dir=none]
	140597856786048 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597367926400 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367926592 -> 140597367926400
	140597367926592 -> 140597480515200 [dir=none]
	140597480515200 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367926592 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367452144 -> 140597367926592
	140597367452144 -> 140597480515600 [dir=none]
	140597480515600 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367452144 -> 140597367464800 [dir=none]
	140597367464800 [label="result1
 (512)" fillcolor=orange]
	140597367452144 -> 140597367465440 [dir=none]
	140597367465440 [label="result2
 (512)" fillcolor=orange]
	140597367452144 -> 140597367464960 [dir=none]
	140597367464960 [label="result3
 (0)" fillcolor=orange]
	140597367452144 -> 140597390289472 [dir=none]
	140597390289472 [label="running_mean
 (512)" fillcolor=orange]
	140597367452144 -> 140597857880560 [dir=none]
	140597857880560 [label="running_var
 (512)" fillcolor=orange]
	140597367452144 -> 140597856785648 [dir=none]
	140597856785648 [label="weight
 (512)" fillcolor=orange]
	140597367452144 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367451808 -> 140597367452144
	140597367451808 -> 140597480515280 [dir=none]
	140597480515280 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367451808 -> 140597856785488 [dir=none]
	140597856785488 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367451808 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367455024 -> 140597367451808
	140597367455024 -> 140597480515440 [dir=none]
	140597480515440 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367455024 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367455360 -> 140597367455024
	140597367455360 -> 140597480515520 [dir=none]
	140597480515520 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367455360 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367455216 -> 140597367455360
	140597367455216 -> 140597480516160 [dir=none]
	140597480516160 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367455216 -> 140597856785328 [dir=none]
	140597856785328 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367455216 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367455264 -> 140597367455216
	140597367455264 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367484912 -> 140597367455264
	140597367484912 -> 140597856785248 [dir=none]
	140597856785248 [label="bias
 (512)" fillcolor=orange]
	140597367484912 -> 140597480515680 [dir=none]
	140597480515680 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367484912 -> 140597367465840 [dir=none]
	140597367465840 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367484912 -> 140597367465520 [dir=none]
	140597367465520 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367484912 -> 140597856785168 [dir=none]
	140597856785168 [label="weight
 (512)" fillcolor=orange]
	140597367484912 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367867088 -> 140597367484912
	140597367485008 -> 140597367484912
	140597856785168 [label="ct.encoder.layers.1.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597856785168 -> 140597367485008
	140597367485008 [label=AccumulateGrad]
	140597367485056 -> 140597367484912
	140597856785248 [label="ct.encoder.layers.1.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597856785248 -> 140597367485056
	140597367485056 [label=AccumulateGrad]
	140597367454448 -> 140597367455216
	140597856785328 [label="ct.encoder.layers.1.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597856785328 -> 140597367454448
	140597367454448 [label=AccumulateGrad]
	140597367484624 -> 140597367455216
	140597856785408 [label="ct.encoder.layers.1.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597856785408 -> 140597367484624
	140597367484624 [label=AccumulateGrad]
	140597367454736 -> 140597367451808
	140597856785488 [label="ct.encoder.layers.1.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597856785488 -> 140597367454736
	140597367454736 [label=AccumulateGrad]
	140597367454640 -> 140597367451808
	140597856785568 [label="ct.encoder.layers.1.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597856785568 -> 140597367454640
	140597367454640 [label=AccumulateGrad]
	140597367452336 -> 140597367452144
	140597856785648 [label="ct.encoder.layers.1.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597856785648 -> 140597367452336
	140597367452336 [label=AccumulateGrad]
	140597367451760 -> 140597367452144
	140597856785728 [label="ct.encoder.layers.1.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597856785728 -> 140597367451760
	140597367451760 [label=AccumulateGrad]
	140597367926688 -> 140597367926400
	140597856786048 [label="ct.encoder.layers.1.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597856786048 -> 140597367926688
	140597367926688 [label=AccumulateGrad]
	140597367924480 -> 140597367926400
	140597856786128 [label="ct.encoder.layers.1.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597856786128 -> 140597367924480
	140597367924480 [label=AccumulateGrad]
	140597367866896 -> 140597367866752
	140597367866896 -> 140597367466000 [dir=none]
	140597367466000 [label="other
 ()" fillcolor=orange]
	140597367866896 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367867136 -> 140597367866896
	140597367867136 -> 140597367465920 [dir=none]
	140597367465920 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367867136 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367926160 -> 140597367867136
	140597367926160 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367454928 -> 140597367926160
	140597367454928 -> 140597367466160 [dir=none]
	140597367466160 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367454928 -> 140597367465280 [dir=none]
	140597367465280 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367454928 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367455552 -> 140597367454928
	140597856787648 [label="ct.encoder.layers.1.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597856787648 -> 140597367455552
	140597367455552 [label=AccumulateGrad]
	140597367452672 -> 140597367454928
	140597367452672 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367484816 -> 140597367452672
	140597367484816 -> 140597367465760 [dir=none]
	140597367465760 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367484816 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367485152 -> 140597367484816
	140597367485152 -> 140597480515040 [dir=none]
	140597480515040 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367485152 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367485248 -> 140597367485152
	140597367485248 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367485344 -> 140597367485248
	140597367485344 -> 140597367465600 [dir=none]
	140597367465600 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367485344 -> 140597367466240 [dir=none]
	140597367466240 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367485344 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367485440 -> 140597367485344
	140597856787488 [label="ct.encoder.layers.1.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597856787488 -> 140597367485440
	140597367485440 [label=AccumulateGrad]
	140597367485392 -> 140597367485344
	140597367485392 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367485536 -> 140597367485392
	140597367485536 -> 140597856787328 [dir=none]
	140597856787328 [label="bias
 (512)" fillcolor=orange]
	140597367485536 -> 140597480515840 [dir=none]
	140597480515840 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367485536 -> 140597367466400 [dir=none]
	140597367466400 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367485536 -> 140597367466320 [dir=none]
	140597367466320 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367485536 -> 140597856787248 [dir=none]
	140597856787248 [label="weight
 (512)" fillcolor=orange]
	140597367485536 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367866944 -> 140597367485536
	140597367485728 -> 140597367485536
	140597856787248 [label="ct.encoder.layers.1.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597856787248 -> 140597367485728
	140597367485728 [label=AccumulateGrad]
	140597367485680 -> 140597367485536
	140597856787328 [label="ct.encoder.layers.1.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597856787328 -> 140597367485680
	140597367485680 [label=AccumulateGrad]
	140597367484480 -> 140597367485344
	140597367484480 [label=TBackward0]
	140597367485776 -> 140597367484480
	140597856787408 [label="ct.encoder.layers.1.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597856787408 -> 140597367485776
	140597367485776 [label=AccumulateGrad]
	140597367451952 -> 140597367454928
	140597367451952 [label=TBackward0]
	140597367485200 -> 140597367451952
	140597856787568 [label="ct.encoder.layers.1.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597856787568 -> 140597367485200
	140597367485200 [label=AccumulateGrad]
	140597367866704 -> 140597367866560
	140597856787728 [label="ct.encoder.layers.1.norm_out.weight
 (512)" fillcolor=lightblue]
	140597856787728 -> 140597367866704
	140597367866704 [label=AccumulateGrad]
	140597367866608 -> 140597367866560
	140597856787808 [label="ct.encoder.layers.1.norm_out.bias
 (512)" fillcolor=lightblue]
	140597856787808 -> 140597367866608
	140597367866608 [label=AccumulateGrad]
	140597367866512 -> 140597367866416
	140597367866512 -> 140597367466560 [dir=none]
	140597367466560 [label="other
 ()" fillcolor=orange]
	140597367866512 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367866992 -> 140597367866512
	140597367866992 -> 140597367466480 [dir=none]
	140597367466480 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367866992 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367452864 -> 140597367866992
	140597367452864 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367866800 -> 140597367452864
	140597367866800 -> 140597367466720 [dir=none]
	140597367466720 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367866800 -> 140597367465680 [dir=none]
	140597367465680 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367866800 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367485296 -> 140597367866800
	140597856788288 [label="ct.encoder.layers.2.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597856788288 -> 140597367485296
	140597367485296 [label=AccumulateGrad]
	140597367485488 -> 140597367866800
	140597367485488 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367485632 -> 140597367485488
	140597367485632 -> 140597367466880 [dir=none]
	140597367466880 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367485632 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367485920 -> 140597367485632
	140597367485920 -> 140597480516560 [dir=none]
	140597480516560 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367485920 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367486016 -> 140597367485920
	140597367486016 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367486112 -> 140597367486016
	140597367486112 -> 140597367466960 [dir=none]
	140597367466960 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367486112 -> 140597367466800 [dir=none]
	140597367466800 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367486112 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367486208 -> 140597367486112
	140597856788128 [label="ct.encoder.layers.2.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597856788128 -> 140597367486208
	140597367486208 [label=AccumulateGrad]
	140597367486160 -> 140597367486112
	140597367486160 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367486304 -> 140597367486160
	140597367486304 -> 140597856787968 [dir=none]
	140597856787968 [label="bias
 (512)" fillcolor=orange]
	140597367486304 -> 140597480516480 [dir=none]
	140597480516480 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367486304 -> 140597367467120 [dir=none]
	140597367467120 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367486304 -> 140597367467040 [dir=none]
	140597367467040 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367486304 -> 140597856787888 [dir=none]
	140597856787888 [label="weight
 (512)" fillcolor=orange]
	140597367486304 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367866560 -> 140597367486304
	140597367486496 -> 140597367486304
	140597856787888 [label="ct.encoder.layers.2.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597856787888 -> 140597367486496
	140597367486496 [label=AccumulateGrad]
	140597367486448 -> 140597367486304
	140597856787968 [label="ct.encoder.layers.2.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597856787968 -> 140597367486448
	140597367486448 [label=AccumulateGrad]
	140597367485824 -> 140597367486112
	140597367485824 [label=TBackward0]
	140597367486544 -> 140597367485824
	140597856788048 [label="ct.encoder.layers.2.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597856788048 -> 140597367486544
	140597367486544 [label=AccumulateGrad]
	140597367484576 -> 140597367866800
	140597367484576 [label=TBackward0]
	140597367485968 -> 140597367484576
	140597856788208 [label="ct.encoder.layers.2.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597856788208 -> 140597367485968
	140597367485968 [label=AccumulateGrad]
	140597367866368 -> 140597367866272
	140597367866368 -> 140597367467280 [dir=none]
	140597367467280 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367866368 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367925824 -> 140597367866368
	140597367925824 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367866464 -> 140597367925824
	140597367866464 -> 140597367467360 [dir=none]
	140597367467360 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367866464 -> 140597367466640 [dir=none]
	140597367466640 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367866464 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367486256 -> 140597367866464
	140597855864528 [label="ct.encoder.layers.2.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597855864528 -> 140597367486256
	140597367486256 [label=AccumulateGrad]
	140597367485584 -> 140597367866464
	140597367485584 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367485872 -> 140597367485584
	140597367485872 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367486352 -> 140597367485872
	140597367486352 [label=CloneBackward0]
	140597367486736 -> 140597367486352
	140597367486736 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367486832 -> 140597367486736
	140597367486832 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367486928 -> 140597367486832
	140597367486928 -> 140597367467520 [dir=none]
	140597367467520 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367486928 -> 140597367467440 [dir=none]
	140597367467440 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367486928 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367487024 -> 140597367486928
	140597367487024 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367487168 -> 140597367487024
	140597367487168 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367487264 -> 140597367487168
	140597367487264 -> 140597367467200 [dir=none]
	140597367467200 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367487264 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367487360 -> 140597367487264
	140597367487360 -> 140597480518096 [dir=none]
	140597480518096 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367487360 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367487456 -> 140597367487360
	140597367487456 -> 140597367467840 [dir=none]
	140597367467840 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367487456 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367487552 -> 140597367487456
	140597367487552 -> 140597480518096 [dir=none]
	140597480518096 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367487552 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367487648 -> 140597367487552
	140597367487648 -> 140597367467760 [dir=none]
	140597367467760 [label="other
 ()" fillcolor=orange]
	140597367487648 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367487744 -> 140597367487648
	140597367487744 [label="AddBackward0
------------
alpha: 1"]
	140597367487840 -> 140597367487744
	140597367487840 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367487984 -> 140597367487840
	140597367487984 -> 140597367467680 [dir=none]
	140597367467680 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367487984 -> 140597367467600 [dir=none]
	140597367467600 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367487984 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367488080 -> 140597367487984
	140597367488080 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367488224 -> 140597367488080
	140597367488224 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367488320 -> 140597367488224
	140597367488320 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367488416 -> 140597367488320
	140597367488416 [label="AddBackward0
------------
alpha: 1"]
	140597367488464 -> 140597367488416
	140597367488464 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367537872 -> 140597367488464
	140597367537872 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367537968 -> 140597367537872
	140597367537968 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367538064 -> 140597367537968
	140597367538064 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367538160 -> 140597367538064
	140597367538160 -> 140597367466080 [dir=none]
	140597367466080 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367538160 -> 140597367465040 [dir=none]
	140597367465040 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367538160 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367538256 -> 140597367538160
	140597855864048 [label="ct.encoder.layers.2.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597855864048 -> 140597367538256
	140597367538256 [label=AccumulateGrad]
	140597367538208 -> 140597367538160
	140597367538208 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367538352 -> 140597367538208
	140597367538352 -> 140597855863888 [dir=none]
	140597855863888 [label="bias
 (512)" fillcolor=orange]
	140597367538352 -> 140597480516800 [dir=none]
	140597480516800 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367538352 -> 140597367467920 [dir=none]
	140597367467920 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367538352 -> 140597367550016 [dir=none]
	140597367550016 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367538352 -> 140597855863808 [dir=none]
	140597855863808 [label="weight
 (512)" fillcolor=orange]
	140597367538352 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367866416 -> 140597367538352
	140597367538544 -> 140597367538352
	140597855863808 [label="ct.encoder.layers.2.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597855863808 -> 140597367538544
	140597367538544 [label=AccumulateGrad]
	140597367538496 -> 140597367538352
	140597855863888 [label="ct.encoder.layers.2.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597855863888 -> 140597367538496
	140597367538496 [label=AccumulateGrad]
	140597367537776 -> 140597367538160
	140597367537776 [label=TBackward0]
	140597367538592 -> 140597367537776
	140597855863968 [label="ct.encoder.layers.2.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597855863968 -> 140597367538592
	140597367538592 [label=AccumulateGrad]
	140597367488128 -> 140597367488416
	140597855864688 [label="ct.encoder.layers.2.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597855864688 -> 140597367488128
	140597367488128 [label=AccumulateGrad]
	140597367488032 -> 140597367487984
	140597367488032 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367488368 -> 140597367488032
	140597367488368 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367488176 -> 140597367488368
	140597367488176 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367538112 -> 140597367488176
	140597367538112 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367538304 -> 140597367538112
	140597367538304 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367538640 -> 140597367538304
	140597367538640 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367538736 -> 140597367538640
	140597367538736 -> 140597367550256 [dir=none]
	140597367550256 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367538736 -> 140597367550096 [dir=none]
	140597367550096 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367538736 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367538832 -> 140597367538736
	140597855864208 [label="ct.encoder.layers.2.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597855864208 -> 140597367538832
	140597367538832 [label=AccumulateGrad]
	140597367538784 -> 140597367538736
	140597367538784 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367538352 -> 140597367538784
	140597367537728 -> 140597367538736
	140597367537728 [label=TBackward0]
	140597367539024 -> 140597367537728
	140597855864128 [label="ct.encoder.layers.2.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597855864128 -> 140597367539024
	140597367539024 [label=AccumulateGrad]
	140597367487792 -> 140597367487744
	140597367487792 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597367488272 -> 140597367487792
	140597367488272 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367487936 -> 140597367488272
	140597367487936 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367538688 -> 140597367487936
	140597367538688 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367538400 -> 140597367538688
	140597367538400 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367538976 -> 140597367538400
	140597367538976 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367539072 -> 140597367538976
	140597367539072 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367539168 -> 140597367539072
	140597367539168 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367539264 -> 140597367539168
	140597367539264 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367539360 -> 140597367539264
	140597367539360 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367539456 -> 140597367539360
	140597367539456 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367539552 -> 140597367539456
	140597367539552 -> 140597367550896 [dir=none]
	140597367550896 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367539552 -> 140597367550656 [dir=none]
	140597367550656 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367539552 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367539648 -> 140597367539552
	140597367539648 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367539792 -> 140597367539648
	140597367539792 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367539888 -> 140597367539792
	140597367539888 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367539984 -> 140597367539888
	140597367539984 [label="AddBackward0
------------
alpha: 1"]
	140597367488464 -> 140597367539984
	140597367540080 -> 140597367539984
	140597855864768 [label="ct.encoder.layers.2.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597855864768 -> 140597367540080
	140597367540080 [label=AccumulateGrad]
	140597367539600 -> 140597367539552
	140597367539600 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367539936 -> 140597367539600
	140597367539936 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367540176 -> 140597367539936
	140597367540176 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367540224 -> 140597367540176
	140597367540224 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367540272 -> 140597367540224
	140597367540272 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597367540416 -> 140597367540272
	140597367540416 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597367540512 -> 140597367540416
	140597367540512 -> 140597367550336 [dir=none]
	140597367550336 [label="self
 (251, 512)" fillcolor=orange]
	140597367540512 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597367540608 -> 140597367540512
	140597367540608 [label=TBackward0]
	140597367540704 -> 140597367540608
	140597855864608 [label="ct.encoder.layers.2.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597855864608 -> 140597367540704
	140597367540704 [label=AccumulateGrad]
	140597367486976 -> 140597367486928
	140597367486976 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367487312 -> 140597367486976
	140597367487312 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367487504 -> 140597367487312
	140597367487504 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367487696 -> 140597367487504
	140597367487696 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367487888 -> 140597367487696
	140597367487888 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367487120 -> 140597367487888
	140597367487120 -> 140597367550736 [dir=none]
	140597367550736 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367487120 -> 140597367550576 [dir=none]
	140597367550576 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367487120 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367538880 -> 140597367487120
	140597855864368 [label="ct.encoder.layers.2.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597855864368 -> 140597367538880
	140597367538880 [label=AccumulateGrad]
	140597367538448 -> 140597367487120
	140597367538448 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367538352 -> 140597367538448
	140597367537824 -> 140597367487120
	140597367537824 [label=TBackward0]
	140597367539312 -> 140597367537824
	140597855864288 [label="ct.encoder.layers.2.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597855864288 -> 140597367539312
	140597367539312 [label=AccumulateGrad]
	140597367485104 -> 140597367866464
	140597367485104 [label=TBackward0]
	140597367486688 -> 140597367485104
	140597855864448 [label="ct.encoder.layers.2.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597855864448 -> 140597367486688
	140597367486688 [label=AccumulateGrad]
	140597367866224 -> 140597367866128
	140597367866224 -> 140597367550496 [dir=none]
	140597367550496 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367866224 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367866848 -> 140597367866224
	140597367866848 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367486592 -> 140597367866848
	140597367486592 -> 140597480517856 [dir=none]
	140597480517856 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367486592 -> 140597855863648 [dir=none]
	140597855863648 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597367486592 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367486784 -> 140597367486592
	140597367486784 -> 140597480517936 [dir=none]
	140597480517936 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367486784 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367487408 -> 140597367486784
	140597367487408 -> 140597480516960 [dir=none]
	140597480516960 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367487408 -> 140597367550176 [dir=none]
	140597367550176 [label="result1
 (512)" fillcolor=orange]
	140597367487408 -> 140597367551056 [dir=none]
	140597367551056 [label="result2
 (512)" fillcolor=orange]
	140597367487408 -> 140597367550416 [dir=none]
	140597367550416 [label="result3
 (0)" fillcolor=orange]
	140597367487408 -> 140597857544528 [dir=none]
	140597857544528 [label="running_mean
 (512)" fillcolor=orange]
	140597367487408 -> 140597856785968 [dir=none]
	140597856785968 [label="running_var
 (512)" fillcolor=orange]
	140597367487408 -> 140597855863248 [dir=none]
	140597855863248 [label="weight
 (512)" fillcolor=orange]
	140597367487408 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367487072 -> 140597367487408
	140597367487072 -> 140597480518336 [dir=none]
	140597480518336 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367487072 -> 140597855863088 [dir=none]
	140597855863088 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367487072 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367539504 -> 140597367487072
	140597367539504 -> 140597480517120 [dir=none]
	140597480517120 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367539504 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367539840 -> 140597367539504
	140597367539840 -> 140597480517040 [dir=none]
	140597480517040 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367539840 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367539696 -> 140597367539840
	140597367539696 -> 140597480517280 [dir=none]
	140597480517280 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367539696 -> 140597855862928 [dir=none]
	140597855862928 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367539696 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367540368 -> 140597367539696
	140597367540368 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367540656 -> 140597367540368
	140597367540656 -> 140597855862848 [dir=none]
	140597855862848 [label="bias
 (512)" fillcolor=orange]
	140597367540656 -> 140597480517360 [dir=none]
	140597480517360 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367540656 -> 140597367551456 [dir=none]
	140597367551456 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367540656 -> 140597367551136 [dir=none]
	140597367551136 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367540656 -> 140597856788368 [dir=none]
	140597856788368 [label="weight
 (512)" fillcolor=orange]
	140597367540656 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367866272 -> 140597367540656
	140597367540752 -> 140597367540656
	140597856788368 [label="ct.encoder.layers.2.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597856788368 -> 140597367540752
	140597367540752 [label=AccumulateGrad]
	140597367540800 -> 140597367540656
	140597855862848 [label="ct.encoder.layers.2.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597855862848 -> 140597367540800
	140597367540800 [label=AccumulateGrad]
	140597367540128 -> 140597367539696
	140597855862928 [label="ct.encoder.layers.2.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597855862928 -> 140597367540128
	140597367540128 [label=AccumulateGrad]
	140597367538928 -> 140597367539696
	140597855863008 [label="ct.encoder.layers.2.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597855863008 -> 140597367538928
	140597367538928 [label=AccumulateGrad]
	140597367539216 -> 140597367487072
	140597855863088 [label="ct.encoder.layers.2.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597855863088 -> 140597367539216
	140597367539216 [label=AccumulateGrad]
	140597367539120 -> 140597367487072
	140597855863168 [label="ct.encoder.layers.2.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597855863168 -> 140597367539120
	140597367539120 [label=AccumulateGrad]
	140597367487600 -> 140597367487408
	140597855863248 [label="ct.encoder.layers.2.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597855863248 -> 140597367487600
	140597367487600 [label=AccumulateGrad]
	140597367486640 -> 140597367487408
	140597855863328 [label="ct.encoder.layers.2.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597855863328 -> 140597367486640
	140597367486640 [label=AccumulateGrad]
	140597367486880 -> 140597367486592
	140597855863648 [label="ct.encoder.layers.2.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597855863648 -> 140597367486880
	140597367486880 [label=AccumulateGrad]
	140597367484720 -> 140597367486592
	140597855863728 [label="ct.encoder.layers.2.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597855863728 -> 140597367484720
	140597367484720 [label=AccumulateGrad]
	140597367866080 -> 140597367865936
	140597367866080 -> 140597367551616 [dir=none]
	140597367551616 [label="other
 ()" fillcolor=orange]
	140597367866080 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367866320 -> 140597367866080
	140597367866320 -> 140597367551536 [dir=none]
	140597367551536 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367866320 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367487216 -> 140597367866320
	140597367487216 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367486064 -> 140597367487216
	140597367486064 -> 140597367551776 [dir=none]
	140597367551776 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367486064 -> 140597367550976 [dir=none]
	140597367550976 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367486064 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367540032 -> 140597367486064
	140597855865248 [label="ct.encoder.layers.2.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597855865248 -> 140597367540032
	140597367540032 [label=AccumulateGrad]
	140597367538016 -> 140597367486064
	140597367538016 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367540560 -> 140597367538016
	140597367540560 -> 140597367551376 [dir=none]
	140597367551376 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367540560 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367540896 -> 140597367540560
	140597367540896 -> 140597480517776 [dir=none]
	140597480517776 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367540896 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367540992 -> 140597367540896
	140597367540992 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367541088 -> 140597367540992
	140597367541088 -> 140597367551216 [dir=none]
	140597367551216 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367541088 -> 140597367551856 [dir=none]
	140597367551856 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367541088 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367541184 -> 140597367541088
	140597855865088 [label="ct.encoder.layers.2.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597855865088 -> 140597367541184
	140597367541184 [label=AccumulateGrad]
	140597367541136 -> 140597367541088
	140597367541136 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367541280 -> 140597367541136
	140597367541280 -> 140597855864928 [dir=none]
	140597855864928 [label="bias
 (512)" fillcolor=orange]
	140597367541280 -> 140597480518016 [dir=none]
	140597480518016 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367541280 -> 140597367552016 [dir=none]
	140597367552016 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367541280 -> 140597367551936 [dir=none]
	140597367551936 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367541280 -> 140597855864848 [dir=none]
	140597855864848 [label="weight
 (512)" fillcolor=orange]
	140597367541280 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367866128 -> 140597367541280
	140597367541472 -> 140597367541280
	140597855864848 [label="ct.encoder.layers.2.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597855864848 -> 140597367541472
	140597367541472 [label=AccumulateGrad]
	140597367541424 -> 140597367541280
	140597855864928 [label="ct.encoder.layers.2.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597855864928 -> 140597367541424
	140597367541424 [label=AccumulateGrad]
	140597367539744 -> 140597367541088
	140597367539744 [label=TBackward0]
	140597367541520 -> 140597367539744
	140597855865008 [label="ct.encoder.layers.2.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597855865008 -> 140597367541520
	140597367541520 [label=AccumulateGrad]
	140597367537920 -> 140597367486064
	140597367537920 [label=TBackward0]
	140597367540944 -> 140597367537920
	140597855865168 [label="ct.encoder.layers.2.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597855865168 -> 140597367540944
	140597367540944 [label=AccumulateGrad]
	140597367865888 -> 140597367865792
	140597855865328 [label="ct.encoder.layers.2.norm_out.weight
 (512)" fillcolor=lightblue]
	140597855865328 -> 140597367865888
	140597367865888 [label=AccumulateGrad]
	140597367865840 -> 140597367865792
	140597855865408 [label="ct.encoder.layers.2.norm_out.bias
 (512)" fillcolor=lightblue]
	140597855865408 -> 140597367865840
	140597367865840 [label=AccumulateGrad]
	140597367865744 -> 140597367865648
	140597367865744 -> 140597367552176 [dir=none]
	140597367552176 [label="other
 ()" fillcolor=orange]
	140597367865744 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367866176 -> 140597367865744
	140597367866176 -> 140597367552096 [dir=none]
	140597367552096 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367866176 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367486400 -> 140597367866176
	140597367486400 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367540848 -> 140597367486400
	140597367540848 -> 140597367552336 [dir=none]
	140597367552336 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367540848 -> 140597367551296 [dir=none]
	140597367551296 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367540848 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367541040 -> 140597367540848
	140597855865888 [label="ct.encoder.layers.3.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597855865888 -> 140597367541040
	140597367541040 [label=AccumulateGrad]
	140597367541232 -> 140597367540848
	140597367541232 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367541376 -> 140597367541232
	140597367541376 -> 140597367552496 [dir=none]
	140597367552496 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367541376 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367541664 -> 140597367541376
	140597367541664 -> 140597480518736 [dir=none]
	140597480518736 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367541664 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367541712 -> 140597367541664
	140597367541712 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367595072 -> 140597367541712
	140597367595072 -> 140597367552576 [dir=none]
	140597367552576 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367595072 -> 140597367552416 [dir=none]
	140597367552416 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367595072 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367595264 -> 140597367595072
	140597855865728 [label="ct.encoder.layers.3.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597855865728 -> 140597367595264
	140597367595264 [label=AccumulateGrad]
	140597367595216 -> 140597367595072
	140597367595216 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367595360 -> 140597367595216
	140597367595360 -> 140597855865568 [dir=none]
	140597855865568 [label="bias
 (512)" fillcolor=orange]
	140597367595360 -> 140597480518656 [dir=none]
	140597480518656 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367595360 -> 140597367552736 [dir=none]
	140597367552736 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367595360 -> 140597367552656 [dir=none]
	140597367552656 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367595360 -> 140597855865488 [dir=none]
	140597855865488 [label="weight
 (512)" fillcolor=orange]
	140597367595360 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367865792 -> 140597367595360
	140597367595552 -> 140597367595360
	140597855865488 [label="ct.encoder.layers.3.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597855865488 -> 140597367595552
	140597367595552 [label=AccumulateGrad]
	140597367595504 -> 140597367595360
	140597855865568 [label="ct.encoder.layers.3.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597855865568 -> 140597367595504
	140597367595504 [label=AccumulateGrad]
	140597367595168 -> 140597367595072
	140597367595168 [label=TBackward0]
	140597367595600 -> 140597367595168
	140597855865648 [label="ct.encoder.layers.3.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597855865648 -> 140597367595600
	140597367595600 [label=AccumulateGrad]
	140597367539408 -> 140597367540848
	140597367539408 [label=TBackward0]
	140597367541568 -> 140597367539408
	140597855865808 [label="ct.encoder.layers.3.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597855865808 -> 140597367541568
	140597367541568 [label=AccumulateGrad]
	140597367865600 -> 140597367865504
	140597367865600 -> 140597367552896 [dir=none]
	140597367552896 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367865600 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367865984 -> 140597367865600
	140597367865984 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367865696 -> 140597367865984
	140597367865696 -> 140597367552976 [dir=none]
	140597367552976 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367865696 -> 140597367552256 [dir=none]
	140597367552256 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367865696 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367541616 -> 140597367865696
	140597855617968 [label="ct.encoder.layers.3.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597855617968 -> 140597367541616
	140597367541616 [label=AccumulateGrad]
	140597367541328 -> 140597367865696
	140597367541328 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367595120 -> 140597367541328
	140597367595120 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367595408 -> 140597367595120
	140597367595408 [label=CloneBackward0]
	140597367595792 -> 140597367595408
	140597367595792 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367595888 -> 140597367595792
	140597367595888 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367595984 -> 140597367595888
	140597367595984 -> 140597367553136 [dir=none]
	140597367553136 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367595984 -> 140597367553056 [dir=none]
	140597367553056 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367595984 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367596080 -> 140597367595984
	140597367596080 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367596224 -> 140597367596080
	140597367596224 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367596320 -> 140597367596224
	140597367596320 -> 140597367552816 [dir=none]
	140597367552816 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367596320 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367596416 -> 140597367596320
	140597367596416 -> 140597480520176 [dir=none]
	140597480520176 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367596416 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367596512 -> 140597367596416
	140597367596512 -> 140597367553456 [dir=none]
	140597367553456 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367596512 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367596608 -> 140597367596512
	140597367596608 -> 140597480520176 [dir=none]
	140597480520176 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367596608 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367596704 -> 140597367596608
	140597367596704 -> 140597367553376 [dir=none]
	140597367553376 [label="other
 ()" fillcolor=orange]
	140597367596704 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367596800 -> 140597367596704
	140597367596800 [label="AddBackward0
------------
alpha: 1"]
	140597367596896 -> 140597367596800
	140597367596896 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367597040 -> 140597367596896
	140597367597040 -> 140597367553296 [dir=none]
	140597367553296 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367597040 -> 140597367553216 [dir=none]
	140597367553216 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367597040 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367597136 -> 140597367597040
	140597367597136 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367597280 -> 140597367597136
	140597367597280 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367597376 -> 140597367597280
	140597367597376 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367597472 -> 140597367597376
	140597367597472 [label="AddBackward0
------------
alpha: 1"]
	140597367597568 -> 140597367597472
	140597367597568 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367597712 -> 140597367597568
	140597367597712 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367597808 -> 140597367597712
	140597367597808 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367597904 -> 140597367597808
	140597367597904 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367598000 -> 140597367597904
	140597367598000 -> 140597367550816 [dir=none]
	140597367550816 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367598000 -> 140597367553696 [dir=none]
	140597367553696 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367598000 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367598096 -> 140597367598000
	140597855617488 [label="ct.encoder.layers.3.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597855617488 -> 140597367598096
	140597367598096 [label=AccumulateGrad]
	140597367598048 -> 140597367598000
	140597367598048 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367598192 -> 140597367598048
	140597367598192 -> 140597855617328 [dir=none]
	140597855617328 [label="bias
 (512)" fillcolor=orange]
	140597367598192 -> 140597480518976 [dir=none]
	140597480518976 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367598192 -> 140597367553776 [dir=none]
	140597367553776 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367598192 -> 140597367551696 [dir=none]
	140597367551696 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367598192 -> 140597855617248 [dir=none]
	140597855617248 [label="weight
 (512)" fillcolor=orange]
	140597367598192 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367865648 -> 140597367598192
	140597367598384 -> 140597367598192
	140597855617248 [label="ct.encoder.layers.3.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597855617248 -> 140597367598384
	140597367598384 [label=AccumulateGrad]
	140597367598336 -> 140597367598192
	140597855617328 [label="ct.encoder.layers.3.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597855617328 -> 140597367598336
	140597367598336 [label=AccumulateGrad]
	140597367597616 -> 140597367598000
	140597367597616 [label=TBackward0]
	140597367598432 -> 140597367597616
	140597855617408 [label="ct.encoder.layers.3.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597855617408 -> 140597367598432
	140597367598432 [label=AccumulateGrad]
	140597367597520 -> 140597367597472
	140597855618128 [label="ct.encoder.layers.3.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597855618128 -> 140597367597520
	140597367597520 [label=AccumulateGrad]
	140597367597088 -> 140597367597040
	140597367597088 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367597424 -> 140597367597088
	140597367597424 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367597856 -> 140597367597424
	140597367597856 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367597952 -> 140597367597856
	140597367597952 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367598144 -> 140597367597952
	140597367598144 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367598480 -> 140597367598144
	140597367598480 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367598576 -> 140597367598480
	140597367598576 -> 140597367553856 [dir=none]
	140597367553856 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367598576 -> 140597367553936 [dir=none]
	140597367553936 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367598576 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367598672 -> 140597367598576
	140597855617648 [label="ct.encoder.layers.3.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597855617648 -> 140597367598672
	140597367598672 [label=AccumulateGrad]
	140597367598624 -> 140597367598576
	140597367598624 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367598192 -> 140597367598624
	140597367597232 -> 140597367598576
	140597367597232 [label=TBackward0]
	140597367598864 -> 140597367597232
	140597855617568 [label="ct.encoder.layers.3.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597855617568 -> 140597367598864
	140597367598864 [label=AccumulateGrad]
	140597367596848 -> 140597367596800
	140597367596848 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597367597328 -> 140597367596848
	140597367597328 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367597664 -> 140597367597328
	140597367597664 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367598528 -> 140597367597664
	140597367598528 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367598240 -> 140597367598528
	140597367598240 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367598816 -> 140597367598240
	140597367598816 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367598912 -> 140597367598816
	140597367598912 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367599008 -> 140597367598912
	140597367599008 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367599056 -> 140597367599008
	140597367599056 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367632032 -> 140597367599056
	140597367632032 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367632128 -> 140597367632032
	140597367632128 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367632224 -> 140597367632128
	140597367632224 -> 140597367553616 [dir=none]
	140597367553616 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367632224 -> 140597367553536 [dir=none]
	140597367553536 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367632224 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367632320 -> 140597367632224
	140597367632320 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367632464 -> 140597367632320
	140597367632464 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367632560 -> 140597367632464
	140597367632560 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367632656 -> 140597367632560
	140597367632656 [label="AddBackward0
------------
alpha: 1"]
	140597367597568 -> 140597367632656
	140597367632752 -> 140597367632656
	140597855618208 [label="ct.encoder.layers.3.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597855618208 -> 140597367632752
	140597367632752 [label=AccumulateGrad]
	140597367632272 -> 140597367632224
	140597367632272 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367632608 -> 140597367632272
	140597367632608 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367632848 -> 140597367632608
	140597367632848 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367632896 -> 140597367632848
	140597367632896 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367632944 -> 140597367632896
	140597367632944 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597367633088 -> 140597367632944
	140597367633088 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597367633184 -> 140597367633088
	140597367633184 -> 140597367636272 [dir=none]
	140597367636272 [label="self
 (251, 512)" fillcolor=orange]
	140597367633184 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597367633280 -> 140597367633184
	140597367633280 [label=TBackward0]
	140597367633376 -> 140597367633280
	140597855618048 [label="ct.encoder.layers.3.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597855618048 -> 140597367633376
	140597367633376 [label=AccumulateGrad]
	140597367596032 -> 140597367595984
	140597367596032 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367596368 -> 140597367596032
	140597367596368 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367596560 -> 140597367596368
	140597367596560 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367596752 -> 140597367596560
	140597367596752 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367596944 -> 140597367596752
	140597367596944 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367597760 -> 140597367596944
	140597367597760 -> 140597367636112 [dir=none]
	140597367636112 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367597760 -> 140597367636512 [dir=none]
	140597367636512 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367597760 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367598720 -> 140597367597760
	140597855617808 [label="ct.encoder.layers.3.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597855617808 -> 140597367598720
	140597367598720 [label=AccumulateGrad]
	140597367598288 -> 140597367597760
	140597367598288 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367598192 -> 140597367598288
	140597367596176 -> 140597367597760
	140597367596176 [label=TBackward0]
	140597367596992 -> 140597367596176
	140597855617728 [label="ct.encoder.layers.3.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597855617728 -> 140597367596992
	140597367596992 [label=AccumulateGrad]
	140597367540320 -> 140597367865696
	140597367540320 [label=TBackward0]
	140597367595744 -> 140597367540320
	140597855617888 [label="ct.encoder.layers.3.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597855617888 -> 140597367595744
	140597367595744 [label=AccumulateGrad]
	140597367865456 -> 140597367857104
	140597367865456 -> 140597367636592 [dir=none]
	140597367636592 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367865456 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367540464 -> 140597367865456
	140597367540464 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367865552 -> 140597367540464
	140597367865552 -> 140597480519216 [dir=none]
	140597480519216 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367865552 -> 140597855617088 [dir=none]
	140597855617088 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597367865552 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367595840 -> 140597367865552
	140597367595840 -> 140597480519456 [dir=none]
	140597480519456 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367595840 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367596464 -> 140597367595840
	140597367596464 -> 140597480519856 [dir=none]
	140597480519856 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367596464 -> 140597367636352 [dir=none]
	140597367636352 [label="result1
 (512)" fillcolor=orange]
	140597367596464 -> 140597367636672 [dir=none]
	140597367636672 [label="result2
 (512)" fillcolor=orange]
	140597367596464 -> 140597367636192 [dir=none]
	140597367636192 [label="result3
 (0)" fillcolor=orange]
	140597367596464 -> 140597857543008 [dir=none]
	140597857543008 [label="running_mean
 (512)" fillcolor=orange]
	140597367596464 -> 140597855863568 [dir=none]
	140597855863568 [label="running_var
 (512)" fillcolor=orange]
	140597367596464 -> 140597855866448 [dir=none]
	140597855866448 [label="weight
 (512)" fillcolor=orange]
	140597367596464 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367596128 -> 140597367596464
	140597367596128 -> 140597480519536 [dir=none]
	140597480519536 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367596128 -> 140597855866288 [dir=none]
	140597855866288 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367596128 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367598768 -> 140597367596128
	140597367598768 -> 140597480519696 [dir=none]
	140597480519696 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367598768 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367632512 -> 140597367598768
	140597367632512 -> 140597480519776 [dir=none]
	140597480519776 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367632512 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367632368 -> 140597367632512
	140597367632368 -> 140597480520416 [dir=none]
	140597480520416 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367632368 -> 140597855866128 [dir=none]
	140597855866128 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367632368 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367633040 -> 140597367632368
	140597367633040 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367633328 -> 140597367633040
	140597367633328 -> 140597855866048 [dir=none]
	140597855866048 [label="bias
 (512)" fillcolor=orange]
	140597367633328 -> 140597480519936 [dir=none]
	140597480519936 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367633328 -> 140597367637072 [dir=none]
	140597367637072 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367633328 -> 140597367636752 [dir=none]
	140597367636752 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367633328 -> 140597855865968 [dir=none]
	140597855865968 [label="weight
 (512)" fillcolor=orange]
	140597367633328 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367865504 -> 140597367633328
	140597367633424 -> 140597367633328
	140597855865968 [label="ct.encoder.layers.3.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597855865968 -> 140597367633424
	140597367633424 [label=AccumulateGrad]
	140597367633472 -> 140597367633328
	140597855866048 [label="ct.encoder.layers.3.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597855866048 -> 140597367633472
	140597367633472 [label=AccumulateGrad]
	140597367632800 -> 140597367632368
	140597855866128 [label="ct.encoder.layers.3.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597855866128 -> 140597367632800
	140597367632800 [label=AccumulateGrad]
	140597367631984 -> 140597367632368
	140597855866208 [label="ct.encoder.layers.3.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597855866208 -> 140597367631984
	140597367631984 [label=AccumulateGrad]
	140597367598960 -> 140597367596128
	140597855866288 [label="ct.encoder.layers.3.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597855866288 -> 140597367598960
	140597367598960 [label=AccumulateGrad]
	140597367632176 -> 140597367596128
	140597855866368 [label="ct.encoder.layers.3.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597855866368 -> 140597367632176
	140597367632176 [label=AccumulateGrad]
	140597367596656 -> 140597367596464
	140597855866448 [label="ct.encoder.layers.3.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597855866448 -> 140597367596656
	140597367596656 [label=AccumulateGrad]
	140597367595696 -> 140597367596464
	140597855866528 [label="ct.encoder.layers.3.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597855866528 -> 140597367595696
	140597367595696 [label=AccumulateGrad]
	140597367595936 -> 140597367865552
	140597855617088 [label="ct.encoder.layers.3.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597855617088 -> 140597367595936
	140597367595936 [label=AccumulateGrad]
	140597367595312 -> 140597367865552
	140597855617168 [label="ct.encoder.layers.3.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597855617168 -> 140597367595312
	140597367595312 [label=AccumulateGrad]
	140597367857056 -> 140597367856912
	140597367857056 -> 140597367637232 [dir=none]
	140597367637232 [label="other
 ()" fillcolor=orange]
	140597367857056 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367866032 -> 140597367857056
	140597367866032 -> 140597367637152 [dir=none]
	140597367637152 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367866032 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367596272 -> 140597367866032
	140597367596272 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367597184 -> 140597367596272
	140597367597184 -> 140597367637392 [dir=none]
	140597367637392 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367597184 -> 140597367636432 [dir=none]
	140597367636432 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367597184 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367632704 -> 140597367597184
	140597855618688 [label="ct.encoder.layers.3.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597855618688 -> 140597367632704
	140597367632704 [label=AccumulateGrad]
	140597367631936 -> 140597367597184
	140597367631936 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367633232 -> 140597367631936
	140597367633232 -> 140597367636992 [dir=none]
	140597367636992 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367633232 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367633568 -> 140597367633232
	140597367633568 -> 140597480519296 [dir=none]
	140597480519296 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367633568 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367633664 -> 140597367633568
	140597367633664 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367633760 -> 140597367633664
	140597367633760 -> 140597367636832 [dir=none]
	140597367636832 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367633760 -> 140597367637472 [dir=none]
	140597367637472 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367633760 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367633856 -> 140597367633760
	140597855618528 [label="ct.encoder.layers.3.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597855618528 -> 140597367633856
	140597367633856 [label=AccumulateGrad]
	140597367633808 -> 140597367633760
	140597367633808 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367633952 -> 140597367633808
	140597367633952 -> 140597855618368 [dir=none]
	140597855618368 [label="bias
 (512)" fillcolor=orange]
	140597367633952 -> 140597480520096 [dir=none]
	140597480520096 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367633952 -> 140597367637632 [dir=none]
	140597367637632 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367633952 -> 140597367637552 [dir=none]
	140597367637552 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367633952 -> 140597855618288 [dir=none]
	140597855618288 [label="weight
 (512)" fillcolor=orange]
	140597367633952 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367857104 -> 140597367633952
	140597367634144 -> 140597367633952
	140597855618288 [label="ct.encoder.layers.3.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597855618288 -> 140597367634144
	140597367634144 [label=AccumulateGrad]
	140597367634096 -> 140597367633952
	140597855618368 [label="ct.encoder.layers.3.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597855618368 -> 140597367634096
	140597367634096 [label=AccumulateGrad]
	140597367632416 -> 140597367633760
	140597367632416 [label=TBackward0]
	140597367634192 -> 140597367632416
	140597855618448 [label="ct.encoder.layers.3.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597855618448 -> 140597367634192
	140597367634192 [label=AccumulateGrad]
	140597367632080 -> 140597367597184
	140597367632080 [label=TBackward0]
	140597367633616 -> 140597367632080
	140597855618608 [label="ct.encoder.layers.3.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597855618608 -> 140597367633616
	140597367633616 [label=AccumulateGrad]
	140597367856864 -> 140597367856768
	140597855618768 [label="ct.encoder.layers.3.norm_out.weight
 (512)" fillcolor=lightblue]
	140597855618768 -> 140597367856864
	140597367856864 [label=AccumulateGrad]
	140597367856816 -> 140597367856768
	140597855618848 [label="ct.encoder.layers.3.norm_out.bias
 (512)" fillcolor=lightblue]
	140597855618848 -> 140597367856816
	140597367856816 [label=AccumulateGrad]
	140597367856720 -> 140597367856624
	140597367856720 -> 140597367637792 [dir=none]
	140597367637792 [label="other
 ()" fillcolor=orange]
	140597367856720 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367865408 -> 140597367856720
	140597367865408 -> 140597367637712 [dir=none]
	140597367637712 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367865408 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367595648 -> 140597367865408
	140597367595648 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367856960 -> 140597367595648
	140597367856960 -> 140597367637952 [dir=none]
	140597367637952 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367856960 -> 140597367636912 [dir=none]
	140597367636912 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367856960 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367633712 -> 140597367856960
	140597855619328 [label="ct.encoder.layers.4.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597855619328 -> 140597367633712
	140597367633712 [label=AccumulateGrad]
	140597367633904 -> 140597367856960
	140597367633904 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367634048 -> 140597367633904
	140597367634048 -> 140597367638112 [dir=none]
	140597367638112 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367634048 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367634336 -> 140597367634048
	140597367634336 -> 140597480520816 [dir=none]
	140597480520816 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367634336 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367634432 -> 140597367634336
	140597367634432 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367634528 -> 140597367634432
	140597367634528 -> 140597367638192 [dir=none]
	140597367638192 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367634528 -> 140597367638032 [dir=none]
	140597367638032 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367634528 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367634624 -> 140597367634528
	140597855619168 [label="ct.encoder.layers.4.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597855619168 -> 140597367634624
	140597367634624 [label=AccumulateGrad]
	140597367634576 -> 140597367634528
	140597367634576 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367634720 -> 140597367634576
	140597367634720 -> 140597855619008 [dir=none]
	140597855619008 [label="bias
 (512)" fillcolor=orange]
	140597367634720 -> 140597480520736 [dir=none]
	140597480520736 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367634720 -> 140597367638352 [dir=none]
	140597367638352 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367634720 -> 140597367638272 [dir=none]
	140597367638272 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367634720 -> 140597855618928 [dir=none]
	140597855618928 [label="weight
 (512)" fillcolor=orange]
	140597367634720 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367856768 -> 140597367634720
	140597367634912 -> 140597367634720
	140597855618928 [label="ct.encoder.layers.4.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597855618928 -> 140597367634912
	140597367634912 [label=AccumulateGrad]
	140597367634864 -> 140597367634720
	140597855619008 [label="ct.encoder.layers.4.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597855619008 -> 140597367634864
	140597367634864 [label=AccumulateGrad]
	140597367634240 -> 140597367634528
	140597367634240 [label=TBackward0]
	140597367634960 -> 140597367634240
	140597855619088 [label="ct.encoder.layers.4.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597855619088 -> 140597367634960
	140597367634960 [label=AccumulateGrad]
	140597367632992 -> 140597367856960
	140597367632992 [label=TBackward0]
	140597367634384 -> 140597367632992
	140597855619248 [label="ct.encoder.layers.4.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597855619248 -> 140597367634384
	140597367634384 [label=AccumulateGrad]
	140597367856576 -> 140597367856480
	140597367856576 -> 140597367638512 [dir=none]
	140597367638512 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367856576 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367595456 -> 140597367856576
	140597367595456 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367856672 -> 140597367595456
	140597367856672 -> 140597367638592 [dir=none]
	140597367638592 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367856672 -> 140597367637872 [dir=none]
	140597367637872 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367856672 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367634672 -> 140597367856672
	140597854486672 [label="ct.encoder.layers.4.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597854486672 -> 140597367634672
	140597367634672 [label=AccumulateGrad]
	140597367634000 -> 140597367856672
	140597367634000 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367634288 -> 140597367634000
	140597367634288 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367634768 -> 140597367634288
	140597367634768 [label=CloneBackward0]
	140597367635152 -> 140597367634768
	140597367635152 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367635248 -> 140597367635152
	140597367635248 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367635344 -> 140597367635248
	140597367635344 -> 140597367638752 [dir=none]
	140597367638752 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367635344 -> 140597367638672 [dir=none]
	140597367638672 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367635344 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367635440 -> 140597367635344
	140597367635440 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367635584 -> 140597367635440
	140597367635584 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367635680 -> 140597367635584
	140597367635680 -> 140597367638432 [dir=none]
	140597367638432 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367635680 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367635776 -> 140597367635680
	140597367635776 -> 140597480522352 [dir=none]
	140597480522352 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367635776 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367635872 -> 140597367635776
	140597367635872 -> 140597367639072 [dir=none]
	140597367639072 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367635872 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367635920 -> 140597367635872
	140597367635920 -> 140597480522352 [dir=none]
	140597480522352 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367635920 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367693472 -> 140597367635920
	140597367693472 -> 140597367638992 [dir=none]
	140597367638992 [label="other
 ()" fillcolor=orange]
	140597367693472 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367693568 -> 140597367693472
	140597367693568 [label="AddBackward0
------------
alpha: 1"]
	140597367693664 -> 140597367693568
	140597367693664 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367693808 -> 140597367693664
	140597367693808 -> 140597367638912 [dir=none]
	140597367638912 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367693808 -> 140597367638832 [dir=none]
	140597367638832 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367693808 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367693904 -> 140597367693808
	140597367693904 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367694048 -> 140597367693904
	140597367694048 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367694144 -> 140597367694048
	140597367694144 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367694240 -> 140597367694144
	140597367694240 [label="AddBackward0
------------
alpha: 1"]
	140597367694336 -> 140597367694240
	140597367694336 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367694480 -> 140597367694336
	140597367694480 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367694576 -> 140597367694480
	140597367694576 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367694672 -> 140597367694576
	140597367694672 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367694768 -> 140597367694672
	140597367694768 -> 140597367636032 [dir=none]
	140597367636032 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367694768 -> 140597367639312 [dir=none]
	140597367639312 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367694768 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367694864 -> 140597367694768
	140597855620688 [label="ct.encoder.layers.4.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597855620688 -> 140597367694864
	140597367694864 [label=AccumulateGrad]
	140597367694816 -> 140597367694768
	140597367694816 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367694960 -> 140597367694816
	140597367694960 -> 140597855620528 [dir=none]
	140597855620528 [label="bias
 (512)" fillcolor=orange]
	140597367694960 -> 140597480521056 [dir=none]
	140597480521056 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367694960 -> 140597367639392 [dir=none]
	140597367639392 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367694960 -> 140597367637312 [dir=none]
	140597367637312 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367694960 -> 140597855620448 [dir=none]
	140597855620448 [label="weight
 (512)" fillcolor=orange]
	140597367694960 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367856624 -> 140597367694960
	140597367695152 -> 140597367694960
	140597855620448 [label="ct.encoder.layers.4.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597855620448 -> 140597367695152
	140597367695152 [label=AccumulateGrad]
	140597367695104 -> 140597367694960
	140597855620528 [label="ct.encoder.layers.4.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597855620528 -> 140597367695104
	140597367695104 [label=AccumulateGrad]
	140597367694384 -> 140597367694768
	140597367694384 [label=TBackward0]
	140597367695200 -> 140597367694384
	140597855620608 [label="ct.encoder.layers.4.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597855620608 -> 140597367695200
	140597367695200 [label=AccumulateGrad]
	140597367694288 -> 140597367694240
	140597854486832 [label="ct.encoder.layers.4.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597854486832 -> 140597367694288
	140597367694288 [label=AccumulateGrad]
	140597367693856 -> 140597367693808
	140597367693856 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367694192 -> 140597367693856
	140597367694192 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367694624 -> 140597367694192
	140597367694624 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367694720 -> 140597367694624
	140597367694720 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367694912 -> 140597367694720
	140597367694912 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367695248 -> 140597367694912
	140597367695248 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367695344 -> 140597367695248
	140597367695344 -> 140597367639552 [dir=none]
	140597367639552 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367695344 -> 140597367639152 [dir=none]
	140597367639152 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367695344 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367695440 -> 140597367695344
	140597855620848 [label="ct.encoder.layers.4.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597855620848 -> 140597367695440
	140597367695440 [label=AccumulateGrad]
	140597367695392 -> 140597367695344
	140597367695392 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367694960 -> 140597367695392
	140597367694000 -> 140597367695344
	140597367694000 [label=TBackward0]
	140597367695632 -> 140597367694000
	140597855620768 [label="ct.encoder.layers.4.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597855620768 -> 140597367695632
	140597367695632 [label=AccumulateGrad]
	140597367693616 -> 140597367693568
	140597367693616 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597367694096 -> 140597367693616
	140597367694096 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367694432 -> 140597367694096
	140597367694432 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367695296 -> 140597367694432
	140597367695296 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367695008 -> 140597367695296
	140597367695008 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367695584 -> 140597367695008
	140597367695584 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367695680 -> 140597367695584
	140597367695680 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367695776 -> 140597367695680
	140597367695776 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367695872 -> 140597367695776
	140597367695872 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367695968 -> 140597367695872
	140597367695968 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367696064 -> 140597367695968
	140597367696064 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367696160 -> 140597367696064
	140597367696160 -> 140597367639472 [dir=none]
	140597367639472 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367696160 -> 140597367639792 [dir=none]
	140597367639792 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367696160 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367696256 -> 140597367696160
	140597367696256 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367696400 -> 140597367696256
	140597367696400 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367696496 -> 140597367696400
	140597367696496 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367696592 -> 140597367696496
	140597367696592 [label="AddBackward0
------------
alpha: 1"]
	140597367694336 -> 140597367696592
	140597367696688 -> 140597367696592
	140597854486912 [label="ct.encoder.layers.4.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597854486912 -> 140597367696688
	140597367696688 [label=AccumulateGrad]
	140597367696208 -> 140597367696160
	140597367696208 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367696544 -> 140597367696208
	140597367696544 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367696784 -> 140597367696544
	140597367696784 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367696832 -> 140597367696784
	140597367696832 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367696880 -> 140597367696832
	140597367696880 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597367697024 -> 140597367696880
	140597367697024 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597367697120 -> 140597367697024
	140597367697120 -> 140597367639632 [dir=none]
	140597367639632 [label="self
 (251, 512)" fillcolor=orange]
	140597367697120 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597367697216 -> 140597367697120
	140597367697216 [label=TBackward0]
	140597367697312 -> 140597367697216
	140597854486752 [label="ct.encoder.layers.4.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597854486752 -> 140597367697312
	140597367697312 [label=AccumulateGrad]
	140597367635392 -> 140597367635344
	140597367635392 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367635728 -> 140597367635392
	140597367635728 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367635488 -> 140597367635728
	140597367635488 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367635536 -> 140597367635488
	140597367635536 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367693712 -> 140597367635536
	140597367693712 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367694528 -> 140597367693712
	140597367694528 -> 140597367639232 [dir=none]
	140597367639232 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367694528 -> 140597367639872 [dir=none]
	140597367639872 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367694528 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367695488 -> 140597367694528
	140597855621008 [label="ct.encoder.layers.4.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597855621008 -> 140597367695488
	140597367695488 [label=AccumulateGrad]
	140597367695056 -> 140597367694528
	140597367695056 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367694960 -> 140597367695056
	140597367693424 -> 140597367694528
	140597367693424 [label=TBackward0]
	140597367695920 -> 140597367693424
	140597855620928 [label="ct.encoder.layers.4.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597855620928 -> 140597367695920
	140597367695920 [label=AccumulateGrad]
	140597367633520 -> 140597367856672
	140597367633520 [label=TBackward0]
	140597367635104 -> 140597367633520
	140597854486592 [label="ct.encoder.layers.4.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597854486592 -> 140597367635104
	140597367635104 [label=AccumulateGrad]
	140597367856432 -> 140597367856336
	140597367856432 -> 140597367639952 [dir=none]
	140597367639952 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367856432 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367857008 -> 140597367856432
	140597367857008 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367635008 -> 140597367857008
	140597367635008 -> 140597480521952 [dir=none]
	140597480521952 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367635008 -> 140597855620288 [dir=none]
	140597855620288 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597367635008 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367635200 -> 140597367635008
	140597367635200 -> 140597480522112 [dir=none]
	140597480522112 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367635200 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367635824 -> 140597367635200
	140597367635824 -> 140597480521376 [dir=none]
	140597480521376 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367635824 -> 140597367639712 [dir=none]
	140597367639712 [label="result1
 (512)" fillcolor=orange]
	140597367635824 -> 140597367197840 [dir=none]
	140597367197840 [label="result2
 (512)" fillcolor=orange]
	140597367635824 -> 140597367197760 [dir=none]
	140597367197760 [label="result3
 (0)" fillcolor=orange]
	140597367635824 -> 140597857880640 [dir=none]
	140597857880640 [label="running_mean
 (512)" fillcolor=orange]
	140597367635824 -> 140597855866768 [dir=none]
	140597855866768 [label="running_var
 (512)" fillcolor=orange]
	140597367635824 -> 140597855619888 [dir=none]
	140597855619888 [label="weight
 (512)" fillcolor=orange]
	140597367635824 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367635056 -> 140597367635824
	140597367635056 -> 140597480522192 [dir=none]
	140597480522192 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367635056 -> 140597855619728 [dir=none]
	140597855619728 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367635056 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367696112 -> 140597367635056
	140597367696112 -> 140597480522592 [dir=none]
	140597480522592 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367696112 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367696448 -> 140597367696112
	140597367696448 -> 140597480521216 [dir=none]
	140597480521216 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367696448 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367696304 -> 140597367696448
	140597367696304 -> 140597480521296 [dir=none]
	140597480521296 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367696304 -> 140597855619568 [dir=none]
	140597855619568 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367696304 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367696976 -> 140597367696304
	140597367696976 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367697264 -> 140597367696976
	140597367697264 -> 140597855619488 [dir=none]
	140597855619488 [label="bias
 (512)" fillcolor=orange]
	140597367697264 -> 140597480521536 [dir=none]
	140597480521536 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367697264 -> 140597367198400 [dir=none]
	140597367198400 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367697264 -> 140597367198000 [dir=none]
	140597367198000 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367697264 -> 140597855619408 [dir=none]
	140597855619408 [label="weight
 (512)" fillcolor=orange]
	140597367697264 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367856480 -> 140597367697264
	140597367696352 -> 140597367697264
	140597855619408 [label="ct.encoder.layers.4.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597855619408 -> 140597367696352
	140597367696352 [label=AccumulateGrad]
	140597367697360 -> 140597367697264
	140597855619488 [label="ct.encoder.layers.4.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597855619488 -> 140597367697360
	140597367697360 [label=AccumulateGrad]
	140597367696736 -> 140597367696304
	140597855619568 [label="ct.encoder.layers.4.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597855619568 -> 140597367696736
	140597367696736 [label=AccumulateGrad]
	140597367695536 -> 140597367696304
	140597855619648 [label="ct.encoder.layers.4.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597855619648 -> 140597367695536
	140597367695536 [label=AccumulateGrad]
	140597367695824 -> 140597367635056
	140597855619728 [label="ct.encoder.layers.4.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597855619728 -> 140597367695824
	140597367695824 [label=AccumulateGrad]
	140597367695728 -> 140597367635056
	140597855619808 [label="ct.encoder.layers.4.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597855619808 -> 140597367695728
	140597367695728 [label=AccumulateGrad]
	140597367693376 -> 140597367635824
	140597855619888 [label="ct.encoder.layers.4.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597855619888 -> 140597367693376
	140597367693376 [label=AccumulateGrad]
	140597367693520 -> 140597367635824
	140597855619968 [label="ct.encoder.layers.4.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597855619968 -> 140597367693520
	140597367693520 [label=AccumulateGrad]
	140597367635296 -> 140597367635008
	140597855620288 [label="ct.encoder.layers.4.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597855620288 -> 140597367635296
	140597367635296 [label=AccumulateGrad]
	140597367633136 -> 140597367635008
	140597855620368 [label="ct.encoder.layers.4.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597855620368 -> 140597367633136
	140597367633136 [label=AccumulateGrad]
	140597367856288 -> 140597367856144
	140597367856288 -> 140597367198560 [dir=none]
	140597367198560 [label="other
 ()" fillcolor=orange]
	140597367856288 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367856528 -> 140597367856288
	140597367856528 -> 140597367198480 [dir=none]
	140597367198480 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367856528 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367635632 -> 140597367856528
	140597367635632 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367634480 -> 140597367635632
	140597367634480 -> 140597367198720 [dir=none]
	140597367198720 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367634480 -> 140597367198320 [dir=none]
	140597367198320 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367634480 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367696640 -> 140597367634480
	140597854487392 [label="ct.encoder.layers.4.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597854487392 -> 140597367696640
	140597367696640 [label=AccumulateGrad]
	140597367693760 -> 140597367634480
	140597367693760 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367697168 -> 140597367693760
	140597367697168 -> 140597367198240 [dir=none]
	140597367198240 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367697168 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367201952 -> 140597367697168
	140597367201952 -> 140597480521872 [dir=none]
	140597480521872 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367201952 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367202048 -> 140597367201952
	140597367202048 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367202144 -> 140597367202048
	140597367202144 -> 140597367198080 [dir=none]
	140597367198080 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367202144 -> 140597367198800 [dir=none]
	140597367198800 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367202144 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367202240 -> 140597367202144
	140597854487232 [label="ct.encoder.layers.4.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854487232 -> 140597367202240
	140597367202240 [label=AccumulateGrad]
	140597367202192 -> 140597367202144
	140597367202192 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367202336 -> 140597367202192
	140597367202336 -> 140597854487072 [dir=none]
	140597854487072 [label="bias
 (512)" fillcolor=orange]
	140597367202336 -> 140597480522272 [dir=none]
	140597480522272 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367202336 -> 140597367198960 [dir=none]
	140597367198960 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367202336 -> 140597367198880 [dir=none]
	140597367198880 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367202336 -> 140597854486992 [dir=none]
	140597854486992 [label="weight
 (512)" fillcolor=orange]
	140597367202336 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367856336 -> 140597367202336
	140597367202528 -> 140597367202336
	140597854486992 [label="ct.encoder.layers.4.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597854486992 -> 140597367202528
	140597367202528 [label=AccumulateGrad]
	140597367202480 -> 140597367202336
	140597854487072 [label="ct.encoder.layers.4.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597854487072 -> 140597367202480
	140597367202480 [label=AccumulateGrad]
	140597367201856 -> 140597367202144
	140597367201856 [label=TBackward0]
	140597367202576 -> 140597367201856
	140597854487152 [label="ct.encoder.layers.4.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854487152 -> 140597367202576
	140597367202576 [label=AccumulateGrad]
	140597367693952 -> 140597367634480
	140597367693952 [label=TBackward0]
	140597367697072 -> 140597367693952
	140597854487312 [label="ct.encoder.layers.4.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854487312 -> 140597367697072
	140597367697072 [label=AccumulateGrad]
	140597367856096 -> 140597367856000
	140597854487472 [label="ct.encoder.layers.4.norm_out.weight
 (512)" fillcolor=lightblue]
	140597854487472 -> 140597367856096
	140597367856096 [label=AccumulateGrad]
	140597367856048 -> 140597367856000
	140597854487552 [label="ct.encoder.layers.4.norm_out.bias
 (512)" fillcolor=lightblue]
	140597854487552 -> 140597367856048
	140597367856048 [label=AccumulateGrad]
	140597367855952 -> 140597367855856
	140597367855952 -> 140597367199120 [dir=none]
	140597367199120 [label="other
 ()" fillcolor=orange]
	140597367855952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367856384 -> 140597367855952
	140597367856384 -> 140597367199040 [dir=none]
	140597367199040 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367856384 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367634816 -> 140597367856384
	140597367634816 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367696928 -> 140597367634816
	140597367696928 -> 140597367199280 [dir=none]
	140597367199280 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367696928 -> 140597367198160 [dir=none]
	140597367198160 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367696928 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367202096 -> 140597367696928
	140597854488032 [label="ct.encoder.layers.5.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597854488032 -> 140597367202096
	140597367202096 [label=AccumulateGrad]
	140597367202288 -> 140597367696928
	140597367202288 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367202432 -> 140597367202288
	140597367202432 -> 140597367199440 [dir=none]
	140597367199440 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367202432 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367202720 -> 140597367202432
	140597367202720 -> 140597480522992 [dir=none]
	140597480522992 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367202720 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367202816 -> 140597367202720
	140597367202816 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367202912 -> 140597367202816
	140597367202912 -> 140597367199520 [dir=none]
	140597367199520 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367202912 -> 140597367199360 [dir=none]
	140597367199360 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367202912 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367203008 -> 140597367202912
	140597854487872 [label="ct.encoder.layers.5.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854487872 -> 140597367203008
	140597367203008 [label=AccumulateGrad]
	140597367202960 -> 140597367202912
	140597367202960 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367203104 -> 140597367202960
	140597367203104 -> 140597854487712 [dir=none]
	140597854487712 [label="bias
 (512)" fillcolor=orange]
	140597367203104 -> 140597480522912 [dir=none]
	140597480522912 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367203104 -> 140597367199680 [dir=none]
	140597367199680 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367203104 -> 140597367199600 [dir=none]
	140597367199600 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367203104 -> 140597854487632 [dir=none]
	140597854487632 [label="weight
 (512)" fillcolor=orange]
	140597367203104 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367856000 -> 140597367203104
	140597367203296 -> 140597367203104
	140597854487632 [label="ct.encoder.layers.5.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597854487632 -> 140597367203296
	140597367203296 [label=AccumulateGrad]
	140597367203248 -> 140597367203104
	140597854487712 [label="ct.encoder.layers.5.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597854487712 -> 140597367203248
	140597367203248 [label=AccumulateGrad]
	140597367202624 -> 140597367202912
	140597367202624 [label=TBackward0]
	140597367203344 -> 140597367202624
	140597854487792 [label="ct.encoder.layers.5.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854487792 -> 140597367203344
	140597367203344 [label=AccumulateGrad]
	140597367202000 -> 140597367696928
	140597367202000 [label=TBackward0]
	140597367202768 -> 140597367202000
	140597854487952 [label="ct.encoder.layers.5.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854487952 -> 140597367202768
	140597367202768 [label=AccumulateGrad]
	140597367855808 -> 140597367855712
	140597367855808 -> 140597367199840 [dir=none]
	140597367199840 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367855808 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367856192 -> 140597367855808
	140597367856192 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367696016 -> 140597367856192
	140597367696016 -> 140597367199920 [dir=none]
	140597367199920 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367696016 -> 140597367199200 [dir=none]
	140597367199200 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367696016 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367203056 -> 140597367696016
	140597854489872 [label="ct.encoder.layers.5.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597854489872 -> 140597367203056
	140597367203056 [label=AccumulateGrad]
	140597367202384 -> 140597367696016
	140597367202384 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367202672 -> 140597367202384
	140597367202672 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367203152 -> 140597367202672
	140597367203152 [label=CloneBackward0]
	140597367203536 -> 140597367203152
	140597367203536 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367203632 -> 140597367203536
	140597367203632 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367203728 -> 140597367203632
	140597367203728 -> 140597367200080 [dir=none]
	140597367200080 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367203728 -> 140597367200000 [dir=none]
	140597367200000 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367203728 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367203824 -> 140597367203728
	140597367203824 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367203968 -> 140597367203824
	140597367203968 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367204064 -> 140597367203968
	140597367204064 -> 140597367199760 [dir=none]
	140597367199760 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367204064 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367204160 -> 140597367204064
	140597367204160 -> 140597480524432 [dir=none]
	140597480524432 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367204160 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367204256 -> 140597367204160
	140597367204256 -> 140597367200400 [dir=none]
	140597367200400 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367204256 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367204352 -> 140597367204256
	140597367204352 -> 140597480524432 [dir=none]
	140597480524432 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367204352 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367204448 -> 140597367204352
	140597367204448 -> 140597367200320 [dir=none]
	140597367200320 [label="other
 ()" fillcolor=orange]
	140597367204448 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367204544 -> 140597367204448
	140597367204544 [label="AddBackward0
------------
alpha: 1"]
	140597367204640 -> 140597367204544
	140597367204640 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367204784 -> 140597367204640
	140597367204784 -> 140597367200240 [dir=none]
	140597367200240 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367204784 -> 140597367200160 [dir=none]
	140597367200160 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367204784 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367204880 -> 140597367204784
	140597367204880 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367205024 -> 140597367204880
	140597367205024 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367205120 -> 140597367205024
	140597367205120 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367205216 -> 140597367205120
	140597367205216 [label="AddBackward0
------------
alpha: 1"]
	140597367205312 -> 140597367205216
	140597367205312 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367205456 -> 140597367205312
	140597367205456 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367205552 -> 140597367205456
	140597367205552 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367205648 -> 140597367205552
	140597367205648 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367205744 -> 140597367205648
	140597367205744 -> 140597367197920 [dir=none]
	140597367197920 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367205744 -> 140597367200640 [dir=none]
	140597367200640 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367205744 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367205840 -> 140597367205744
	140597854489392 [label="ct.encoder.layers.5.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597854489392 -> 140597367205840
	140597367205840 [label=AccumulateGrad]
	140597367205792 -> 140597367205744
	140597367205792 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367251056 -> 140597367205792
	140597367251056 -> 140597854489232 [dir=none]
	140597854489232 [label="bias
 (512)" fillcolor=orange]
	140597367251056 -> 140597480523232 [dir=none]
	140597480523232 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367251056 -> 140597367200720 [dir=none]
	140597367200720 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367251056 -> 140597367198640 [dir=none]
	140597367198640 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367251056 -> 140597854489152 [dir=none]
	140597854489152 [label="weight
 (512)" fillcolor=orange]
	140597367251056 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367855856 -> 140597367251056
	140597367251248 -> 140597367251056
	140597854489152 [label="ct.encoder.layers.5.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597854489152 -> 140597367251248
	140597367251248 [label=AccumulateGrad]
	140597367251200 -> 140597367251056
	140597854489232 [label="ct.encoder.layers.5.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597854489232 -> 140597367251200
	140597367251200 [label=AccumulateGrad]
	140597367205360 -> 140597367205744
	140597367205360 [label=TBackward0]
	140597367251296 -> 140597367205360
	140597854489312 [label="ct.encoder.layers.5.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597854489312 -> 140597367251296
	140597367251296 [label=AccumulateGrad]
	140597367205264 -> 140597367205216
	140597856969888 [label="ct.encoder.layers.5.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597856969888 -> 140597367205264
	140597367205264 [label=AccumulateGrad]
	140597367204832 -> 140597367204784
	140597367204832 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367205168 -> 140597367204832
	140597367205168 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367205600 -> 140597367205168
	140597367205600 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367205696 -> 140597367205600
	140597367205696 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367204976 -> 140597367205696
	140597367204976 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367251344 -> 140597367204976
	140597367251344 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367251440 -> 140597367251344
	140597367251440 -> 140597367200880 [dir=none]
	140597367200880 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367251440 -> 140597367200480 [dir=none]
	140597367200480 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367251440 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367251536 -> 140597367251440
	140597854489552 [label="ct.encoder.layers.5.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597854489552 -> 140597367251536
	140597367251536 [label=AccumulateGrad]
	140597367251488 -> 140597367251440
	140597367251488 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367251056 -> 140597367251488
	140597367251392 -> 140597367251440
	140597367251392 [label=TBackward0]
	140597367251728 -> 140597367251392
	140597854489472 [label="ct.encoder.layers.5.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597854489472 -> 140597367251728
	140597367251728 [label=AccumulateGrad]
	140597367204592 -> 140597367204544
	140597367204592 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597367205072 -> 140597367204592
	140597367205072 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367205408 -> 140597367205072
	140597367205408 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367205504 -> 140597367205408
	140597367205504 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367251104 -> 140597367205504
	140597367251104 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367251680 -> 140597367251104
	140597367251680 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367251776 -> 140597367251680
	140597367251776 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367251872 -> 140597367251776
	140597367251872 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367251968 -> 140597367251872
	140597367251968 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367252064 -> 140597367251968
	140597367252064 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367252160 -> 140597367252064
	140597367252160 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367252256 -> 140597367252160
	140597367252256 -> 140597367201440 [dir=none]
	140597367201440 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367252256 -> 140597367200560 [dir=none]
	140597367200560 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367252256 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367252352 -> 140597367252256
	140597367252352 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367252496 -> 140597367252352
	140597367252496 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367252592 -> 140597367252496
	140597367252592 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367252688 -> 140597367252592
	140597367252688 [label="AddBackward0
------------
alpha: 1"]
	140597367205312 -> 140597367252688
	140597367252784 -> 140597367252688
	140597856969968 [label="ct.encoder.layers.5.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597856969968 -> 140597367252784
	140597367252784 [label=AccumulateGrad]
	140597367252304 -> 140597367252256
	140597367252304 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367252640 -> 140597367252304
	140597367252640 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367252880 -> 140597367252640
	140597367252880 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367252928 -> 140597367252880
	140597367252928 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367252976 -> 140597367252928
	140597367252976 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597367253120 -> 140597367252976
	140597367253120 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597367253216 -> 140597367253120
	140597367253216 -> 140597367200960 [dir=none]
	140597367200960 [label="self
 (251, 512)" fillcolor=orange]
	140597367253216 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597367253312 -> 140597367253216
	140597367253312 [label=TBackward0]
	140597367253408 -> 140597367253312
	140597856969808 [label="ct.encoder.layers.5.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597856969808 -> 140597367253408
	140597367253408 [label=AccumulateGrad]
	140597367203776 -> 140597367203728
	140597367203776 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367204112 -> 140597367203776
	140597367204112 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367204304 -> 140597367204112
	140597367204304 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367204496 -> 140597367204304
	140597367204496 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367204688 -> 140597367204496
	140597367204688 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367204736 -> 140597367204688
	140597367204736 -> 140597367201280 [dir=none]
	140597367201280 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367204736 -> 140597367201200 [dir=none]
	140597367201200 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367204736 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367203920 -> 140597367204736
	140597854489712 [label="ct.encoder.layers.5.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597854489712 -> 140597367203920
	140597367203920 [label=AccumulateGrad]
	140597367251584 -> 140597367204736
	140597367251584 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367251056 -> 140597367251584
	140597367251152 -> 140597367204736
	140597367251152 [label=TBackward0]
	140597367252016 -> 140597367251152
	140597854489632 [label="ct.encoder.layers.5.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597854489632 -> 140597367252016
	140597367252016 [label=AccumulateGrad]
	140597367201904 -> 140597367696016
	140597367201904 [label=TBackward0]
	140597367203488 -> 140597367201904
	140597854489792 [label="ct.encoder.layers.5.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597854489792 -> 140597367203488
	140597367203488 [label=AccumulateGrad]
	140597367855664 -> 140597367855568
	140597367855664 -> 140597367201120 [dir=none]
	140597367201120 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367855664 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367855904 -> 140597367855664
	140597367855904 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367855760 -> 140597367855904
	140597367855760 -> 140597480523472 [dir=none]
	140597480523472 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367855760 -> 140597854488992 [dir=none]
	140597854488992 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597367855760 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367203584 -> 140597367855760
	140597367203584 -> 140597480523712 [dir=none]
	140597480523712 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367203584 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367204208 -> 140597367203584
	140597367204208 -> 140597480524112 [dir=none]
	140597480524112 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367204208 -> 140597367200800 [dir=none]
	140597367200800 [label="result1
 (512)" fillcolor=orange]
	140597367204208 -> 140597367201600 [dir=none]
	140597367201600 [label="result2
 (512)" fillcolor=orange]
	140597367204208 -> 140597367201040 [dir=none]
	140597367201040 [label="result3
 (0)" fillcolor=orange]
	140597367204208 -> 140597855866688 [dir=none]
	140597855866688 [label="running_mean
 (512)" fillcolor=orange]
	140597367204208 -> 140597855620208 [dir=none]
	140597855620208 [label="running_var
 (512)" fillcolor=orange]
	140597367204208 -> 140597854488592 [dir=none]
	140597854488592 [label="weight
 (512)" fillcolor=orange]
	140597367204208 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367203872 -> 140597367204208
	140597367203872 -> 140597480523792 [dir=none]
	140597480523792 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367203872 -> 140597854488432 [dir=none]
	140597854488432 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367203872 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367252208 -> 140597367203872
	140597367252208 -> 140597480523952 [dir=none]
	140597480523952 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367252208 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367252544 -> 140597367252208
	140597367252544 -> 140597480524032 [dir=none]
	140597480524032 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367252544 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367252400 -> 140597367252544
	140597367252400 -> 140597480524672 [dir=none]
	140597480524672 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367252400 -> 140597854488272 [dir=none]
	140597854488272 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367252400 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367253072 -> 140597367252400
	140597367253072 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367253360 -> 140597367253072
	140597367253360 -> 140597854488192 [dir=none]
	140597854488192 [label="bias
 (512)" fillcolor=orange]
	140597367253360 -> 140597480524192 [dir=none]
	140597480524192 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367253360 -> 140597367201360 [dir=none]
	140597367201360 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367253360 -> 140597367201520 [dir=none]
	140597367201520 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367253360 -> 140597854488112 [dir=none]
	140597854488112 [label="weight
 (512)" fillcolor=orange]
	140597367253360 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367855712 -> 140597367253360
	140597367253456 -> 140597367253360
	140597854488112 [label="ct.encoder.layers.5.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597854488112 -> 140597367253456
	140597367253456 [label=AccumulateGrad]
	140597367253504 -> 140597367253360
	140597854488192 [label="ct.encoder.layers.5.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597854488192 -> 140597367253504
	140597367253504 [label=AccumulateGrad]
	140597367252832 -> 140597367252400
	140597854488272 [label="ct.encoder.layers.5.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597854488272 -> 140597367252832
	140597367252832 [label=AccumulateGrad]
	140597367251632 -> 140597367252400
	140597854488352 [label="ct.encoder.layers.5.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597854488352 -> 140597367251632
	140597367251632 [label=AccumulateGrad]
	140597367251920 -> 140597367203872
	140597854488432 [label="ct.encoder.layers.5.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597854488432 -> 140597367251920
	140597367251920 [label=AccumulateGrad]
	140597367251824 -> 140597367203872
	140597854488512 [label="ct.encoder.layers.5.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597854488512 -> 140597367251824
	140597367251824 [label=AccumulateGrad]
	140597367204400 -> 140597367204208
	140597854488592 [label="ct.encoder.layers.5.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597854488592 -> 140597367204400
	140597367204400 [label=AccumulateGrad]
	140597367203440 -> 140597367204208
	140597854488672 [label="ct.encoder.layers.5.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597854488672 -> 140597367203440
	140597367203440 [label=AccumulateGrad]
	140597367203680 -> 140597367855760
	140597854488992 [label="ct.encoder.layers.5.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597854488992 -> 140597367203680
	140597367203680 [label=AccumulateGrad]
	140597367202864 -> 140597367855760
	140597854489072 [label="ct.encoder.layers.5.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597854489072 -> 140597367202864
	140597367202864 [label=AccumulateGrad]
	140597367855520 -> 140597367855376
	140597367855520 -> 140597367201680 [dir=none]
	140597367201680 [label="other
 ()" fillcolor=orange]
	140597367855520 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367856240 -> 140597367855520
	140597367856240 -> 140597367287872 [dir=none]
	140597367287872 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367856240 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367204016 -> 140597367856240
	140597367204016 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367204928 -> 140597367204016
	140597367204928 -> 140597367288192 [dir=none]
	140597367288192 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367204928 -> 140597367288112 [dir=none]
	140597367288112 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367204928 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367252736 -> 140597367204928
	140597856970928 [label="ct.encoder.layers.5.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597856970928 -> 140597367252736
	140597367252736 [label=AccumulateGrad]
	140597367251008 -> 140597367204928
	140597367251008 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367253264 -> 140597367251008
	140597367253264 -> 140597367288432 [dir=none]
	140597367288432 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367253264 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367253600 -> 140597367253264
	140597367253600 -> 140597480523552 [dir=none]
	140597480523552 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367253600 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367253696 -> 140597367253600
	140597367253696 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367253792 -> 140597367253696
	140597367253792 -> 140597367288512 [dir=none]
	140597367288512 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367253792 -> 140597367288272 [dir=none]
	140597367288272 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367253792 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367253888 -> 140597367253792
	140597856971088 [label="ct.encoder.layers.5.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597856971088 -> 140597367253888
	140597367253888 [label=AccumulateGrad]
	140597367253840 -> 140597367253792
	140597367253840 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367253984 -> 140597367253840
	140597367253984 -> 140597856969648 [dir=none]
	140597856969648 [label="bias
 (512)" fillcolor=orange]
	140597367253984 -> 140597480524352 [dir=none]
	140597480524352 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367253984 -> 140597367288672 [dir=none]
	140597367288672 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367253984 -> 140597367288592 [dir=none]
	140597367288592 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367253984 -> 140597856969568 [dir=none]
	140597856969568 [label="weight
 (512)" fillcolor=orange]
	140597367253984 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367855568 -> 140597367253984
	140597367254176 -> 140597367253984
	140597856969568 [label="ct.encoder.layers.5.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597856969568 -> 140597367254176
	140597367254176 [label=AccumulateGrad]
	140597367254128 -> 140597367253984
	140597856969648 [label="ct.encoder.layers.5.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597856969648 -> 140597367254128
	140597367254128 [label=AccumulateGrad]
	140597367252448 -> 140597367253792
	140597367252448 [label=TBackward0]
	140597367254224 -> 140597367252448
	140597856971008 [label="ct.encoder.layers.5.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597856971008 -> 140597367254224
	140597367254224 [label=AccumulateGrad]
	140597367252112 -> 140597367204928
	140597367252112 [label=TBackward0]
	140597367253648 -> 140597367252112
	140597856970848 [label="ct.encoder.layers.5.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597856970848 -> 140597367253648
	140597367253648 [label=AccumulateGrad]
	140597367855328 -> 140597367855232
	140597856970688 [label="ct.encoder.layers.5.norm_out.weight
 (512)" fillcolor=lightblue]
	140597856970688 -> 140597367855328
	140597367855328 [label=AccumulateGrad]
	140597367855280 -> 140597367855232
	140597856970768 [label="ct.encoder.layers.5.norm_out.bias
 (512)" fillcolor=lightblue]
	140597856970768 -> 140597367855280
	140597367855280 [label=AccumulateGrad]
	140597367855184 -> 140597367855088
	140597367855184 -> 140597367288832 [dir=none]
	140597367288832 [label="other
 ()" fillcolor=orange]
	140597367855184 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367855616 -> 140597367855184
	140597367855616 -> 140597367288752 [dir=none]
	140597367288752 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367855616 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367203392 -> 140597367855616
	140597367203392 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367855424 -> 140597367203392
	140597367855424 -> 140597367288992 [dir=none]
	140597367288992 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367855424 -> 140597367288352 [dir=none]
	140597367288352 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367855424 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367253744 -> 140597367855424
	140597856970288 [label="ct.encoder.layers.6.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597856970288 -> 140597367253744
	140597367253744 [label=AccumulateGrad]
	140597367253936 -> 140597367855424
	140597367253936 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367254080 -> 140597367253936
	140597367254080 -> 140597367289152 [dir=none]
	140597367289152 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367254080 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367254368 -> 140597367254080
	140597367254368 -> 140597480525072 [dir=none]
	140597480525072 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367254368 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367254464 -> 140597367254368
	140597367254464 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367254560 -> 140597367254464
	140597367254560 -> 140597367289232 [dir=none]
	140597367289232 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367254560 -> 140597367289072 [dir=none]
	140597367289072 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367254560 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367254656 -> 140597367254560
	140597856970448 [label="ct.encoder.layers.6.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597856970448 -> 140597367254656
	140597367254656 [label=AccumulateGrad]
	140597367254608 -> 140597367254560
	140597367254608 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367254752 -> 140597367254608
	140597367254752 -> 140597856970608 [dir=none]
	140597856970608 [label="bias
 (512)" fillcolor=orange]
	140597367254752 -> 140597480524992 [dir=none]
	140597480524992 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367254752 -> 140597367289392 [dir=none]
	140597367289392 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367254752 -> 140597367289312 [dir=none]
	140597367289312 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367254752 -> 140597856970528 [dir=none]
	140597856970528 [label="weight
 (512)" fillcolor=orange]
	140597367254752 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367855232 -> 140597367254752
	140597367254944 -> 140597367254752
	140597856970528 [label="ct.encoder.layers.6.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597856970528 -> 140597367254944
	140597367254944 [label=AccumulateGrad]
	140597367254896 -> 140597367254752
	140597856970608 [label="ct.encoder.layers.6.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597856970608 -> 140597367254896
	140597367254896 [label=AccumulateGrad]
	140597367254272 -> 140597367254560
	140597367254272 [label=TBackward0]
	140597367254992 -> 140597367254272
	140597856970368 [label="ct.encoder.layers.6.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597856970368 -> 140597367254992
	140597367254992 [label=AccumulateGrad]
	140597367253024 -> 140597367855424
	140597367253024 [label=TBackward0]
	140597367254416 -> 140597367253024
	140597856970208 [label="ct.encoder.layers.6.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597856970208 -> 140597367254416
	140597367254416 [label=AccumulateGrad]
	140597367855040 -> 140597367854944
	140597367855040 -> 140597367289552 [dir=none]
	140597367289552 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367855040 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367203200 -> 140597367855040
	140597367203200 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367855136 -> 140597367203200
	140597367855136 -> 140597367289632 [dir=none]
	140597367289632 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367855136 -> 140597367288912 [dir=none]
	140597367288912 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367855136 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367254704 -> 140597367855136
	140597857881200 [label="ct.encoder.layers.6.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597857881200 -> 140597367254704
	140597367254704 [label=AccumulateGrad]
	140597367254032 -> 140597367855136
	140597367254032 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367254320 -> 140597367254032
	140597367254320 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367254800 -> 140597367254320
	140597367254800 [label=CloneBackward0]
	140597367308496 -> 140597367254800
	140597367308496 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367308592 -> 140597367308496
	140597367308592 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367308688 -> 140597367308592
	140597367308688 -> 140597367289792 [dir=none]
	140597367289792 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367308688 -> 140597367289712 [dir=none]
	140597367289712 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367308688 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367308784 -> 140597367308688
	140597367308784 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367308928 -> 140597367308784
	140597367308928 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367309024 -> 140597367308928
	140597367309024 -> 140597367289472 [dir=none]
	140597367289472 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367309024 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367309120 -> 140597367309024
	140597367309120 -> 140597480526608 [dir=none]
	140597480526608 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367309120 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367309216 -> 140597367309120
	140597367309216 -> 140597367290112 [dir=none]
	140597367290112 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367309216 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367309312 -> 140597367309216
	140597367309312 -> 140597480526608 [dir=none]
	140597480526608 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367309312 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367309408 -> 140597367309312
	140597367309408 -> 140597367290032 [dir=none]
	140597367290032 [label="other
 ()" fillcolor=orange]
	140597367309408 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367309504 -> 140597367309408
	140597367309504 [label="AddBackward0
------------
alpha: 1"]
	140597367309600 -> 140597367309504
	140597367309600 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367309744 -> 140597367309600
	140597367309744 -> 140597367289952 [dir=none]
	140597367289952 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367309744 -> 140597367289872 [dir=none]
	140597367289872 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367309744 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367309840 -> 140597367309744
	140597367309840 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367309984 -> 140597367309840
	140597367309984 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367310080 -> 140597367309984
	140597367310080 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367310176 -> 140597367310080
	140597367310176 [label="AddBackward0
------------
alpha: 1"]
	140597367310272 -> 140597367310176
	140597367310272 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367310416 -> 140597367310272
	140597367310416 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367310512 -> 140597367310416
	140597367310512 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367310608 -> 140597367310512
	140597367310608 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367310704 -> 140597367310608
	140597367310704 -> 140597367287952 [dir=none]
	140597367287952 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367310704 -> 140597367290352 [dir=none]
	140597367290352 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367310704 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367310800 -> 140597367310704
	140597857881680 [label="ct.encoder.layers.6.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597857881680 -> 140597367310800
	140597367310800 [label=AccumulateGrad]
	140597367310752 -> 140597367310704
	140597367310752 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367310896 -> 140597367310752
	140597367310896 -> 140597857881840 [dir=none]
	140597857881840 [label="bias
 (512)" fillcolor=orange]
	140597367310896 -> 140597480525312 [dir=none]
	140597480525312 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367310896 -> 140597367290432 [dir=none]
	140597367290432 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367310896 -> 140597367288032 [dir=none]
	140597367288032 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367310896 -> 140597857881760 [dir=none]
	140597857881760 [label="weight
 (512)" fillcolor=orange]
	140597367310896 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367855088 -> 140597367310896
	140597367311088 -> 140597367310896
	140597857881760 [label="ct.encoder.layers.6.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597857881760 -> 140597367311088
	140597367311088 [label=AccumulateGrad]
	140597367311040 -> 140597367310896
	140597857881840 [label="ct.encoder.layers.6.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597857881840 -> 140597367311040
	140597367311040 [label=AccumulateGrad]
	140597367310320 -> 140597367310704
	140597367310320 [label=TBackward0]
	140597367311136 -> 140597367310320
	140597857881600 [label="ct.encoder.layers.6.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597857881600 -> 140597367311136
	140597367311136 [label=AccumulateGrad]
	140597367310224 -> 140597367310176
	140597857881040 [label="ct.encoder.layers.6.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597857881040 -> 140597367310224
	140597367310224 [label=AccumulateGrad]
	140597367309792 -> 140597367309744
	140597367309792 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367310128 -> 140597367309792
	140597367310128 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367310560 -> 140597367310128
	140597367310560 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367310656 -> 140597367310560
	140597367310656 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367310848 -> 140597367310656
	140597367310848 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367311184 -> 140597367310848
	140597367311184 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367311280 -> 140597367311184
	140597367311280 -> 140597367290592 [dir=none]
	140597367290592 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367311280 -> 140597367290192 [dir=none]
	140597367290192 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367311280 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367311376 -> 140597367311280
	140597857881520 [label="ct.encoder.layers.6.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597857881520 -> 140597367311376
	140597367311376 [label=AccumulateGrad]
	140597367311328 -> 140597367311280
	140597367311328 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367310896 -> 140597367311328
	140597367309936 -> 140597367311280
	140597367309936 [label=TBackward0]
	140597367311568 -> 140597367309936
	140597857881440 [label="ct.encoder.layers.6.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597857881440 -> 140597367311568
	140597367311568 [label=AccumulateGrad]
	140597367309552 -> 140597367309504
	140597367309552 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597367310032 -> 140597367309552
	140597367310032 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367310368 -> 140597367310032
	140597367310368 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367311232 -> 140597367310368
	140597367311232 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367310944 -> 140597367311232
	140597367310944 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367311520 -> 140597367310944
	140597367311520 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367311616 -> 140597367311520
	140597367311616 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367311712 -> 140597367311616
	140597367311712 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367311808 -> 140597367311712
	140597367311808 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367311904 -> 140597367311808
	140597367311904 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367312000 -> 140597367311904
	140597367312000 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367312096 -> 140597367312000
	140597367312096 -> 140597367291152 [dir=none]
	140597367291152 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367312096 -> 140597367290272 [dir=none]
	140597367290272 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367312096 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367312192 -> 140597367312096
	140597367312192 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367312336 -> 140597367312192
	140597367312336 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367312240 -> 140597367312336
	140597367312240 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367337168 -> 140597367312240
	140597367337168 [label="AddBackward0
------------
alpha: 1"]
	140597367310272 -> 140597367337168
	140597367337264 -> 140597367337168
	140597854488832 [label="ct.encoder.layers.6.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597854488832 -> 140597367337264
	140597367337264 [label=AccumulateGrad]
	140597367312144 -> 140597367312096
	140597367312144 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367312288 -> 140597367312144
	140597367312288 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367337360 -> 140597367312288
	140597367337360 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367337408 -> 140597367337360
	140597367337408 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367337456 -> 140597367337408
	140597367337456 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597367337600 -> 140597367337456
	140597367337600 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597367337696 -> 140597367337600
	140597367337696 -> 140597367290672 [dir=none]
	140597367290672 [label="self
 (251, 512)" fillcolor=orange]
	140597367337696 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597367337792 -> 140597367337696
	140597367337792 [label=TBackward0]
	140597367337888 -> 140597367337792
	140597857880800 [label="ct.encoder.layers.6.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597857880800 -> 140597367337888
	140597367337888 [label=AccumulateGrad]
	140597367308736 -> 140597367308688
	140597367308736 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367309072 -> 140597367308736
	140597367309072 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367309264 -> 140597367309072
	140597367309264 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367309456 -> 140597367309264
	140597367309456 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367309648 -> 140597367309456
	140597367309648 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367310464 -> 140597367309648
	140597367310464 -> 140597367290992 [dir=none]
	140597367290992 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367310464 -> 140597367290912 [dir=none]
	140597367290912 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367310464 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367311424 -> 140597367310464
	140597857881360 [label="ct.encoder.layers.6.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597857881360 -> 140597367311424
	140597367311424 [label=AccumulateGrad]
	140597367310992 -> 140597367310464
	140597367310992 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367310896 -> 140597367310992
	140597367308880 -> 140597367310464
	140597367308880 [label=TBackward0]
	140597367311856 -> 140597367308880
	140597857881280 [label="ct.encoder.layers.6.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597857881280 -> 140597367311856
	140597367311856 [label=AccumulateGrad]
	140597367253552 -> 140597367855136
	140597367253552 [label=TBackward0]
	140597367254848 -> 140597367253552
	140597857881120 [label="ct.encoder.layers.6.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597857881120 -> 140597367254848
	140597367254848 [label=AccumulateGrad]
	140597367854896 -> 140597367854800
	140597367854896 -> 140597367290832 [dir=none]
	140597367290832 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367854896 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367855472 -> 140597367854896
	140597367855472 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367254512 -> 140597367855472
	140597367254512 -> 140597480526128 [dir=none]
	140597480526128 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367254512 -> 140597857881920 [dir=none]
	140597857881920 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597367254512 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367308544 -> 140597367254512
	140597367308544 -> 140597480526288 [dir=none]
	140597480526288 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367308544 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367309168 -> 140597367308544
	140597367309168 -> 140597480526448 [dir=none]
	140597480526448 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367309168 -> 140597367290512 [dir=none]
	140597367290512 [label="result1
 (512)" fillcolor=orange]
	140597367309168 -> 140597367291312 [dir=none]
	140597367291312 [label="result2
 (512)" fillcolor=orange]
	140597367309168 -> 140597367290752 [dir=none]
	140597367290752 [label="result3
 (0)" fillcolor=orange]
	140597367309168 -> 140597855863488 [dir=none]
	140597855863488 [label="running_mean
 (512)" fillcolor=orange]
	140597367309168 -> 140597856969728 [dir=none]
	140597856969728 [label="running_var
 (512)" fillcolor=orange]
	140597367309168 -> 140597856969088 [dir=none]
	140597856969088 [label="weight
 (512)" fillcolor=orange]
	140597367309168 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367308832 -> 140597367309168
	140597367308832 -> 140597480526208 [dir=none]
	140597480526208 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367308832 -> 140597856969248 [dir=none]
	140597856969248 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367308832 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367312048 -> 140597367308832
	140597367312048 -> 140597480526368 [dir=none]
	140597480526368 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367312048 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367309696 -> 140597367312048
	140597367309696 -> 140597480526848 [dir=none]
	140597480526848 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367309696 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367337024 -> 140597367309696
	140597367337024 -> 140597480525632 [dir=none]
	140597480525632 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367337024 -> 140597856969408 [dir=none]
	140597856969408 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367337024 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367337552 -> 140597367337024
	140597367337552 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367337840 -> 140597367337552
	140597367337840 -> 140597856970128 [dir=none]
	140597856970128 [label="bias
 (512)" fillcolor=orange]
	140597367337840 -> 140597480525472 [dir=none]
	140597480525472 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367337840 -> 140597367291712 [dir=none]
	140597367291712 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367337840 -> 140597367291392 [dir=none]
	140597367291392 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367337840 -> 140597856970048 [dir=none]
	140597856970048 [label="weight
 (512)" fillcolor=orange]
	140597367337840 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367854944 -> 140597367337840
	140597367337936 -> 140597367337840
	140597856970048 [label="ct.encoder.layers.6.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597856970048 -> 140597367337936
	140597367337936 [label=AccumulateGrad]
	140597367337984 -> 140597367337840
	140597856970128 [label="ct.encoder.layers.6.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597856970128 -> 140597367337984
	140597367337984 [label=AccumulateGrad]
	140597367337312 -> 140597367337024
	140597856969408 [label="ct.encoder.layers.6.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597856969408 -> 140597367337312
	140597367337312 [label=AccumulateGrad]
	140597367337120 -> 140597367337024
	140597856969488 [label="ct.encoder.layers.6.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597856969488 -> 140597367337120
	140597367337120 [label=AccumulateGrad]
	140597367311760 -> 140597367308832
	140597856969248 [label="ct.encoder.layers.6.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597856969248 -> 140597367311760
	140597367311760 [label=AccumulateGrad]
	140597367311664 -> 140597367308832
	140597856969328 [label="ct.encoder.layers.6.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597856969328 -> 140597367311664
	140597367311664 [label=AccumulateGrad]
	140597367309360 -> 140597367309168
	140597856969088 [label="ct.encoder.layers.6.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597856969088 -> 140597367309360
	140597367309360 [label=AccumulateGrad]
	140597367308352 -> 140597367309168
	140597856969168 [label="ct.encoder.layers.6.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597856969168 -> 140597367308352
	140597367308352 [label=AccumulateGrad]
	140597367308640 -> 140597367254512
	140597857881920 [label="ct.encoder.layers.6.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597857881920 -> 140597367308640
	140597367308640 [label=AccumulateGrad]
	140597367308448 -> 140597367254512
	140597857882000 [label="ct.encoder.layers.6.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597857882000 -> 140597367308448
	140597367308448 [label=AccumulateGrad]
	140597367854752 -> 140597367854608
	140597367854752 -> 140597367291072 [dir=none]
	140597367291072 [label="other
 ()" fillcolor=orange]
	140597367854752 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367253168 -> 140597367854752
	140597367253168 -> 140597367291792 [dir=none]
	140597367291792 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367253168 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367854848 -> 140597367253168
	140597367854848 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367311952 -> 140597367854848
	140597367311952 -> 140597367291472 [dir=none]
	140597367291472 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367311952 -> 140597367291232 [dir=none]
	140597367291232 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367311952 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367311472 -> 140597367311952
	140597854490272 [label="ct.encoder.layers.6.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597854490272 -> 140597367311472
	140597367311472 [label=AccumulateGrad]
	140597367308400 -> 140597367311952
	140597367308400 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367337744 -> 140597367308400
	140597367337744 -> 140597367291632 [dir=none]
	140597367291632 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367337744 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367338080 -> 140597367337744
	140597367338080 -> 140597480525888 [dir=none]
	140597480525888 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367338080 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367338176 -> 140597367338080
	140597367338176 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367338272 -> 140597367338176
	140597367338272 -> 140597367291552 [dir=none]
	140597367291552 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367338272 -> 140597367365696 [dir=none]
	140597367365696 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367338272 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367338368 -> 140597367338272
	140597854490112 [label="ct.encoder.layers.6.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854490112 -> 140597367338368
	140597367338368 [label=AccumulateGrad]
	140597367338320 -> 140597367338272
	140597367338320 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367338464 -> 140597367338320
	140597367338464 -> 140597854489952 [dir=none]
	140597854489952 [label="bias
 (512)" fillcolor=orange]
	140597367338464 -> 140597480526528 [dir=none]
	140597480526528 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367338464 -> 140597367366016 [dir=none]
	140597367366016 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367338464 -> 140597367365856 [dir=none]
	140597367365856 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367338464 -> 140597854488912 [dir=none]
	140597854488912 [label="weight
 (512)" fillcolor=orange]
	140597367338464 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367854800 -> 140597367338464
	140597367338656 -> 140597367338464
	140597854488912 [label="ct.encoder.layers.6.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597854488912 -> 140597367338656
	140597367338656 [label=AccumulateGrad]
	140597367338608 -> 140597367338464
	140597854489952 [label="ct.encoder.layers.6.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597854489952 -> 140597367338608
	140597367338608 [label=AccumulateGrad]
	140597367337072 -> 140597367338272
	140597367337072 [label=TBackward0]
	140597367338704 -> 140597367337072
	140597854490032 [label="ct.encoder.layers.6.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854490032 -> 140597367338704
	140597367338704 [label=AccumulateGrad]
	140597367337216 -> 140597367311952
	140597367337216 [label=TBackward0]
	140597367338128 -> 140597367337216
	140597854490192 [label="ct.encoder.layers.6.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854490192 -> 140597367338128
	140597367338128 [label=AccumulateGrad]
	140597367854560 -> 140597367854464
	140597854490352 [label="ct.encoder.layers.6.norm_out.weight
 (512)" fillcolor=lightblue]
	140597854490352 -> 140597367854560
	140597367854560 [label=AccumulateGrad]
	140597367854512 -> 140597367854464
	140597854490432 [label="ct.encoder.layers.6.norm_out.bias
 (512)" fillcolor=lightblue]
	140597854490432 -> 140597367854512
	140597367854512 [label=AccumulateGrad]
	140597367854416 -> 140597367854320
	140597367854416 -> 140597367366176 [dir=none]
	140597367366176 [label="other
 ()" fillcolor=orange]
	140597367854416 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367854992 -> 140597367854416
	140597367854992 -> 140597367366096 [dir=none]
	140597367366096 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367854992 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367309888 -> 140597367854992
	140597367309888 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367854656 -> 140597367309888
	140597367854656 -> 140597367366416 [dir=none]
	140597367366416 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367854656 -> 140597367365776 [dir=none]
	140597367365776 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367854656 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367338224 -> 140597367854656
	140597854970240 [label="ct.encoder.layers.7.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597854970240 -> 140597367338224
	140597367338224 [label=AccumulateGrad]
	140597367338416 -> 140597367854656
	140597367338416 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367338560 -> 140597367338416
	140597367338560 -> 140597367366576 [dir=none]
	140597367366576 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367338560 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367338848 -> 140597367338560
	140597367338848 -> 140597480527248 [dir=none]
	140597480527248 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367338848 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367338944 -> 140597367338848
	140597367338944 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367339040 -> 140597367338944
	140597367339040 -> 140597367366656 [dir=none]
	140597367366656 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367339040 -> 140597367366496 [dir=none]
	140597367366496 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367339040 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367339136 -> 140597367339040
	140597854970080 [label="ct.encoder.layers.7.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854970080 -> 140597367339136
	140597367339136 [label=AccumulateGrad]
	140597367339088 -> 140597367339040
	140597367339088 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367339232 -> 140597367339088
	140597367339232 -> 140597854969920 [dir=none]
	140597854969920 [label="bias
 (512)" fillcolor=orange]
	140597367339232 -> 140597480527168 [dir=none]
	140597480527168 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367339232 -> 140597367366816 [dir=none]
	140597367366816 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367339232 -> 140597367366736 [dir=none]
	140597367366736 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367339232 -> 140597854490512 [dir=none]
	140597854490512 [label="weight
 (512)" fillcolor=orange]
	140597367339232 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367854464 -> 140597367339232
	140597367339424 -> 140597367339232
	140597854490512 [label="ct.encoder.layers.7.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597854490512 -> 140597367339424
	140597367339424 [label=AccumulateGrad]
	140597367339376 -> 140597367339232
	140597854969920 [label="ct.encoder.layers.7.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597854969920 -> 140597367339376
	140597367339376 [label=AccumulateGrad]
	140597367338752 -> 140597367339040
	140597367338752 [label=TBackward0]
	140597367339472 -> 140597367338752
	140597854970000 [label="ct.encoder.layers.7.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854970000 -> 140597367339472
	140597367339472 [label=AccumulateGrad]
	140597367337504 -> 140597367854656
	140597367337504 [label=TBackward0]
	140597367338896 -> 140597367337504
	140597854970160 [label="ct.encoder.layers.7.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854970160 -> 140597367338896
	140597367338896 [label=AccumulateGrad]
	140597367854272 -> 140597367854176
	140597367854272 -> 140597367366976 [dir=none]
	140597367366976 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367854272 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367308976 -> 140597367854272
	140597367308976 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367854368 -> 140597367308976
	140597367854368 -> 140597367367056 [dir=none]
	140597367367056 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367854368 -> 140597367366336 [dir=none]
	140597367366336 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367854368 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367339184 -> 140597367854368
	140597854972080 [label="ct.encoder.layers.7.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597854972080 -> 140597367339184
	140597367339184 [label=AccumulateGrad]
	140597367338512 -> 140597367854368
	140597367338512 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367338800 -> 140597367338512
	140597367338800 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367339280 -> 140597367338800
	140597367339280 [label=CloneBackward0]
	140597367339664 -> 140597367339280
	140597367339664 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367339760 -> 140597367339664
	140597367339760 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367339856 -> 140597367339760
	140597367339856 -> 140597367367216 [dir=none]
	140597367367216 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367339856 -> 140597367367136 [dir=none]
	140597367367136 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367339856 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367339952 -> 140597367339856
	140597367339952 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367340096 -> 140597367339952
	140597367340096 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367340192 -> 140597367340096
	140597367340192 -> 140597367366896 [dir=none]
	140597367366896 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367340192 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367340288 -> 140597367340192
	140597367340288 -> 140597480528688 [dir=none]
	140597480528688 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367340288 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367340384 -> 140597367340288
	140597367340384 -> 140597367367536 [dir=none]
	140597367367536 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367340384 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367340480 -> 140597367340384
	140597367340480 -> 140597480528688 [dir=none]
	140597480528688 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367340480 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367340576 -> 140597367340480
	140597367340576 -> 140597367367456 [dir=none]
	140597367367456 [label="other
 ()" fillcolor=orange]
	140597367340576 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367340672 -> 140597367340576
	140597367340672 [label="AddBackward0
------------
alpha: 1"]
	140597367340768 -> 140597367340672
	140597367340768 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367340912 -> 140597367340768
	140597367340912 -> 140597367367376 [dir=none]
	140597367367376 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367340912 -> 140597367367296 [dir=none]
	140597367367296 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367340912 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367341008 -> 140597367340912
	140597367341008 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367394464 -> 140597367341008
	140597367394464 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367394560 -> 140597367394464
	140597367394560 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367394656 -> 140597367394560
	140597367394656 [label="AddBackward0
------------
alpha: 1"]
	140597367394752 -> 140597367394656
	140597367394752 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367394896 -> 140597367394752
	140597367394896 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367394992 -> 140597367394896
	140597367394992 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367996320 -> 140597367394992
	140597367996320 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597368008768 -> 140597367996320
	140597368008768 -> 140597368461888 [dir=none]
	140597368461888 [label="mat1
 (126, 512)" fillcolor=orange]
	140597368008768 -> 140597368313232 [dir=none]
	140597368313232 [label="mat2
 (512, 512)" fillcolor=orange]
	140597368008768 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597368445296 -> 140597368008768
	140597854971600 [label="ct.encoder.layers.7.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597854971600 -> 140597368445296
	140597368445296 [label=AccumulateGrad]
	140597368008816 -> 140597368008768
	140597368008816 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597368481104 -> 140597368008816
	140597368481104 -> 140597854971440 [dir=none]
	140597854971440 [label="bias
 (512)" fillcolor=orange]
	140597368481104 -> 140597480527488 [dir=none]
	140597480527488 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597368481104 -> 140597368261904 [dir=none]
	140597368261904 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597368481104 -> 140597367999040 [dir=none]
	140597367999040 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597368481104 -> 140597854971360 [dir=none]
	140597854971360 [label="weight
 (512)" fillcolor=orange]
	140597368481104 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367854320 -> 140597368481104
	140597368390224 -> 140597368481104
	140597854971360 [label="ct.encoder.layers.7.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597854971360 -> 140597368390224
	140597368390224 [label=AccumulateGrad]
	140597368326560 -> 140597368481104
	140597854971440 [label="ct.encoder.layers.7.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597854971440 -> 140597368326560
	140597368326560 [label=AccumulateGrad]
	140597368008912 -> 140597368008768
	140597368008912 [label=TBackward0]
	140597368390272 -> 140597368008912
	140597854971520 [label="ct.encoder.layers.7.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597854971520 -> 140597368390272
	140597368390272 [label=AccumulateGrad]
	140597367394704 -> 140597367394656
	140597854972240 [label="ct.encoder.layers.7.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597854972240 -> 140597367394704
	140597367394704 [label=AccumulateGrad]
	140597367340960 -> 140597367340912
	140597367340960 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367994832 -> 140597367340960
	140597367994832 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597368481248 -> 140597367994832
	140597368481248 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597368326464 -> 140597368481248
	140597368326464 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597368326320 -> 140597368326464
	140597368326320 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367394368 -> 140597368326320
	140597367394368 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367394944 -> 140597367394368
	140597367394944 -> 140597367998960 [dir=none]
	140597367998960 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367394944 -> 140597367367616 [dir=none]
	140597367367616 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367394944 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367395040 -> 140597367394944
	140597854971760 [label="ct.encoder.layers.7.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597854971760 -> 140597367395040
	140597367395040 [label=AccumulateGrad]
	140597367395088 -> 140597367394944
	140597367395088 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597368481104 -> 140597367395088
	140597367394608 -> 140597367394944
	140597367394608 [label=TBackward0]
	140597367395232 -> 140597367394608
	140597854971680 [label="ct.encoder.layers.7.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597854971680 -> 140597367395232
	140597367395232 [label=AccumulateGrad]
	140597367340720 -> 140597367340672
	140597367340720 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597368008864 -> 140597367340720
	140597368008864 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367340864 -> 140597368008864
	140597367340864 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597368326512 -> 140597367340864
	140597368326512 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367394848 -> 140597368326512
	140597367394848 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367395184 -> 140597367394848
	140597367395184 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367395280 -> 140597367395184
	140597367395280 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367395376 -> 140597367395280
	140597367395376 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367395472 -> 140597367395376
	140597367395472 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367395568 -> 140597367395472
	140597367395568 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367395664 -> 140597367395568
	140597367395664 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367395760 -> 140597367395664
	140597367395760 -> 140597367368176 [dir=none]
	140597367368176 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367395760 -> 140597367367936 [dir=none]
	140597367367936 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367395760 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367395856 -> 140597367395760
	140597367395856 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367396000 -> 140597367395856
	140597367396000 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367396096 -> 140597367396000
	140597367396096 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367396192 -> 140597367396096
	140597367396192 [label="AddBackward0
------------
alpha: 1"]
	140597367394752 -> 140597367396192
	140597367396288 -> 140597367396192
	140597854972320 [label="ct.encoder.layers.7.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597854972320 -> 140597367396288
	140597367396288 [label=AccumulateGrad]
	140597367395808 -> 140597367395760
	140597367395808 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367396144 -> 140597367395808
	140597367396144 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367396384 -> 140597367396144
	140597367396384 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367396432 -> 140597367396384
	140597367396432 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367396480 -> 140597367396432
	140597367396480 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597367396624 -> 140597367396480
	140597367396624 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597367396720 -> 140597367396624
	140597367396720 -> 140597367367776 [dir=none]
	140597367367776 [label="self
 (251, 512)" fillcolor=orange]
	140597367396720 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597367396816 -> 140597367396720
	140597367396816 [label=TBackward0]
	140597367396912 -> 140597367396816
	140597854972160 [label="ct.encoder.layers.7.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597854972160 -> 140597367396912
	140597367396912 [label=AccumulateGrad]
	140597367339904 -> 140597367339856
	140597367339904 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367340240 -> 140597367339904
	140597367340240 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367340432 -> 140597367340240
	140597367340432 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367340624 -> 140597367340432
	140597367340624 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367340816 -> 140597367340624
	140597367340816 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597368326032 -> 140597367340816
	140597368326032 -> 140597367368016 [dir=none]
	140597367368016 [label="mat1
 (126, 512)" fillcolor=orange]
	140597368326032 -> 140597367367856 [dir=none]
	140597367367856 [label="mat2
 (512, 512)" fillcolor=orange]
	140597368326032 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367340048 -> 140597368326032
	140597854971920 [label="ct.encoder.layers.7.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597854971920 -> 140597367340048
	140597367340048 [label=AccumulateGrad]
	140597367394800 -> 140597368326032
	140597367394800 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597368481104 -> 140597367394800
	140597367394512 -> 140597368326032
	140597367394512 [label=TBackward0]
	140597367395520 -> 140597367394512
	140597854971840 [label="ct.encoder.layers.7.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597854971840 -> 140597367395520
	140597367395520 [label=AccumulateGrad]
	140597367338032 -> 140597367854368
	140597367338032 [label=TBackward0]
	140597367339616 -> 140597367338032
	140597854972000 [label="ct.encoder.layers.7.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597854972000 -> 140597367339616
	140597367339616 [label=AccumulateGrad]
	140597367854128 -> 140597367854032
	140597367854128 -> 140597367367696 [dir=none]
	140597367367696 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367854128 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367854704 -> 140597367854128
	140597367854704 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367339520 -> 140597367854704
	140597367339520 -> 140597480527728 [dir=none]
	140597480527728 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367339520 -> 140597854971200 [dir=none]
	140597854971200 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597367339520 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367339712 -> 140597367339520
	140597367339712 -> 140597480527968 [dir=none]
	140597480527968 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367339712 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367340336 -> 140597367339712
	140597367340336 -> 140597480528368 [dir=none]
	140597480528368 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367340336 -> 140597367365936 [dir=none]
	140597367365936 [label="result1
 (512)" fillcolor=orange]
	140597367340336 -> 140597367368336 [dir=none]
	140597367368336 [label="result2
 (512)" fillcolor=orange]
	140597367340336 -> 140597367366256 [dir=none]
	140597367366256 [label="result3
 (0)" fillcolor=orange]
	140597367340336 -> 140597856785888 [dir=none]
	140597856785888 [label="running_mean
 (512)" fillcolor=orange]
	140597367340336 -> 140597856968848 [dir=none]
	140597856968848 [label="running_var
 (512)" fillcolor=orange]
	140597367340336 -> 140597854970800 [dir=none]
	140597854970800 [label="weight
 (512)" fillcolor=orange]
	140597367340336 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367340000 -> 140597367340336
	140597367340000 -> 140597480528048 [dir=none]
	140597480528048 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367340000 -> 140597854970640 [dir=none]
	140597854970640 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367340000 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367395712 -> 140597367340000
	140597367395712 -> 140597480528208 [dir=none]
	140597480528208 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367395712 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367396048 -> 140597367395712
	140597367396048 -> 140597480528288 [dir=none]
	140597480528288 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367396048 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367395904 -> 140597367396048
	140597367395904 -> 140597480528928 [dir=none]
	140597480528928 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367395904 -> 140597854970480 [dir=none]
	140597854970480 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367395904 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367396576 -> 140597367395904
	140597367396576 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367396864 -> 140597367396576
	140597367396864 -> 140597854970400 [dir=none]
	140597854970400 [label="bias
 (512)" fillcolor=orange]
	140597367396864 -> 140597480528448 [dir=none]
	140597480528448 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367396864 -> 140597367368736 [dir=none]
	140597367368736 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367396864 -> 140597367368416 [dir=none]
	140597367368416 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367396864 -> 140597854970320 [dir=none]
	140597854970320 [label="weight
 (512)" fillcolor=orange]
	140597367396864 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367854176 -> 140597367396864
	140597367396960 -> 140597367396864
	140597854970320 [label="ct.encoder.layers.7.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597854970320 -> 140597367396960
	140597367396960 [label=AccumulateGrad]
	140597367397008 -> 140597367396864
	140597854970400 [label="ct.encoder.layers.7.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597854970400 -> 140597367397008
	140597367397008 [label=AccumulateGrad]
	140597367396336 -> 140597367395904
	140597854970480 [label="ct.encoder.layers.7.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597854970480 -> 140597367396336
	140597367396336 [label=AccumulateGrad]
	140597367395136 -> 140597367395904
	140597854970560 [label="ct.encoder.layers.7.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597854970560 -> 140597367395136
	140597367395136 [label=AccumulateGrad]
	140597367395424 -> 140597367340000
	140597854970640 [label="ct.encoder.layers.7.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597854970640 -> 140597367395424
	140597367395424 [label=AccumulateGrad]
	140597367395328 -> 140597367340000
	140597854970720 [label="ct.encoder.layers.7.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597854970720 -> 140597367395328
	140597367395328 [label=AccumulateGrad]
	140597367340528 -> 140597367340336
	140597854970800 [label="ct.encoder.layers.7.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597854970800 -> 140597367340528
	140597367340528 [label=AccumulateGrad]
	140597367339568 -> 140597367340336
	140597854970880 [label="ct.encoder.layers.7.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597854970880 -> 140597367339568
	140597367339568 [label=AccumulateGrad]
	140597367339808 -> 140597367339520
	140597854971200 [label="ct.encoder.layers.7.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597854971200 -> 140597367339808
	140597367339808 [label=AccumulateGrad]
	140597367337648 -> 140597367339520
	140597854971280 [label="ct.encoder.layers.7.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597854971280 -> 140597367337648
	140597367337648 [label=AccumulateGrad]
	140597367853984 -> 140597367853840
	140597367853984 -> 140597367368896 [dir=none]
	140597367368896 [label="other
 ()" fillcolor=orange]
	140597367853984 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367854224 -> 140597367853984
	140597367854224 -> 140597367368816 [dir=none]
	140597367368816 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367854224 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367340144 -> 140597367854224
	140597367340144 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597368326272 -> 140597367340144
	140597368326272 -> 140597367369056 [dir=none]
	140597367369056 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597368326272 -> 140597367368256 [dir=none]
	140597367368256 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597368326272 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367396240 -> 140597368326272
	140597854972800 [label="ct.encoder.layers.7.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597854972800 -> 140597367396240
	140597367396240 [label=AccumulateGrad]
	140597367394416 -> 140597368326272
	140597367394416 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367396768 -> 140597367394416
	140597367396768 -> 140597367368656 [dir=none]
	140597367368656 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367396768 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367397104 -> 140597367396768
	140597367397104 -> 140597480527808 [dir=none]
	140597480527808 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367397104 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367397200 -> 140597367397104
	140597367397200 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367397296 -> 140597367397200
	140597367397296 -> 140597367368496 [dir=none]
	140597367368496 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367397296 -> 140597367369136 [dir=none]
	140597367369136 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367397296 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367397392 -> 140597367397296
	140597854972640 [label="ct.encoder.layers.7.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854972640 -> 140597367397392
	140597367397392 [label=AccumulateGrad]
	140597367397344 -> 140597367397296
	140597367397344 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367397488 -> 140597367397344
	140597367397488 -> 140597854972480 [dir=none]
	140597854972480 [label="bias
 (512)" fillcolor=orange]
	140597367397488 -> 140597480528608 [dir=none]
	140597480528608 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367397488 -> 140597367369296 [dir=none]
	140597367369296 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367397488 -> 140597367369216 [dir=none]
	140597367369216 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367397488 -> 140597854972400 [dir=none]
	140597854972400 [label="weight
 (512)" fillcolor=orange]
	140597367397488 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367854032 -> 140597367397488
	140597367397680 -> 140597367397488
	140597854972400 [label="ct.encoder.layers.7.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597854972400 -> 140597367397680
	140597367397680 [label=AccumulateGrad]
	140597367397632 -> 140597367397488
	140597854972480 [label="ct.encoder.layers.7.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597854972480 -> 140597367397632
	140597367397632 [label=AccumulateGrad]
	140597367395952 -> 140597367397296
	140597367395952 [label=TBackward0]
	140597367397728 -> 140597367395952
	140597854972560 [label="ct.encoder.layers.7.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854972560 -> 140597367397728
	140597367397728 [label=AccumulateGrad]
	140597367395616 -> 140597368326272
	140597367395616 [label=TBackward0]
	140597367397152 -> 140597367395616
	140597854972720 [label="ct.encoder.layers.7.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854972720 -> 140597367397152
	140597367397152 [label=AccumulateGrad]
	140597367853792 -> 140597367853696
	140597854972880 [label="ct.encoder.layers.7.norm_out.weight
 (512)" fillcolor=lightblue]
	140597854972880 -> 140597367853792
	140597367853792 [label=AccumulateGrad]
	140597367853744 -> 140597367853696
	140597854972960 [label="ct.encoder.layers.7.norm_out.bias
 (512)" fillcolor=lightblue]
	140597854972960 -> 140597367853744
	140597367853744 [label=AccumulateGrad]
	140597367853648 -> 140597367853552
	140597367853648 -> 140597367369456 [dir=none]
	140597367369456 [label="other
 ()" fillcolor=orange]
	140597367853648 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367854080 -> 140597367853648
	140597367854080 -> 140597367369376 [dir=none]
	140597367369376 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367854080 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367338992 -> 140597367854080
	140597367338992 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367853888 -> 140597367338992
	140597367853888 -> 140597367369616 [dir=none]
	140597367369616 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367853888 -> 140597367368576 [dir=none]
	140597367368576 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367853888 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367397248 -> 140597367853888
	140597854973440 [label="ct.encoder.layers.8.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597854973440 -> 140597367397248
	140597367397248 [label=AccumulateGrad]
	140597367397440 -> 140597367853888
	140597367397440 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367397584 -> 140597367397440
	140597367397584 -> 140597367368096 [dir=none]
	140597367368096 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367397584 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367397872 -> 140597367397584
	140597367397872 -> 140597480529328 [dir=none]
	140597480529328 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367397872 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367397968 -> 140597367397872
	140597367397968 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367398064 -> 140597367397968
	140597367398064 -> 140597367369536 [dir=none]
	140597367369536 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367398064 -> 140597367368976 [dir=none]
	140597367368976 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367398064 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367398160 -> 140597367398064
	140597854973280 [label="ct.encoder.layers.8.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854973280 -> 140597367398160
	140597367398160 [label=AccumulateGrad]
	140597367398112 -> 140597367398064
	140597367398112 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367398256 -> 140597367398112
	140597367398256 -> 140597854973120 [dir=none]
	140597854973120 [label="bias
 (512)" fillcolor=orange]
	140597367398256 -> 140597480529248 [dir=none]
	140597480529248 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367398256 -> 140597367447616 [dir=none]
	140597367447616 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367398256 -> 140597367447776 [dir=none]
	140597367447776 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367398256 -> 140597854973040 [dir=none]
	140597854973040 [label="weight
 (512)" fillcolor=orange]
	140597367398256 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367853696 -> 140597367398256
	140597367398352 -> 140597367398256
	140597854973040 [label="ct.encoder.layers.8.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597854973040 -> 140597367398352
	140597367398352 [label=AccumulateGrad]
	140597367443568 -> 140597367398256
	140597854973120 [label="ct.encoder.layers.8.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597854973120 -> 140597367443568
	140597367443568 [label=AccumulateGrad]
	140597367397776 -> 140597367398064
	140597367397776 [label=TBackward0]
	140597367398304 -> 140597367397776
	140597854973200 [label="ct.encoder.layers.8.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854973200 -> 140597367398304
	140597367398304 [label=AccumulateGrad]
	140597367396528 -> 140597367853888
	140597367396528 [label=TBackward0]
	140597367397920 -> 140597367396528
	140597854973360 [label="ct.encoder.layers.8.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854973360 -> 140597367397920
	140597367397920 [label=AccumulateGrad]
	140597367853504 -> 140597367853408
	140597367853504 -> 140597367447936 [dir=none]
	140597367447936 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367853504 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367339328 -> 140597367853504
	140597367339328 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367853600 -> 140597367339328
	140597367853600 -> 140597367448096 [dir=none]
	140597367448096 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367853600 -> 140597367448016 [dir=none]
	140597367448016 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367853600 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367398208 -> 140597367853600
	140597855180176 [label="ct.encoder.layers.8.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597855180176 -> 140597367398208
	140597367398208 [label=AccumulateGrad]
	140597367397536 -> 140597367853600
	140597367397536 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367397824 -> 140597367397536
	140597367397824 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367443520 -> 140597367397824
	140597367443520 [label=CloneBackward0]
	140597367443808 -> 140597367443520
	140597367443808 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367443904 -> 140597367443808
	140597367443904 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367444000 -> 140597367443904
	140597367444000 -> 140597367448256 [dir=none]
	140597367448256 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367444000 -> 140597367448176 [dir=none]
	140597367448176 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367444000 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367444096 -> 140597367444000
	140597367444096 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367444240 -> 140597367444096
	140597367444240 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367444336 -> 140597367444240
	140597367444336 -> 140597367447856 [dir=none]
	140597367447856 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367444336 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367444432 -> 140597367444336
	140597367444432 -> 140597480530864 [dir=none]
	140597480530864 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367444432 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367444528 -> 140597367444432
	140597367444528 -> 140597367448656 [dir=none]
	140597367448656 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367444528 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367444624 -> 140597367444528
	140597367444624 -> 140597480530864 [dir=none]
	140597480530864 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367444624 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367444720 -> 140597367444624
	140597367444720 -> 140597367448576 [dir=none]
	140597367448576 [label="other
 ()" fillcolor=orange]
	140597367444720 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367444816 -> 140597367444720
	140597367444816 [label="AddBackward0
------------
alpha: 1"]
	140597367444912 -> 140597367444816
	140597367444912 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367445056 -> 140597367444912
	140597367445056 -> 140597367448496 [dir=none]
	140597367448496 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367445056 -> 140597367448416 [dir=none]
	140597367448416 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367445056 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367445152 -> 140597367445056
	140597367445152 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367445296 -> 140597367445152
	140597367445296 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367445392 -> 140597367445296
	140597367445392 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367445488 -> 140597367445392
	140597367445488 [label="AddBackward0
------------
alpha: 1"]
	140597367445584 -> 140597367445488
	140597367445584 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367445728 -> 140597367445584
	140597367445728 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367445824 -> 140597367445728
	140597367445824 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367445920 -> 140597367445824
	140597367445920 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367446016 -> 140597367445920
	140597367446016 -> 140597367447696 [dir=none]
	140597367447696 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367446016 -> 140597367448896 [dir=none]
	140597367448896 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367446016 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367446112 -> 140597367446016
	140597855179696 [label="ct.encoder.layers.8.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597855179696 -> 140597367446112
	140597367446112 [label=AccumulateGrad]
	140597367446064 -> 140597367446016
	140597367446064 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367446208 -> 140597367446064
	140597367446208 -> 140597855179536 [dir=none]
	140597855179536 [label="bias
 (512)" fillcolor=orange]
	140597367446208 -> 140597480529568 [dir=none]
	140597480529568 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367446208 -> 140597367448976 [dir=none]
	140597367448976 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367446208 -> 140597367448336 [dir=none]
	140597367448336 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367446208 -> 140597855179456 [dir=none]
	140597855179456 [label="weight
 (512)" fillcolor=orange]
	140597367446208 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367853552 -> 140597367446208
	140597367446400 -> 140597367446208
	140597855179456 [label="ct.encoder.layers.8.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597855179456 -> 140597367446400
	140597367446400 [label=AccumulateGrad]
	140597367446352 -> 140597367446208
	140597855179536 [label="ct.encoder.layers.8.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597855179536 -> 140597367446352
	140597367446352 [label=AccumulateGrad]
	140597367445632 -> 140597367446016
	140597367445632 [label=TBackward0]
	140597367446448 -> 140597367445632
	140597855179616 [label="ct.encoder.layers.8.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597855179616 -> 140597367446448
	140597367446448 [label=AccumulateGrad]
	140597367445536 -> 140597367445488
	140597855180336 [label="ct.encoder.layers.8.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597855180336 -> 140597367445536
	140597367445536 [label=AccumulateGrad]
	140597367445104 -> 140597367445056
	140597367445104 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367445440 -> 140597367445104
	140597367445440 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367445872 -> 140597367445440
	140597367445872 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367445968 -> 140597367445872
	140597367445968 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367446160 -> 140597367445968
	140597367446160 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367446496 -> 140597367446160
	140597367446496 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367446592 -> 140597367446496
	140597367446592 -> 140597367449136 [dir=none]
	140597367449136 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367446592 -> 140597367448736 [dir=none]
	140597367448736 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367446592 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367446688 -> 140597367446592
	140597855179856 [label="ct.encoder.layers.8.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597855179856 -> 140597367446688
	140597367446688 [label=AccumulateGrad]
	140597367446640 -> 140597367446592
	140597367446640 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367446208 -> 140597367446640
	140597367445248 -> 140597367446592
	140597367445248 [label=TBackward0]
	140597367446880 -> 140597367445248
	140597855179776 [label="ct.encoder.layers.8.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597855179776 -> 140597367446880
	140597367446880 [label=AccumulateGrad]
	140597367444864 -> 140597367444816
	140597367444864 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597367445344 -> 140597367444864
	140597367445344 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367445680 -> 140597367445344
	140597367445680 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367446544 -> 140597367445680
	140597367446544 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367446256 -> 140597367446544
	140597367446256 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367446832 -> 140597367446256
	140597367446832 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367446928 -> 140597367446832
	140597367446928 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367447024 -> 140597367446928
	140597367447024 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367447120 -> 140597367447024
	140597367447120 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367447216 -> 140597367447120
	140597367447216 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367447312 -> 140597367447216
	140597367447312 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367447408 -> 140597367447312
	140597367447408 -> 140597367449696 [dir=none]
	140597367449696 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367447408 -> 140597367448816 [dir=none]
	140597367448816 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367447408 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367447504 -> 140597367447408
	140597367447504 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366960288 -> 140597367447504
	140597366960288 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366960384 -> 140597366960288
	140597366960384 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366960480 -> 140597366960384
	140597366960480 [label="AddBackward0
------------
alpha: 1"]
	140597367445584 -> 140597366960480
	140597366960576 -> 140597366960480
	140597855180416 [label="ct.encoder.layers.8.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597855180416 -> 140597366960576
	140597366960576 [label=AccumulateGrad]
	140597367447456 -> 140597367447408
	140597367447456 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366960432 -> 140597367447456
	140597366960432 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366960672 -> 140597366960432
	140597366960672 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366960720 -> 140597366960672
	140597366960720 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366960768 -> 140597366960720
	140597366960768 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597366960912 -> 140597366960768
	140597366960912 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597366961008 -> 140597366960912
	140597366961008 -> 140597367449216 [dir=none]
	140597367449216 [label="self
 (251, 512)" fillcolor=orange]
	140597366961008 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597366961104 -> 140597366961008
	140597366961104 [label=TBackward0]
	140597366961200 -> 140597366961104
	140597855180256 [label="ct.encoder.layers.8.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597855180256 -> 140597366961200
	140597366961200 [label=AccumulateGrad]
	140597367444048 -> 140597367444000
	140597367444048 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367444384 -> 140597367444048
	140597367444384 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367444576 -> 140597367444384
	140597367444576 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367444768 -> 140597367444576
	140597367444768 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367444960 -> 140597367444768
	140597367444960 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367445776 -> 140597367444960
	140597367445776 -> 140597367449536 [dir=none]
	140597367449536 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367445776 -> 140597367449456 [dir=none]
	140597367449456 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367445776 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367446736 -> 140597367445776
	140597855180016 [label="ct.encoder.layers.8.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597855180016 -> 140597367446736
	140597367446736 [label=AccumulateGrad]
	140597367446304 -> 140597367445776
	140597367446304 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367446208 -> 140597367446304
	140597367444192 -> 140597367445776
	140597367444192 [label=TBackward0]
	140597367447168 -> 140597367444192
	140597855179936 [label="ct.encoder.layers.8.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597855179936 -> 140597367447168
	140597367447168 [label=AccumulateGrad]
	140597367397056 -> 140597367853600
	140597367397056 [label=TBackward0]
	140597367443760 -> 140597367397056
	140597855180096 [label="ct.encoder.layers.8.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597855180096 -> 140597367443760
	140597367443760 [label=AccumulateGrad]
	140597367853360 -> 140597367853264
	140597367853360 -> 140597367449376 [dir=none]
	140597367449376 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367853360 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367853936 -> 140597367853360
	140597367853936 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367398016 -> 140597367853936
	140597367398016 -> 140597480530224 [dir=none]
	140597480530224 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367398016 -> 140597855179296 [dir=none]
	140597855179296 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597367398016 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367443856 -> 140597367398016
	140597367443856 -> 140597480530384 [dir=none]
	140597480530384 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367443856 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367444480 -> 140597367443856
	140597367444480 -> 140597480530624 [dir=none]
	140597480530624 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367444480 -> 140597367449056 [dir=none]
	140597367449056 [label="result1
 (512)" fillcolor=orange]
	140597367444480 -> 140597367449856 [dir=none]
	140597367449856 [label="result2
 (512)" fillcolor=orange]
	140597367444480 -> 140597367449296 [dir=none]
	140597367449296 [label="result3
 (0)" fillcolor=orange]
	140597367444480 -> 140597856969008 [dir=none]
	140597856969008 [label="running_mean
 (512)" fillcolor=orange]
	140597367444480 -> 140597854971120 [dir=none]
	140597854971120 [label="running_var
 (512)" fillcolor=orange]
	140597367444480 -> 140597855178896 [dir=none]
	140597855178896 [label="weight
 (512)" fillcolor=orange]
	140597367444480 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367444144 -> 140597367444480
	140597367444144 -> 140597480530544 [dir=none]
	140597480530544 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367444144 -> 140597854973840 [dir=none]
	140597854973840 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367444144 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367447360 -> 140597367444144
	140597367447360 -> 140597480530464 [dir=none]
	140597480530464 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367447360 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367445008 -> 140597367447360
	140597367445008 -> 140597480530704 [dir=none]
	140597480530704 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367445008 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597366960192 -> 140597367445008
	140597366960192 -> 140597480531104 [dir=none]
	140597480531104 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366960192 -> 140597854973680 [dir=none]
	140597854973680 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597366960192 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366960864 -> 140597366960192
	140597366960864 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366961152 -> 140597366960864
	140597366961152 -> 140597854973600 [dir=none]
	140597854973600 [label="bias
 (512)" fillcolor=orange]
	140597366961152 -> 140597480529728 [dir=none]
	140597480529728 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366961152 -> 140597367450256 [dir=none]
	140597367450256 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366961152 -> 140597367449936 [dir=none]
	140597367449936 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366961152 -> 140597854973520 [dir=none]
	140597854973520 [label="weight
 (512)" fillcolor=orange]
	140597366961152 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367853408 -> 140597366961152
	140597366961248 -> 140597366961152
	140597854973520 [label="ct.encoder.layers.8.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597854973520 -> 140597366961248
	140597366961248 [label=AccumulateGrad]
	140597366961296 -> 140597366961152
	140597854973600 [label="ct.encoder.layers.8.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597854973600 -> 140597366961296
	140597366961296 [label=AccumulateGrad]
	140597366960624 -> 140597366960192
	140597854973680 [label="ct.encoder.layers.8.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597854973680 -> 140597366960624
	140597366960624 [label=AccumulateGrad]
	140597366960336 -> 140597366960192
	140597854973760 [label="ct.encoder.layers.8.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597854973760 -> 140597366960336
	140597366960336 [label=AccumulateGrad]
	140597367447072 -> 140597367444144
	140597854973840 [label="ct.encoder.layers.8.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597854973840 -> 140597367447072
	140597367447072 [label=AccumulateGrad]
	140597367446976 -> 140597367444144
	140597855178816 [label="ct.encoder.layers.8.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597855178816 -> 140597367446976
	140597367446976 [label=AccumulateGrad]
	140597367444672 -> 140597367444480
	140597855178896 [label="ct.encoder.layers.8.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597855178896 -> 140597367444672
	140597367444672 [label=AccumulateGrad]
	140597367443712 -> 140597367444480
	140597855178976 [label="ct.encoder.layers.8.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597855178976 -> 140597367443712
	140597367443712 [label=AccumulateGrad]
	140597367443952 -> 140597367398016
	140597855179296 [label="ct.encoder.layers.8.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597855179296 -> 140597367443952
	140597367443952 [label=AccumulateGrad]
	140597367443664 -> 140597367398016
	140597855179376 [label="ct.encoder.layers.8.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597855179376 -> 140597367443664
	140597367443664 [label=AccumulateGrad]
	140597367853216 -> 140597368049616
	140597367853216 -> 140597367450416 [dir=none]
	140597367450416 [label="other
 ()" fillcolor=orange]
	140597367853216 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367396672 -> 140597367853216
	140597367396672 -> 140597367450336 [dir=none]
	140597367450336 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367396672 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367853312 -> 140597367396672
	140597367853312 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367447264 -> 140597367853312
	140597367447264 -> 140597367450576 [dir=none]
	140597367450576 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367447264 -> 140597367449776 [dir=none]
	140597367449776 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367447264 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367446784 -> 140597367447264
	140597855180896 [label="ct.encoder.layers.8.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597855180896 -> 140597367446784
	140597367446784 [label=AccumulateGrad]
	140597367443616 -> 140597367447264
	140597367443616 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366961056 -> 140597367443616
	140597366961056 -> 140597367450176 [dir=none]
	140597367450176 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366961056 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366961392 -> 140597366961056
	140597366961392 -> 140597480529984 [dir=none]
	140597480529984 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366961392 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366961488 -> 140597366961392
	140597366961488 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366961584 -> 140597366961488
	140597366961584 -> 140597367450016 [dir=none]
	140597367450016 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366961584 -> 140597367450656 [dir=none]
	140597367450656 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366961584 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366961680 -> 140597366961584
	140597855180736 [label="ct.encoder.layers.8.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597855180736 -> 140597366961680
	140597366961680 [label=AccumulateGrad]
	140597366961632 -> 140597366961584
	140597366961632 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366961776 -> 140597366961632
	140597366961776 -> 140597855180576 [dir=none]
	140597855180576 [label="bias
 (512)" fillcolor=orange]
	140597366961776 -> 140597480530784 [dir=none]
	140597480530784 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366961776 -> 140597367450816 [dir=none]
	140597367450816 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366961776 -> 140597367450736 [dir=none]
	140597367450736 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366961776 -> 140597855180496 [dir=none]
	140597855180496 [label="weight
 (512)" fillcolor=orange]
	140597366961776 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597367853264 -> 140597366961776
	140597366961968 -> 140597366961776
	140597855180496 [label="ct.encoder.layers.8.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597855180496 -> 140597366961968
	140597366961968 [label=AccumulateGrad]
	140597366961920 -> 140597366961776
	140597855180576 [label="ct.encoder.layers.8.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597855180576 -> 140597366961920
	140597366961920 [label=AccumulateGrad]
	140597366960240 -> 140597366961584
	140597366960240 [label=TBackward0]
	140597366962016 -> 140597366960240
	140597855180656 [label="ct.encoder.layers.8.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597855180656 -> 140597366962016
	140597366962016 [label=AccumulateGrad]
	140597366960528 -> 140597367447264
	140597366960528 [label=TBackward0]
	140597366961440 -> 140597366960528
	140597855180816 [label="ct.encoder.layers.8.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597855180816 -> 140597366961440
	140597366961440 [label=AccumulateGrad]
	140597368049568 -> 140597368049472
	140597855180976 [label="ct.encoder.layers.8.norm_out.weight
 (512)" fillcolor=lightblue]
	140597855180976 -> 140597368049568
	140597368049568 [label=AccumulateGrad]
	140597368049520 -> 140597368049472
	140597855181056 [label="ct.encoder.layers.8.norm_out.bias
 (512)" fillcolor=lightblue]
	140597855181056 -> 140597368049520
	140597368049520 [label=AccumulateGrad]
	140597368049424 -> 140597368049328
	140597368049424 -> 140597367450976 [dir=none]
	140597367450976 [label="other
 ()" fillcolor=orange]
	140597368049424 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367853456 -> 140597368049424
	140597367853456 -> 140597367450896 [dir=none]
	140597367450896 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597367853456 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367445200 -> 140597367853456
	140597367445200 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367853120 -> 140597367445200
	140597367853120 -> 140597367451136 [dir=none]
	140597367451136 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367853120 -> 140597367450096 [dir=none]
	140597367450096 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367853120 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366961536 -> 140597367853120
	140597855181536 [label="ct.encoder.layers.9.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597855181536 -> 140597366961536
	140597366961536 [label=AccumulateGrad]
	140597366961728 -> 140597367853120
	140597366961728 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366961872 -> 140597366961728
	140597366961872 -> 140597367451296 [dir=none]
	140597367451296 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366961872 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366962160 -> 140597366961872
	140597366962160 -> 140597480531504 [dir=none]
	140597480531504 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366962160 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366962256 -> 140597366962160
	140597366962256 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366962352 -> 140597366962256
	140597366962352 -> 140597367451376 [dir=none]
	140597367451376 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366962352 -> 140597367451216 [dir=none]
	140597367451216 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366962352 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366962448 -> 140597366962352
	140597855181376 [label="ct.encoder.layers.9.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597855181376 -> 140597366962448
	140597366962448 [label=AccumulateGrad]
	140597366962400 -> 140597366962352
	140597366962400 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366962544 -> 140597366962400
	140597366962544 -> 140597855181216 [dir=none]
	140597855181216 [label="bias
 (512)" fillcolor=orange]
	140597366962544 -> 140597480531424 [dir=none]
	140597480531424 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366962544 -> 140597367451536 [dir=none]
	140597367451536 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366962544 -> 140597367451456 [dir=none]
	140597367451456 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366962544 -> 140597855181136 [dir=none]
	140597855181136 [label="weight
 (512)" fillcolor=orange]
	140597366962544 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597368049472 -> 140597366962544
	140597366962736 -> 140597366962544
	140597855181136 [label="ct.encoder.layers.9.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597855181136 -> 140597366962736
	140597366962736 [label=AccumulateGrad]
	140597366962688 -> 140597366962544
	140597855181216 [label="ct.encoder.layers.9.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597855181216 -> 140597366962688
	140597366962688 [label=AccumulateGrad]
	140597366962064 -> 140597366962352
	140597366962064 [label=TBackward0]
	140597366962784 -> 140597366962064
	140597855181296 [label="ct.encoder.layers.9.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597855181296 -> 140597366962784
	140597366962784 [label=AccumulateGrad]
	140597366960816 -> 140597367853120
	140597366960816 [label=TBackward0]
	140597366962208 -> 140597366960816
	140597855181456 [label="ct.encoder.layers.9.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597855181456 -> 140597366962208
	140597366962208 [label=AccumulateGrad]
	140597368049136 -> 140597368048944
	140597368049136 -> 140597367450496 [dir=none]
	140597367450496 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597368049136 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367444288 -> 140597368049136
	140597367444288 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597368049376 -> 140597367444288
	140597368049376 -> 140597367449616 [dir=none]
	140597367449616 [label="mat1
 (126, 512)" fillcolor=orange]
	140597368049376 -> 140597367451056 [dir=none]
	140597367451056 [label="mat2
 (512, 512)" fillcolor=orange]
	140597368049376 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366962496 -> 140597368049376
	140597854053056 [label="ct.encoder.layers.9.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597854053056 -> 140597366962496
	140597366962496 [label=AccumulateGrad]
	140597366961824 -> 140597368049376
	140597366961824 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366962112 -> 140597366961824
	140597366962112 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597366962592 -> 140597366962112
	140597366962592 [label=CloneBackward0]
	140597366962976 -> 140597366962592
	140597366962976 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366963072 -> 140597366962976
	140597366963072 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597366963168 -> 140597366963072
	140597366963168 -> 140597367013680 [dir=none]
	140597367013680 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597366963168 -> 140597367013520 [dir=none]
	140597367013520 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597366963168 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366963264 -> 140597366963168
	140597366963264 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366963408 -> 140597366963264
	140597366963408 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366963504 -> 140597366963408
	140597366963504 -> 140597367013600 [dir=none]
	140597367013600 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597366963504 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366963600 -> 140597366963504
	140597366963600 -> 140597480532944 [dir=none]
	140597480532944 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366963600 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366963696 -> 140597366963600
	140597366963696 -> 140597367014000 [dir=none]
	140597367014000 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597366963696 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597366963792 -> 140597366963696
	140597366963792 -> 140597480532944 [dir=none]
	140597480532944 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366963792 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366963888 -> 140597366963792
	140597366963888 -> 140597367013920 [dir=none]
	140597367013920 [label="other
 ()" fillcolor=orange]
	140597366963888 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597366963984 -> 140597366963888
	140597366963984 [label="AddBackward0
------------
alpha: 1"]
	140597366964080 -> 140597366963984
	140597366964080 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597366964176 -> 140597366964080
	140597366964176 -> 140597367013840 [dir=none]
	140597367013840 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597366964176 -> 140597367013440 [dir=none]
	140597367013440 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366964176 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367017632 -> 140597366964176
	140597367017632 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367017776 -> 140597367017632
	140597367017776 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367017872 -> 140597367017776
	140597367017872 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367017968 -> 140597367017872
	140597367017968 [label="AddBackward0
------------
alpha: 1"]
	140597367018064 -> 140597367017968
	140597367018064 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367018208 -> 140597367018064
	140597367018208 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367018304 -> 140597367018208
	140597367018304 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367018400 -> 140597367018304
	140597367018400 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367018496 -> 140597367018400
	140597367018496 -> 140597367014160 [dir=none]
	140597367014160 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367018496 -> 140597367014320 [dir=none]
	140597367014320 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367018496 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367018592 -> 140597367018496
	140597854052576 [label="ct.encoder.layers.9.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597854052576 -> 140597367018592
	140597367018592 [label=AccumulateGrad]
	140597367018544 -> 140597367018496
	140597367018544 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367018688 -> 140597367018544
	140597367018688 -> 140597854052416 [dir=none]
	140597854052416 [label="bias
 (512)" fillcolor=orange]
	140597367018688 -> 140597480531744 [dir=none]
	140597480531744 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367018688 -> 140597367014400 [dir=none]
	140597367014400 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367018688 -> 140597367013760 [dir=none]
	140597367013760 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367018688 -> 140597855182736 [dir=none]
	140597855182736 [label="weight
 (512)" fillcolor=orange]
	140597367018688 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597368049328 -> 140597367018688
	140597367018880 -> 140597367018688
	140597855182736 [label="ct.encoder.layers.9.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597855182736 -> 140597367018880
	140597367018880 [label=AccumulateGrad]
	140597367018832 -> 140597367018688
	140597854052416 [label="ct.encoder.layers.9.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597854052416 -> 140597367018832
	140597367018832 [label=AccumulateGrad]
	140597367018112 -> 140597367018496
	140597367018112 [label=TBackward0]
	140597367018928 -> 140597367018112
	140597854052496 [label="ct.encoder.layers.9.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597854052496 -> 140597367018928
	140597367018928 [label=AccumulateGrad]
	140597367018016 -> 140597367017968
	140597854053216 [label="ct.encoder.layers.9.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597854053216 -> 140597367018016
	140597367018016 [label=AccumulateGrad]
	140597367017584 -> 140597366964176
	140597367017584 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367017920 -> 140597367017584
	140597367017920 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367018352 -> 140597367017920
	140597367018352 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367018448 -> 140597367018352
	140597367018448 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367018640 -> 140597367018448
	140597367018640 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367018976 -> 140597367018640
	140597367018976 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367019072 -> 140597367018976
	140597367019072 -> 140597367014560 [dir=none]
	140597367014560 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367019072 -> 140597367014080 [dir=none]
	140597367014080 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367019072 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367019168 -> 140597367019072
	140597854052736 [label="ct.encoder.layers.9.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597854052736 -> 140597367019168
	140597367019168 [label=AccumulateGrad]
	140597367019120 -> 140597367019072
	140597367019120 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367018688 -> 140597367019120
	140597367017728 -> 140597367019072
	140597367017728 [label=TBackward0]
	140597367019360 -> 140597367017728
	140597854052656 [label="ct.encoder.layers.9.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597854052656 -> 140597367019360
	140597367019360 [label=AccumulateGrad]
	140597366964032 -> 140597366963984
	140597366964032 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597366964128 -> 140597366964032
	140597366964128 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367018160 -> 140597366964128
	140597367018160 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367019024 -> 140597367018160
	140597367019024 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367018736 -> 140597367019024
	140597367018736 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367019312 -> 140597367018736
	140597367019312 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367019408 -> 140597367019312
	140597367019408 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367019504 -> 140597367019408
	140597367019504 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367019600 -> 140597367019504
	140597367019600 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367019696 -> 140597367019600
	140597367019696 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367019792 -> 140597367019696
	140597367019792 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367019888 -> 140597367019792
	140597367019888 -> 140597367015120 [dir=none]
	140597367015120 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367019888 -> 140597367014240 [dir=none]
	140597367014240 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367019888 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367019984 -> 140597367019888
	140597367019984 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367020128 -> 140597367019984
	140597367020128 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367020224 -> 140597367020128
	140597367020224 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367020320 -> 140597367020224
	140597367020320 [label="AddBackward0
------------
alpha: 1"]
	140597367018064 -> 140597367020320
	140597367020416 -> 140597367020320
	140597854053296 [label="ct.encoder.layers.9.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597854053296 -> 140597367020416
	140597367020416 [label=AccumulateGrad]
	140597367019936 -> 140597367019888
	140597367019936 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367020272 -> 140597367019936
	140597367020272 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367020512 -> 140597367020272
	140597367020512 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367020560 -> 140597367020512
	140597367020560 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367020608 -> 140597367020560
	140597367020608 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597367020752 -> 140597367020608
	140597367020752 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597367020848 -> 140597367020752
	140597367020848 -> 140597367014640 [dir=none]
	140597367014640 [label="self
 (251, 512)" fillcolor=orange]
	140597367020848 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597367020944 -> 140597367020848
	140597367020944 [label=TBackward0]
	140597367021040 -> 140597367020944
	140597854053136 [label="ct.encoder.layers.9.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597854053136 -> 140597367021040
	140597367021040 [label=AccumulateGrad]
	140597366963216 -> 140597366963168
	140597366963216 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366963552 -> 140597366963216
	140597366963552 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366963744 -> 140597366963552
	140597366963744 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366963936 -> 140597366963744
	140597366963936 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366963312 -> 140597366963936
	140597366963312 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367018256 -> 140597366963312
	140597367018256 -> 140597367014960 [dir=none]
	140597367014960 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367018256 -> 140597367014880 [dir=none]
	140597367014880 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367018256 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367019216 -> 140597367018256
	140597854052896 [label="ct.encoder.layers.9.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597854052896 -> 140597367019216
	140597367019216 [label=AccumulateGrad]
	140597367018784 -> 140597367018256
	140597367018784 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367018688 -> 140597367018784
	140597367017824 -> 140597367018256
	140597367017824 [label=TBackward0]
	140597367019648 -> 140597367017824
	140597854052816 [label="ct.encoder.layers.9.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597854052816 -> 140597367019648
	140597367019648 [label=AccumulateGrad]
	140597366961344 -> 140597368049376
	140597366961344 [label=TBackward0]
	140597366962928 -> 140597366961344
	140597854052976 [label="ct.encoder.layers.9.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597854052976 -> 140597366962928
	140597366962928 [label=AccumulateGrad]
	140597368048896 -> 140597368048800
	140597368048896 -> 140597367014800 [dir=none]
	140597367014800 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597368048896 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367853168 -> 140597368048896
	140597367853168 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366962832 -> 140597367853168
	140597366962832 -> 140597480531984 [dir=none]
	140597480531984 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366962832 -> 140597855182576 [dir=none]
	140597855182576 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597366962832 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366963024 -> 140597366962832
	140597366963024 -> 140597480532224 [dir=none]
	140597480532224 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597366963024 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366963648 -> 140597366963024
	140597366963648 -> 140597480532624 [dir=none]
	140597480532624 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366963648 -> 140597367014480 [dir=none]
	140597367014480 [label="result1
 (512)" fillcolor=orange]
	140597366963648 -> 140597367015280 [dir=none]
	140597367015280 [label="result2
 (512)" fillcolor=orange]
	140597366963648 -> 140597367014720 [dir=none]
	140597367014720 [label="result3
 (0)" fillcolor=orange]
	140597366963648 -> 140597856968768 [dir=none]
	140597856968768 [label="running_mean
 (512)" fillcolor=orange]
	140597366963648 -> 140597855182096 [dir=none]
	140597855182096 [label="running_var
 (512)" fillcolor=orange]
	140597366963648 -> 140597855182176 [dir=none]
	140597855182176 [label="weight
 (512)" fillcolor=orange]
	140597366963648 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597366963360 -> 140597366963648
	140597366963360 -> 140597480532304 [dir=none]
	140597480532304 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366963360 -> 140597855181936 [dir=none]
	140597855181936 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597366963360 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367019840 -> 140597366963360
	140597367019840 -> 140597480532464 [dir=none]
	140597480532464 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367019840 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367020176 -> 140597367019840
	140597367020176 -> 140597480532544 [dir=none]
	140597480532544 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367020176 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367020032 -> 140597367020176
	140597367020032 -> 140597480533184 [dir=none]
	140597480533184 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367020032 -> 140597855181776 [dir=none]
	140597855181776 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367020032 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367020704 -> 140597367020032
	140597367020704 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367020992 -> 140597367020704
	140597367020992 -> 140597855181696 [dir=none]
	140597855181696 [label="bias
 (512)" fillcolor=orange]
	140597367020992 -> 140597480532704 [dir=none]
	140597480532704 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367020992 -> 140597367015680 [dir=none]
	140597367015680 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367020992 -> 140597367015360 [dir=none]
	140597367015360 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367020992 -> 140597855181616 [dir=none]
	140597855181616 [label="weight
 (512)" fillcolor=orange]
	140597367020992 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597368048944 -> 140597367020992
	140597367021088 -> 140597367020992
	140597855181616 [label="ct.encoder.layers.9.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597855181616 -> 140597367021088
	140597367021088 [label=AccumulateGrad]
	140597367021136 -> 140597367020992
	140597855181696 [label="ct.encoder.layers.9.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597855181696 -> 140597367021136
	140597367021136 [label=AccumulateGrad]
	140597367020464 -> 140597367020032
	140597855181776 [label="ct.encoder.layers.9.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597855181776 -> 140597367020464
	140597367020464 [label=AccumulateGrad]
	140597367019264 -> 140597367020032
	140597855181856 [label="ct.encoder.layers.9.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597855181856 -> 140597367019264
	140597367019264 [label=AccumulateGrad]
	140597367019552 -> 140597366963360
	140597855181936 [label="ct.encoder.layers.9.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597855181936 -> 140597367019552
	140597367019552 [label=AccumulateGrad]
	140597367019456 -> 140597366963360
	140597855182016 [label="ct.encoder.layers.9.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597855182016 -> 140597367019456
	140597367019456 [label=AccumulateGrad]
	140597366963840 -> 140597366963648
	140597855182176 [label="ct.encoder.layers.9.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597855182176 -> 140597366963840
	140597366963840 [label=AccumulateGrad]
	140597366962880 -> 140597366963648
	140597855182256 [label="ct.encoder.layers.9.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597855182256 -> 140597366962880
	140597366962880 [label=AccumulateGrad]
	140597366963120 -> 140597366962832
	140597855182576 [label="ct.encoder.layers.9.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597855182576 -> 140597366963120
	140597366963120 [label=AccumulateGrad]
	140597366960960 -> 140597366962832
	140597855182656 [label="ct.encoder.layers.9.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597855182656 -> 140597366960960
	140597366960960 [label=AccumulateGrad]
	140597368048752 -> 140597368049184
	140597368048752 -> 140597367015840 [dir=none]
	140597367015840 [label="other
 ()" fillcolor=orange]
	140597368048752 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597368048992 -> 140597368048752
	140597368048992 -> 140597367015760 [dir=none]
	140597367015760 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597368048992 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366963456 -> 140597368048992
	140597366963456 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366962304 -> 140597366963456
	140597366962304 -> 140597367016000 [dir=none]
	140597367016000 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597366962304 -> 140597367015200 [dir=none]
	140597367015200 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597366962304 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367020368 -> 140597366962304
	140597854053776 [label="ct.encoder.layers.9.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597854053776 -> 140597367020368
	140597367020368 [label=AccumulateGrad]
	140597367017536 -> 140597366962304
	140597367017536 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367020896 -> 140597367017536
	140597367020896 -> 140597367015600 [dir=none]
	140597367015600 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367020896 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367021232 -> 140597367020896
	140597367021232 -> 140597480532064 [dir=none]
	140597480532064 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367021232 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367021328 -> 140597367021232
	140597367021328 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367021424 -> 140597367021328
	140597367021424 -> 140597367015440 [dir=none]
	140597367015440 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367021424 -> 140597367016080 [dir=none]
	140597367016080 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367021424 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367021520 -> 140597367021424
	140597854053616 [label="ct.encoder.layers.9.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854053616 -> 140597367021520
	140597367021520 [label=AccumulateGrad]
	140597367021472 -> 140597367021424
	140597367021472 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367054448 -> 140597367021472
	140597367054448 -> 140597854053456 [dir=none]
	140597854053456 [label="bias
 (512)" fillcolor=orange]
	140597367054448 -> 140597480532864 [dir=none]
	140597480532864 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367054448 -> 140597367016240 [dir=none]
	140597367016240 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367054448 -> 140597367016160 [dir=none]
	140597367016160 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367054448 -> 140597854053376 [dir=none]
	140597854053376 [label="weight
 (512)" fillcolor=orange]
	140597367054448 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597368048800 -> 140597367054448
	140597367054640 -> 140597367054448
	140597854053376 [label="ct.encoder.layers.9.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597854053376 -> 140597367054640
	140597367054640 [label=AccumulateGrad]
	140597367054592 -> 140597367054448
	140597854053456 [label="ct.encoder.layers.9.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597854053456 -> 140597367054592
	140597367054592 [label=AccumulateGrad]
	140597367020080 -> 140597367021424
	140597367020080 [label=TBackward0]
	140597367054688 -> 140597367020080
	140597854053536 [label="ct.encoder.layers.9.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854053536 -> 140597367054688
	140597367054688 [label=AccumulateGrad]
	140597367017680 -> 140597366962304
	140597367017680 [label=TBackward0]
	140597367021280 -> 140597367017680
	140597854053696 [label="ct.encoder.layers.9.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854053696 -> 140597367021280
	140597367021280 [label=AccumulateGrad]
	140597368049088 -> 140597368048608
	140597854053856 [label="ct.encoder.layers.9.norm_out.weight
 (512)" fillcolor=lightblue]
	140597854053856 -> 140597368049088
	140597368049088 [label=AccumulateGrad]
	140597368049040 -> 140597368048608
	140597854053936 [label="ct.encoder.layers.9.norm_out.bias
 (512)" fillcolor=lightblue]
	140597854053936 -> 140597368049040
	140597368049040 [label=AccumulateGrad]
	140597368048512 -> 140597368046304
	140597368048512 -> 140597367016400 [dir=none]
	140597367016400 [label="other
 ()" fillcolor=orange]
	140597368048512 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597368048848 -> 140597368048512
	140597368048848 -> 140597367016320 [dir=none]
	140597367016320 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597368048848 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366962640 -> 140597368048848
	140597366962640 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367021184 -> 140597366962640
	140597367021184 -> 140597367016560 [dir=none]
	140597367016560 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367021184 -> 140597367015520 [dir=none]
	140597367015520 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367021184 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367020800 -> 140597367021184
	140597854054416 [label="ct.encoder.layers.10.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597854054416 -> 140597367020800
	140597367020800 [label=AccumulateGrad]
	140597367021376 -> 140597367021184
	140597367021376 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367054544 -> 140597367021376
	140597367054544 -> 140597367016720 [dir=none]
	140597367016720 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367054544 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367054832 -> 140597367054544
	140597367054832 -> 140597480533584 [dir=none]
	140597480533584 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367054832 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367054928 -> 140597367054832
	140597367054928 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367055024 -> 140597367054928
	140597367055024 -> 140597367016800 [dir=none]
	140597367016800 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367055024 -> 140597367016640 [dir=none]
	140597367016640 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367055024 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367055120 -> 140597367055024
	140597854054256 [label="ct.encoder.layers.10.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854054256 -> 140597367055120
	140597367055120 [label=AccumulateGrad]
	140597367055072 -> 140597367055024
	140597367055072 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367055216 -> 140597367055072
	140597367055216 -> 140597854054096 [dir=none]
	140597854054096 [label="bias
 (512)" fillcolor=orange]
	140597367055216 -> 140597480533504 [dir=none]
	140597480533504 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367055216 -> 140597367016960 [dir=none]
	140597367016960 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367055216 -> 140597367016880 [dir=none]
	140597367016880 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367055216 -> 140597854054016 [dir=none]
	140597854054016 [label="weight
 (512)" fillcolor=orange]
	140597367055216 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597368048608 -> 140597367055216
	140597367055408 -> 140597367055216
	140597854054016 [label="ct.encoder.layers.10.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597854054016 -> 140597367055408
	140597367055408 [label=AccumulateGrad]
	140597367055360 -> 140597367055216
	140597854054096 [label="ct.encoder.layers.10.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597854054096 -> 140597367055360
	140597367055360 [label=AccumulateGrad]
	140597367054736 -> 140597367055024
	140597367054736 [label=TBackward0]
	140597367055456 -> 140597367054736
	140597854054176 [label="ct.encoder.layers.10.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854054176 -> 140597367055456
	140597367055456 [label=AccumulateGrad]
	140597367019744 -> 140597367021184
	140597367019744 [label=TBackward0]
	140597367054880 -> 140597367019744
	140597854054336 [label="ct.encoder.layers.10.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854054336 -> 140597367054880
	140597367054880 [label=AccumulateGrad]
	140597368046400 -> 140597480053728
	140597368046400 -> 140597367017120 [dir=none]
	140597367017120 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597368046400 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597368048656 -> 140597368046400
	140597368048656 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367020656 -> 140597368048656
	140597367020656 -> 140597367017200 [dir=none]
	140597367017200 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367020656 -> 140597367016480 [dir=none]
	140597367016480 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367020656 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367055168 -> 140597367020656
	140597854056256 [label="ct.encoder.layers.10.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597854056256 -> 140597367055168
	140597367055168 [label=AccumulateGrad]
	140597367054496 -> 140597367020656
	140597367054496 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367054784 -> 140597367054496
	140597367054784 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367055264 -> 140597367054784
	140597367055264 [label=CloneBackward0]
	140597367055648 -> 140597367055264
	140597367055648 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367055744 -> 140597367055648
	140597367055744 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367055840 -> 140597367055744
	140597367055840 -> 140597367017360 [dir=none]
	140597367017360 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367055840 -> 140597367017280 [dir=none]
	140597367017280 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367055840 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367055936 -> 140597367055840
	140597367055936 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367056080 -> 140597367055936
	140597367056080 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367056176 -> 140597367056080
	140597367056176 -> 140597367015040 [dir=none]
	140597367015040 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367056176 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367056272 -> 140597367056176
	140597367056272 -> 140597480535120 [dir=none]
	140597480535120 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367056272 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367056368 -> 140597367056272
	140597367056368 -> 140597367015920 [dir=none]
	140597367015920 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367056368 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367056464 -> 140597367056368
	140597367056464 -> 140597480535120 [dir=none]
	140597480535120 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367056464 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367056560 -> 140597367056464
	140597367056560 -> 140597367017040 [dir=none]
	140597367017040 [label="other
 ()" fillcolor=orange]
	140597367056560 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367056656 -> 140597367056560
	140597367056656 [label="AddBackward0
------------
alpha: 1"]
	140597367056752 -> 140597367056656
	140597367056752 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367056896 -> 140597367056752
	140597367056896 -> 140597367091424 [dir=none]
	140597367091424 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367056896 -> 140597367091344 [dir=none]
	140597367091344 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367056896 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367056992 -> 140597367056896
	140597367056992 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367057136 -> 140597367056992
	140597367057136 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367057232 -> 140597367057136
	140597367057232 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367057328 -> 140597367057232
	140597367057328 [label="AddBackward0
------------
alpha: 1"]
	140597367057424 -> 140597367057328
	140597367057424 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367057568 -> 140597367057424
	140597367057568 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367057664 -> 140597367057568
	140597367057664 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367057760 -> 140597367057664
	140597367057760 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367057856 -> 140597367057760
	140597367057856 -> 140597367091584 [dir=none]
	140597367091584 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367057856 -> 140597367091744 [dir=none]
	140597367091744 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367057856 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367057952 -> 140597367057856
	140597854055776 [label="ct.encoder.layers.10.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597854055776 -> 140597367057952
	140597367057952 [label=AccumulateGrad]
	140597367057904 -> 140597367057856
	140597367057904 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367058048 -> 140597367057904
	140597367058048 -> 140597854055616 [dir=none]
	140597854055616 [label="bias
 (512)" fillcolor=orange]
	140597367058048 -> 140597480533824 [dir=none]
	140597480533824 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367058048 -> 140597367091824 [dir=none]
	140597367091824 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367058048 -> 140597367091264 [dir=none]
	140597367091264 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367058048 -> 140597854055536 [dir=none]
	140597854055536 [label="weight
 (512)" fillcolor=orange]
	140597367058048 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597368046304 -> 140597367058048
	140597367058240 -> 140597367058048
	140597854055536 [label="ct.encoder.layers.10.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597854055536 -> 140597367058240
	140597367058240 [label=AccumulateGrad]
	140597367058192 -> 140597367058048
	140597854055616 [label="ct.encoder.layers.10.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597854055616 -> 140597367058192
	140597367058192 [label=AccumulateGrad]
	140597367057472 -> 140597367057856
	140597367057472 [label=TBackward0]
	140597367058288 -> 140597367057472
	140597854055696 [label="ct.encoder.layers.10.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597854055696 -> 140597367058288
	140597367058288 [label=AccumulateGrad]
	140597367057376 -> 140597367057328
	140597854175296 [label="ct.encoder.layers.10.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597854175296 -> 140597367057376
	140597367057376 [label=AccumulateGrad]
	140597367056944 -> 140597367056896
	140597367056944 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367057280 -> 140597367056944
	140597367057280 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367057712 -> 140597367057280
	140597367057712 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367057808 -> 140597367057712
	140597367057808 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367058000 -> 140597367057808
	140597367058000 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367058336 -> 140597367058000
	140597367058336 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367058096 -> 140597367058336
	140597367058096 -> 140597367091984 [dir=none]
	140597367091984 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367058096 -> 140597367091504 [dir=none]
	140597367091504 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367058096 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367107648 -> 140597367058096
	140597854055936 [label="ct.encoder.layers.10.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597854055936 -> 140597367107648
	140597367107648 [label=AccumulateGrad]
	140597367107696 -> 140597367058096
	140597367107696 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367058048 -> 140597367107696
	140597367107744 -> 140597367058096
	140597367107744 [label=TBackward0]
	140597367107936 -> 140597367107744
	140597854055856 [label="ct.encoder.layers.10.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597854055856 -> 140597367107936
	140597367107936 [label=AccumulateGrad]
	140597367056704 -> 140597367056656
	140597367056704 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597367057184 -> 140597367056704
	140597367057184 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367057520 -> 140597367057184
	140597367057520 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367058384 -> 140597367057520
	140597367058384 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367057088 -> 140597367058384
	140597367057088 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367056848 -> 140597367057088
	140597367056848 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367107984 -> 140597367056848
	140597367107984 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367108080 -> 140597367107984
	140597367108080 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367108176 -> 140597367108080
	140597367108176 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367108272 -> 140597367108176
	140597367108272 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367108368 -> 140597367108272
	140597367108368 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367108464 -> 140597367108368
	140597367108464 -> 140597367092544 [dir=none]
	140597367092544 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367108464 -> 140597367091664 [dir=none]
	140597367091664 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367108464 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367108560 -> 140597367108464
	140597367108560 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367108704 -> 140597367108560
	140597367108704 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367108800 -> 140597367108704
	140597367108800 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367108896 -> 140597367108800
	140597367108896 [label="AddBackward0
------------
alpha: 1"]
	140597367057424 -> 140597367108896
	140597367108992 -> 140597367108896
	140597854175376 [label="ct.encoder.layers.10.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597854175376 -> 140597367108992
	140597367108992 [label=AccumulateGrad]
	140597367108512 -> 140597367108464
	140597367108512 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367108848 -> 140597367108512
	140597367108848 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367109088 -> 140597367108848
	140597367109088 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367109136 -> 140597367109088
	140597367109136 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367109184 -> 140597367109136
	140597367109184 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597367109328 -> 140597367109184
	140597367109328 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597367109424 -> 140597367109328
	140597367109424 -> 140597367092064 [dir=none]
	140597367092064 [label="self
 (251, 512)" fillcolor=orange]
	140597367109424 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597367109520 -> 140597367109424
	140597367109520 [label=TBackward0]
	140597367109616 -> 140597367109520
	140597854056336 [label="ct.encoder.layers.10.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597854056336 -> 140597367109616
	140597367109616 [label=AccumulateGrad]
	140597367055888 -> 140597367055840
	140597367055888 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367056224 -> 140597367055888
	140597367056224 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367056416 -> 140597367056224
	140597367056416 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367056608 -> 140597367056416
	140597367056608 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367056800 -> 140597367056608
	140597367056800 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367057616 -> 140597367056800
	140597367057616 -> 140597367092384 [dir=none]
	140597367092384 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367057616 -> 140597367092304 [dir=none]
	140597367092304 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367057616 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367058144 -> 140597367057616
	140597854056096 [label="ct.encoder.layers.10.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597854056096 -> 140597367058144
	140597367058144 [label=AccumulateGrad]
	140597367056032 -> 140597367057616
	140597367056032 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367058048 -> 140597367056032
	140597367107888 -> 140597367057616
	140597367107888 [label=TBackward0]
	140597367108224 -> 140597367107888
	140597854056016 [label="ct.encoder.layers.10.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597854056016 -> 140597367108224
	140597367108224 [label=AccumulateGrad]
	140597367054400 -> 140597367020656
	140597367054400 [label=TBackward0]
	140597367055600 -> 140597367054400
	140597854056176 [label="ct.encoder.layers.10.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597854056176 -> 140597367055600
	140597367055600 [label=AccumulateGrad]
	140597480053584 -> 140597480054016
	140597480053584 -> 140597367092224 [dir=none]
	140597367092224 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480053584 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597368046256 -> 140597480053584
	140597368046256 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597368046496 -> 140597368046256
	140597368046496 -> 140597480534160 [dir=none]
	140597480534160 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597368046496 -> 140597854055376 [dir=none]
	140597854055376 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597368046496 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367055696 -> 140597368046496
	140597367055696 -> 140597480534400 [dir=none]
	140597480534400 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367055696 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367056320 -> 140597367055696
	140597367056320 -> 140597480534800 [dir=none]
	140597480534800 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367056320 -> 140597367091904 [dir=none]
	140597367091904 [label="result1
 (512)" fillcolor=orange]
	140597367056320 -> 140597367092704 [dir=none]
	140597367092704 [label="result2
 (512)" fillcolor=orange]
	140597367056320 -> 140597367092144 [dir=none]
	140597367092144 [label="result3
 (0)" fillcolor=orange]
	140597367056320 -> 140597855620128 [dir=none]
	140597855620128 [label="running_mean
 (512)" fillcolor=orange]
	140597367056320 -> 140597855182496 [dir=none]
	140597855182496 [label="running_var
 (512)" fillcolor=orange]
	140597367056320 -> 140597854054976 [dir=none]
	140597854054976 [label="weight
 (512)" fillcolor=orange]
	140597367056320 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367055984 -> 140597367056320
	140597367055984 -> 140597480534480 [dir=none]
	140597480534480 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367055984 -> 140597854054816 [dir=none]
	140597854054816 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367055984 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367108416 -> 140597367055984
	140597367108416 -> 140597480534640 [dir=none]
	140597480534640 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367108416 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367108752 -> 140597367108416
	140597367108752 -> 140597480534720 [dir=none]
	140597480534720 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367108752 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367108608 -> 140597367108752
	140597367108608 -> 140597480535360 [dir=none]
	140597480535360 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367108608 -> 140597854054656 [dir=none]
	140597854054656 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367108608 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367109280 -> 140597367108608
	140597367109280 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367109568 -> 140597367109280
	140597367109568 -> 140597854054576 [dir=none]
	140597854054576 [label="bias
 (512)" fillcolor=orange]
	140597367109568 -> 140597480534880 [dir=none]
	140597480534880 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367109568 -> 140597367093104 [dir=none]
	140597367093104 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367109568 -> 140597367092784 [dir=none]
	140597367092784 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367109568 -> 140597854054496 [dir=none]
	140597854054496 [label="weight
 (512)" fillcolor=orange]
	140597367109568 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480053728 -> 140597367109568
	140597367109664 -> 140597367109568
	140597854054496 [label="ct.encoder.layers.10.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597854054496 -> 140597367109664
	140597367109664 [label=AccumulateGrad]
	140597367109712 -> 140597367109568
	140597854054576 [label="ct.encoder.layers.10.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597854054576 -> 140597367109712
	140597367109712 [label=AccumulateGrad]
	140597367109040 -> 140597367108608
	140597854054656 [label="ct.encoder.layers.10.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597854054656 -> 140597367109040
	140597367109040 [label=AccumulateGrad]
	140597367107840 -> 140597367108608
	140597854054736 [label="ct.encoder.layers.10.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597854054736 -> 140597367107840
	140597367107840 [label=AccumulateGrad]
	140597367108128 -> 140597367055984
	140597854054816 [label="ct.encoder.layers.10.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597854054816 -> 140597367108128
	140597367108128 [label=AccumulateGrad]
	140597367108032 -> 140597367055984
	140597854054896 [label="ct.encoder.layers.10.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597854054896 -> 140597367108032
	140597367108032 [label=AccumulateGrad]
	140597367056512 -> 140597367056320
	140597854054976 [label="ct.encoder.layers.10.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597854054976 -> 140597367056512
	140597367056512 [label=AccumulateGrad]
	140597367055552 -> 140597367056320
	140597854055056 [label="ct.encoder.layers.10.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597854055056 -> 140597367055552
	140597367055552 [label=AccumulateGrad]
	140597367055792 -> 140597368046496
	140597854055376 [label="ct.encoder.layers.10.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597854055376 -> 140597367055792
	140597367055792 [label=AccumulateGrad]
	140597367054976 -> 140597368046496
	140597854055456 [label="ct.encoder.layers.10.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597854055456 -> 140597367054976
	140597367054976 [label=AccumulateGrad]
	140597480053920 -> 140597480054256
	140597480053920 -> 140597367093264 [dir=none]
	140597367093264 [label="other
 ()" fillcolor=orange]
	140597480053920 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597368048704 -> 140597480053920
	140597368048704 -> 140597367093184 [dir=none]
	140597367093184 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597368048704 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367056128 -> 140597368048704
	140597367056128 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367057040 -> 140597367056128
	140597367057040 -> 140597367093424 [dir=none]
	140597367093424 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367057040 -> 140597367092624 [dir=none]
	140597367092624 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367057040 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367108944 -> 140597367057040
	140597854175856 [label="ct.encoder.layers.10.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597854175856 -> 140597367108944
	140597367108944 [label=AccumulateGrad]
	140597367107792 -> 140597367057040
	140597367107792 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367109472 -> 140597367107792
	140597367109472 -> 140597367093024 [dir=none]
	140597367093024 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367109472 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367109808 -> 140597367109472
	140597367109808 -> 140597480534240 [dir=none]
	140597480534240 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367109808 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367109904 -> 140597367109808
	140597367109904 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367110000 -> 140597367109904
	140597367110000 -> 140597367092864 [dir=none]
	140597367092864 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367110000 -> 140597367093504 [dir=none]
	140597367093504 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367110000 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367110096 -> 140597367110000
	140597854175696 [label="ct.encoder.layers.10.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854175696 -> 140597367110096
	140597367110096 [label=AccumulateGrad]
	140597367110048 -> 140597367110000
	140597367110048 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367110192 -> 140597367110048
	140597367110192 -> 140597854175536 [dir=none]
	140597854175536 [label="bias
 (512)" fillcolor=orange]
	140597367110192 -> 140597480535040 [dir=none]
	140597480535040 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367110192 -> 140597367093664 [dir=none]
	140597367093664 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367110192 -> 140597367093584 [dir=none]
	140597367093584 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367110192 -> 140597854175456 [dir=none]
	140597854175456 [label="weight
 (512)" fillcolor=orange]
	140597367110192 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480054016 -> 140597367110192
	140597367110384 -> 140597367110192
	140597854175456 [label="ct.encoder.layers.10.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597854175456 -> 140597367110384
	140597367110384 [label=AccumulateGrad]
	140597367110336 -> 140597367110192
	140597854175536 [label="ct.encoder.layers.10.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597854175536 -> 140597367110336
	140597367110336 [label=AccumulateGrad]
	140597367108656 -> 140597367110000
	140597367108656 [label=TBackward0]
	140597367110432 -> 140597367108656
	140597854175616 [label="ct.encoder.layers.10.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854175616 -> 140597367110432
	140597367110432 [label=AccumulateGrad]
	140597367108320 -> 140597367057040
	140597367108320 [label=TBackward0]
	140597367109856 -> 140597367108320
	140597854175776 [label="ct.encoder.layers.10.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854175776 -> 140597367109856
	140597367109856 [label=AccumulateGrad]
	140597480054208 -> 140597480054592
	140597854175936 [label="ct.encoder.layers.10.norm_out.weight
 (512)" fillcolor=lightblue]
	140597854175936 -> 140597480054208
	140597480054208 [label=AccumulateGrad]
	140597480054688 -> 140597480054592
	140597854176016 [label="ct.encoder.layers.10.norm_out.bias
 (512)" fillcolor=lightblue]
	140597854176016 -> 140597480054688
	140597480054688 [label=AccumulateGrad]
	140597480054544 -> 140597480029632
	140597480054544 -> 140597367093824 [dir=none]
	140597367093824 [label="other
 ()" fillcolor=orange]
	140597480054544 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480053440 -> 140597480054544
	140597480053440 -> 140597367093744 [dir=none]
	140597367093744 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480053440 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367055504 -> 140597480053440
	140597367055504 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597480054352 -> 140597367055504
	140597480054352 -> 140597367093984 [dir=none]
	140597367093984 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597480054352 -> 140597367092944 [dir=none]
	140597367092944 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597480054352 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367109952 -> 140597480054352
	140597854176496 [label="ct.encoder.layers.11.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597854176496 -> 140597367109952
	140597367109952 [label=AccumulateGrad]
	140597367110144 -> 140597480054352
	140597367110144 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597367110288 -> 140597367110144
	140597367110288 -> 140597367094144 [dir=none]
	140597367094144 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597367110288 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367110576 -> 140597367110288
	140597367110576 -> 140597480535760 [dir=none]
	140597480535760 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597367110576 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367110672 -> 140597367110576
	140597367110672 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597367110768 -> 140597367110672
	140597367110768 -> 140597367094224 [dir=none]
	140597367094224 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367110768 -> 140597367094064 [dir=none]
	140597367094064 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597367110768 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597367110864 -> 140597367110768
	140597854176336 [label="ct.encoder.layers.11.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854176336 -> 140597367110864
	140597367110864 [label=AccumulateGrad]
	140597367110816 -> 140597367110768
	140597367110816 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367110960 -> 140597367110816
	140597367110960 -> 140597854176176 [dir=none]
	140597854176176 [label="bias
 (512)" fillcolor=orange]
	140597367110960 -> 140597480535680 [dir=none]
	140597480535680 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367110960 -> 140597367094384 [dir=none]
	140597367094384 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367110960 -> 140597367094304 [dir=none]
	140597367094304 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367110960 -> 140597854176096 [dir=none]
	140597854176096 [label="weight
 (512)" fillcolor=orange]
	140597367110960 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480054592 -> 140597367110960
	140597367111152 -> 140597367110960
	140597854176096 [label="ct.encoder.layers.11.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597854176096 -> 140597367111152
	140597367111152 [label=AccumulateGrad]
	140597367111104 -> 140597367110960
	140597854176176 [label="ct.encoder.layers.11.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597854176176 -> 140597367111104
	140597367111104 [label=AccumulateGrad]
	140597367110480 -> 140597367110768
	140597367110480 [label=TBackward0]
	140597367111200 -> 140597367110480
	140597854176256 [label="ct.encoder.layers.11.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854176256 -> 140597367111200
	140597367111200 [label=AccumulateGrad]
	140597367109232 -> 140597480054352
	140597367109232 [label=TBackward0]
	140597367110624 -> 140597367109232
	140597854176416 [label="ct.encoder.layers.11.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854176416 -> 140597367110624
	140597367110624 [label=AccumulateGrad]
	140597480028144 -> 140597480028672
	140597480028144 -> 140597367094544 [dir=none]
	140597367094544 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480028144 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367055312 -> 140597480028144
	140597367055312 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597480053392 -> 140597367055312
	140597480053392 -> 140597367094624 [dir=none]
	140597367094624 [label="mat1
 (126, 512)" fillcolor=orange]
	140597480053392 -> 140597367093904 [dir=none]
	140597367093904 [label="mat2
 (512, 512)" fillcolor=orange]
	140597480053392 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367110912 -> 140597480053392
	140597854178336 [label="ct.encoder.layers.11.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597854178336 -> 140597367110912
	140597367110912 [label=AccumulateGrad]
	140597367110240 -> 140597480053392
	140597367110240 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367110528 -> 140597367110240
	140597367110528 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597367111008 -> 140597367110528
	140597367111008 [label=CloneBackward0]
	140597367111392 -> 140597367111008
	140597367111392 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367111488 -> 140597367111392
	140597367111488 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597367111584 -> 140597367111488
	140597367111584 -> 140597367094784 [dir=none]
	140597367094784 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597367111584 -> 140597367094704 [dir=none]
	140597367094704 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597367111584 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367111632 -> 140597367111584
	140597367111632 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367156944 -> 140597367111632
	140597367156944 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597367157040 -> 140597367156944
	140597367157040 -> 140597367094464 [dir=none]
	140597367094464 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597367157040 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367157136 -> 140597367157040
	140597367157136 -> 140597480537200 [dir=none]
	140597480537200 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367157136 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367157232 -> 140597367157136
	140597367157232 -> 140597367095104 [dir=none]
	140597367095104 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597367157232 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597367157328 -> 140597367157232
	140597367157328 -> 140597480537200 [dir=none]
	140597480537200 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597367157328 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367157424 -> 140597367157328
	140597367157424 -> 140597367095024 [dir=none]
	140597367095024 [label="other
 ()" fillcolor=orange]
	140597367157424 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597367157520 -> 140597367157424
	140597367157520 [label="AddBackward0
------------
alpha: 1"]
	140597367157616 -> 140597367157520
	140597367157616 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597367157760 -> 140597367157616
	140597367157760 -> 140597367094944 [dir=none]
	140597367094944 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597367157760 -> 140597367094864 [dir=none]
	140597367094864 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367157760 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367157856 -> 140597367157760
	140597367157856 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367158000 -> 140597367157856
	140597367158000 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367158096 -> 140597367158000
	140597367158096 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367158192 -> 140597367158096
	140597367158192 [label="AddBackward0
------------
alpha: 1"]
	140597367158288 -> 140597367158192
	140597367158288 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367158432 -> 140597367158288
	140597367158432 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367158528 -> 140597367158432
	140597367158528 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367158624 -> 140597367158528
	140597367158624 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367158720 -> 140597367158624
	140597367158720 -> 140597367093344 [dir=none]
	140597367093344 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367158720 -> 140597367092464 [dir=none]
	140597367092464 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367158720 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367158816 -> 140597367158720
	140597854177856 [label="ct.encoder.layers.11.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597854177856 -> 140597367158816
	140597367158816 [label=AccumulateGrad]
	140597367158768 -> 140597367158720
	140597367158768 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367158912 -> 140597367158768
	140597367158912 -> 140597854177696 [dir=none]
	140597854177696 [label="bias
 (512)" fillcolor=orange]
	140597367158912 -> 140597480536000 [dir=none]
	140597480536000 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597367158912 -> 140597367095184 [dir=none]
	140597367095184 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597367158912 -> 140597367185552 [dir=none]
	140597367185552 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597367158912 -> 140597854177616 [dir=none]
	140597854177616 [label="weight
 (512)" fillcolor=orange]
	140597367158912 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480029632 -> 140597367158912
	140597367159104 -> 140597367158912
	140597854177616 [label="ct.encoder.layers.11.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597854177616 -> 140597367159104
	140597367159104 [label=AccumulateGrad]
	140597367159056 -> 140597367158912
	140597854177696 [label="ct.encoder.layers.11.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597854177696 -> 140597367159056
	140597367159056 [label=AccumulateGrad]
	140597367158336 -> 140597367158720
	140597367158336 [label=TBackward0]
	140597367159152 -> 140597367158336
	140597854177776 [label="ct.encoder.layers.11.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597854177776 -> 140597367159152
	140597367159152 [label=AccumulateGrad]
	140597367158240 -> 140597367158192
	140597854178496 [label="ct.encoder.layers.11.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597854178496 -> 140597367158240
	140597367158240 [label=AccumulateGrad]
	140597367157808 -> 140597367157760
	140597367157808 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367158144 -> 140597367157808
	140597367158144 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597367158576 -> 140597367158144
	140597367158576 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367158672 -> 140597367158576
	140597367158672 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367158864 -> 140597367158672
	140597367158864 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367159200 -> 140597367158864
	140597367159200 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367159296 -> 140597367159200
	140597367159296 -> 140597367185712 [dir=none]
	140597367185712 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367159296 -> 140597367185472 [dir=none]
	140597367185472 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367159296 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367159392 -> 140597367159296
	140597854178016 [label="ct.encoder.layers.11.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597854178016 -> 140597367159392
	140597367159392 [label=AccumulateGrad]
	140597367159344 -> 140597367159296
	140597367159344 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367158912 -> 140597367159344
	140597367157952 -> 140597367159296
	140597367157952 [label=TBackward0]
	140597367159584 -> 140597367157952
	140597854177936 [label="ct.encoder.layers.11.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597854177936 -> 140597367159584
	140597367159584 [label=AccumulateGrad]
	140597367157568 -> 140597367157520
	140597367157568 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597367158048 -> 140597367157568
	140597367158048 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367158384 -> 140597367158048
	140597367158384 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367159248 -> 140597367158384
	140597367159248 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597367158960 -> 140597367159248
	140597367158960 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597367159536 -> 140597367158960
	140597367159536 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597367159632 -> 140597367159536
	140597367159632 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367159728 -> 140597367159632
	140597367159728 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597367159824 -> 140597367159728
	140597367159824 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597367159920 -> 140597367159824
	140597367159920 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597367160016 -> 140597367159920
	140597367160016 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597367160112 -> 140597367160016
	140597367160112 -> 140597367186352 [dir=none]
	140597367186352 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597367160112 -> 140597367186112 [dir=none]
	140597367186112 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597367160112 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597367160208 -> 140597367160112
	140597367160208 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367160352 -> 140597367160208
	140597367160352 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367160448 -> 140597367160352
	140597367160448 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367160544 -> 140597367160448
	140597367160544 [label="AddBackward0
------------
alpha: 1"]
	140597367158288 -> 140597367160544
	140597367160640 -> 140597367160544
	140597854178576 [label="ct.encoder.layers.11.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597854178576 -> 140597367160640
	140597367160640 [label=AccumulateGrad]
	140597367160160 -> 140597367160112
	140597367160160 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367160496 -> 140597367160160
	140597367160496 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597367160736 -> 140597367160496
	140597367160736 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597367160784 -> 140597367160736
	140597367160784 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367160304 -> 140597367160784
	140597367160304 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597366669520 -> 140597367160304
	140597366669520 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597366669616 -> 140597366669520
	140597366669616 -> 140597367185792 [dir=none]
	140597367185792 [label="self
 (251, 512)" fillcolor=orange]
	140597366669616 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597366669712 -> 140597366669616
	140597366669712 [label=TBackward0]
	140597366669808 -> 140597366669712
	140597854178416 [label="ct.encoder.layers.11.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597854178416 -> 140597366669808
	140597366669808 [label=AccumulateGrad]
	140597367111296 -> 140597367111584
	140597367111296 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367157088 -> 140597367111296
	140597367157088 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597367157280 -> 140597367157088
	140597367157280 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367157472 -> 140597367157280
	140597367157472 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367157664 -> 140597367157472
	140597367157664 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367158480 -> 140597367157664
	140597367158480 -> 140597367186192 [dir=none]
	140597367186192 [label="mat1
 (126, 512)" fillcolor=orange]
	140597367158480 -> 140597367186032 [dir=none]
	140597367186032 [label="mat2
 (512, 512)" fillcolor=orange]
	140597367158480 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597367159440 -> 140597367158480
	140597854178176 [label="ct.encoder.layers.11.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597854178176 -> 140597367159440
	140597367159440 [label=AccumulateGrad]
	140597367159008 -> 140597367158480
	140597367159008 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597367158912 -> 140597367159008
	140597367156896 -> 140597367158480
	140597367156896 [label=TBackward0]
	140597367159872 -> 140597367156896
	140597854178096 [label="ct.encoder.layers.11.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597854178096 -> 140597367159872
	140597367159872 [label=AccumulateGrad]
	140597367109760 -> 140597480053392
	140597367109760 [label=TBackward0]
	140597367111344 -> 140597367109760
	140597854178256 [label="ct.encoder.layers.11.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597854178256 -> 140597367111344
	140597367111344 [label=AccumulateGrad]
	140597480028624 -> 140597480028960
	140597480028624 -> 140597367185952 [dir=none]
	140597367185952 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480028624 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597480053872 -> 140597480028624
	140597480053872 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597367111248 -> 140597480053872
	140597367111248 -> 140597480536240 [dir=none]
	140597480536240 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367111248 -> 140597854177456 [dir=none]
	140597854177456 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597367111248 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367111440 -> 140597367111248
	140597367111440 -> 140597480536480 [dir=none]
	140597480536480 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597367111440 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597367157184 -> 140597367111440
	140597367157184 -> 140597480536880 [dir=none]
	140597480536880 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367157184 -> 140597367185632 [dir=none]
	140597367185632 [label="result1
 (512)" fillcolor=orange]
	140597367157184 -> 140597367186512 [dir=none]
	140597367186512 [label="result2
 (512)" fillcolor=orange]
	140597367157184 -> 140597367185872 [dir=none]
	140597367185872 [label="result3
 (0)" fillcolor=orange]
	140597367157184 -> 140597855182416 [dir=none]
	140597855182416 [label="running_mean
 (512)" fillcolor=orange]
	140597367157184 -> 140597854055296 [dir=none]
	140597854055296 [label="running_var
 (512)" fillcolor=orange]
	140597367157184 -> 140597854177056 [dir=none]
	140597854177056 [label="weight
 (512)" fillcolor=orange]
	140597367157184 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597367156848 -> 140597367157184
	140597367156848 -> 140597480536560 [dir=none]
	140597480536560 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367156848 -> 140597854176896 [dir=none]
	140597854176896 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597367156848 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367160064 -> 140597367156848
	140597367160064 -> 140597480536720 [dir=none]
	140597480536720 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597367160064 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597367160400 -> 140597367160064
	140597367160400 -> 140597480536800 [dir=none]
	140597480536800 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597367160400 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597367160256 -> 140597367160400
	140597367160256 -> 140597480537440 [dir=none]
	140597480537440 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597367160256 -> 140597854176736 [dir=none]
	140597854176736 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597367160256 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597367160688 -> 140597367160256
	140597367160688 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366669760 -> 140597367160688
	140597366669760 -> 140597854176656 [dir=none]
	140597854176656 [label="bias
 (512)" fillcolor=orange]
	140597366669760 -> 140597480536960 [dir=none]
	140597480536960 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366669760 -> 140597367186912 [dir=none]
	140597367186912 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366669760 -> 140597367186592 [dir=none]
	140597367186592 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366669760 -> 140597854176576 [dir=none]
	140597854176576 [label="weight
 (512)" fillcolor=orange]
	140597366669760 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480028672 -> 140597366669760
	140597366669856 -> 140597366669760
	140597854176576 [label="ct.encoder.layers.11.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597854176576 -> 140597366669856
	140597366669856 [label=AccumulateGrad]
	140597366669904 -> 140597366669760
	140597854176656 [label="ct.encoder.layers.11.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597854176656 -> 140597366669904
	140597366669904 [label=AccumulateGrad]
	140597367159488 -> 140597367160256
	140597854176736 [label="ct.encoder.layers.11.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597854176736 -> 140597367159488
	140597367159488 [label=AccumulateGrad]
	140597366669376 -> 140597367160256
	140597854176816 [label="ct.encoder.layers.11.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597854176816 -> 140597366669376
	140597366669376 [label=AccumulateGrad]
	140597367159776 -> 140597367156848
	140597854176896 [label="ct.encoder.layers.11.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597854176896 -> 140597367159776
	140597367159776 [label=AccumulateGrad]
	140597367159680 -> 140597367156848
	140597854176976 [label="ct.encoder.layers.11.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597854176976 -> 140597367159680
	140597367159680 [label=AccumulateGrad]
	140597367157376 -> 140597367157184
	140597854177056 [label="ct.encoder.layers.11.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597854177056 -> 140597367157376
	140597367157376 [label=AccumulateGrad]
	140597367156800 -> 140597367157184
	140597854177136 [label="ct.encoder.layers.11.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597854177136 -> 140597367156800
	140597367156800 [label=AccumulateGrad]
	140597367111536 -> 140597367111248
	140597854177456 [label="ct.encoder.layers.11.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597854177456 -> 140597367111536
	140597367111536 [label=AccumulateGrad]
	140597367109376 -> 140597367111248
	140597854177536 [label="ct.encoder.layers.11.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597854177536 -> 140597367109376
	140597367109376 [label=AccumulateGrad]
	140597480028912 -> 140597480028720
	140597480028912 -> 140597367187072 [dir=none]
	140597367187072 [label="other
 ()" fillcolor=orange]
	140597480028912 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480053200 -> 140597480028912
	140597480053200 -> 140597367186992 [dir=none]
	140597367186992 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480053200 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367111056 -> 140597480053200
	140597367111056 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597367159968 -> 140597367111056
	140597367159968 -> 140597367187232 [dir=none]
	140597367187232 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597367159968 -> 140597367186432 [dir=none]
	140597367186432 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597367159968 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597367160592 -> 140597367159968
	140597854179056 [label="ct.encoder.layers.11.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597854179056 -> 140597367160592
	140597367160592 [label=AccumulateGrad]
	140597367157712 -> 140597367159968
	140597367157712 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366669664 -> 140597367157712
	140597366669664 -> 140597367186832 [dir=none]
	140597367186832 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366669664 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366670000 -> 140597366669664
	140597366670000 -> 140597480536320 [dir=none]
	140597480536320 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366670000 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366670096 -> 140597366670000
	140597366670096 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366670192 -> 140597366670096
	140597366670192 -> 140597367186672 [dir=none]
	140597367186672 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366670192 -> 140597367187312 [dir=none]
	140597367187312 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366670192 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366670288 -> 140597366670192
	140597854178896 [label="ct.encoder.layers.11.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597854178896 -> 140597366670288
	140597366670288 [label=AccumulateGrad]
	140597366670240 -> 140597366670192
	140597366670240 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366670384 -> 140597366670240
	140597366670384 -> 140597854178736 [dir=none]
	140597854178736 [label="bias
 (512)" fillcolor=orange]
	140597366670384 -> 140597480537120 [dir=none]
	140597480537120 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366670384 -> 140597367187472 [dir=none]
	140597367187472 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366670384 -> 140597367187392 [dir=none]
	140597367187392 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366670384 -> 140597854178656 [dir=none]
	140597854178656 [label="weight
 (512)" fillcolor=orange]
	140597366670384 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480028960 -> 140597366670384
	140597366670576 -> 140597366670384
	140597854178656 [label="ct.encoder.layers.11.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597854178656 -> 140597366670576
	140597366670576 [label=AccumulateGrad]
	140597366670528 -> 140597366670384
	140597854178736 [label="ct.encoder.layers.11.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597854178736 -> 140597366670528
	140597366670528 [label=AccumulateGrad]
	140597366669472 -> 140597366670192
	140597366669472 [label=TBackward0]
	140597366670624 -> 140597366669472
	140597854178816 [label="ct.encoder.layers.11.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597854178816 -> 140597366670624
	140597366670624 [label=AccumulateGrad]
	140597367156992 -> 140597367159968
	140597367156992 [label=TBackward0]
	140597366670048 -> 140597367156992
	140597854178976 [label="ct.encoder.layers.11.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597854178976 -> 140597366670048
	140597366670048 [label=AccumulateGrad]
	140597480028816 -> 140597480029104
	140597854179136 [label="ct.encoder.layers.11.norm_out.weight
 (512)" fillcolor=lightblue]
	140597854179136 -> 140597480028816
	140597480028816 [label=AccumulateGrad]
	140597480029200 -> 140597480029104
	140597854179216 [label="ct.encoder.layers.11.norm_out.bias
 (512)" fillcolor=lightblue]
	140597854179216 -> 140597480029200
	140597480029200 [label=AccumulateGrad]
	140597480029248 -> 140597480028240
	140597480029248 -> 140597367187632 [dir=none]
	140597367187632 [label="other
 ()" fillcolor=orange]
	140597480029248 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480029056 -> 140597480029248
	140597480029056 -> 140597367187552 [dir=none]
	140597367187552 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480029056 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367157904 -> 140597480029056
	140597367157904 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597480028768 -> 140597367157904
	140597480028768 -> 140597367187792 [dir=none]
	140597367187792 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597480028768 -> 140597367186752 [dir=none]
	140597367186752 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597480028768 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366670144 -> 140597480028768
	140597566636496 [label="ct.encoder.layers.12.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597566636496 -> 140597366670144
	140597366670144 [label=AccumulateGrad]
	140597366670336 -> 140597480028768
	140597366670336 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366670480 -> 140597366670336
	140597366670480 -> 140597367187952 [dir=none]
	140597367187952 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366670480 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366670768 -> 140597366670480
	140597366670768 -> 140597480537840 [dir=none]
	140597480537840 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366670768 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366670864 -> 140597366670768
	140597366670864 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366670960 -> 140597366670864
	140597366670960 -> 140597367188032 [dir=none]
	140597367188032 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366670960 -> 140597367187872 [dir=none]
	140597367187872 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366670960 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366671056 -> 140597366670960
	140597566636336 [label="ct.encoder.layers.12.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597566636336 -> 140597366671056
	140597366671056 [label=AccumulateGrad]
	140597366671008 -> 140597366670960
	140597366671008 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366671152 -> 140597366671008
	140597366671152 -> 140597566636176 [dir=none]
	140597566636176 [label="bias
 (512)" fillcolor=orange]
	140597366671152 -> 140597480537760 [dir=none]
	140597480537760 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366671152 -> 140597367188192 [dir=none]
	140597367188192 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366671152 -> 140597367188112 [dir=none]
	140597367188112 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366671152 -> 140597566636096 [dir=none]
	140597566636096 [label="weight
 (512)" fillcolor=orange]
	140597366671152 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480029104 -> 140597366671152
	140597366671344 -> 140597366671152
	140597566636096 [label="ct.encoder.layers.12.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597566636096 -> 140597366671344
	140597366671344 [label=AccumulateGrad]
	140597366671296 -> 140597366671152
	140597566636176 [label="ct.encoder.layers.12.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597566636176 -> 140597366671296
	140597366671296 [label=AccumulateGrad]
	140597366670672 -> 140597366670960
	140597366670672 [label=TBackward0]
	140597366671392 -> 140597366670672
	140597566636256 [label="ct.encoder.layers.12.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597566636256 -> 140597366671392
	140597366671392 [label=AccumulateGrad]
	140597366669424 -> 140597480028768
	140597366669424 [label=TBackward0]
	140597366670816 -> 140597366669424
	140597566636416 [label="ct.encoder.layers.12.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597566636416 -> 140597366670816
	140597366670816 [label=AccumulateGrad]
	140597480028288 -> 140597480028192
	140597480028288 -> 140597367188352 [dir=none]
	140597367188352 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480028288 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597367110720 -> 140597480028288
	140597367110720 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597480028336 -> 140597367110720
	140597480028336 -> 140597367188432 [dir=none]
	140597367188432 [label="mat1
 (126, 512)" fillcolor=orange]
	140597480028336 -> 140597367187712 [dir=none]
	140597367187712 [label="mat2
 (512, 512)" fillcolor=orange]
	140597480028336 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366671104 -> 140597480028336
	140597566638336 [label="ct.encoder.layers.12.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597566638336 -> 140597366671104
	140597366671104 [label=AccumulateGrad]
	140597366670432 -> 140597480028336
	140597366670432 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366670720 -> 140597366670432
	140597366670720 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597366671200 -> 140597366670720
	140597366671200 [label=CloneBackward0]
	140597366671584 -> 140597366671200
	140597366671584 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366671680 -> 140597366671584
	140597366671680 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597366671776 -> 140597366671680
	140597366671776 -> 140597367188592 [dir=none]
	140597367188592 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597366671776 -> 140597367188512 [dir=none]
	140597367188512 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597366671776 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366671872 -> 140597366671776
	140597366671872 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366672016 -> 140597366671872
	140597366672016 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366672112 -> 140597366672016
	140597366672112 -> 140597367188272 [dir=none]
	140597367188272 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597366672112 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366672208 -> 140597366672112
	140597366672208 -> 140597480539376 [dir=none]
	140597480539376 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366672208 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366672304 -> 140597366672208
	140597366672304 -> 140597367188912 [dir=none]
	140597367188912 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597366672304 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597366672400 -> 140597366672304
	140597366672400 -> 140597480539376 [dir=none]
	140597480539376 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366672400 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366672496 -> 140597366672400
	140597366672496 -> 140597367188832 [dir=none]
	140597367188832 [label="other
 ()" fillcolor=orange]
	140597366672496 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597366672592 -> 140597366672496
	140597366672592 [label="AddBackward0
------------
alpha: 1"]
	140597366672688 -> 140597366672592
	140597366672688 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597366672832 -> 140597366672688
	140597366672832 -> 140597367188752 [dir=none]
	140597367188752 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597366672832 -> 140597367188672 [dir=none]
	140597367188672 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366672832 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366672928 -> 140597366672832
	140597366672928 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366673072 -> 140597366672928
	140597366673072 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366673168 -> 140597366673072
	140597366673168 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366673264 -> 140597366673168
	140597366673264 [label="AddBackward0
------------
alpha: 1"]
	140597366673360 -> 140597366673264
	140597366673360 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366718624 -> 140597366673360
	140597366718624 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366718720 -> 140597366718624
	140597366718720 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366718816 -> 140597366718720
	140597366718816 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366718912 -> 140597366718816
	140597366718912 -> 140597367186272 [dir=none]
	140597367186272 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366718912 -> 140597367189152 [dir=none]
	140597367189152 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366718912 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366719008 -> 140597366718912
	140597566637856 [label="ct.encoder.layers.12.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597566637856 -> 140597366719008
	140597366719008 [label=AccumulateGrad]
	140597366718960 -> 140597366718912
	140597366718960 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366719104 -> 140597366718960
	140597366719104 -> 140597566637696 [dir=none]
	140597566637696 [label="bias
 (512)" fillcolor=orange]
	140597366719104 -> 140597480538176 [dir=none]
	140597480538176 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366719104 -> 140597367189232 [dir=none]
	140597367189232 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366719104 -> 140597367187152 [dir=none]
	140597367187152 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366719104 -> 140597566637616 [dir=none]
	140597566637616 [label="weight
 (512)" fillcolor=orange]
	140597366719104 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480028240 -> 140597366719104
	140597366719296 -> 140597366719104
	140597566637616 [label="ct.encoder.layers.12.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597566637616 -> 140597366719296
	140597366719296 [label=AccumulateGrad]
	140597366719248 -> 140597366719104
	140597566637696 [label="ct.encoder.layers.12.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597566637696 -> 140597366719248
	140597366719248 [label=AccumulateGrad]
	140597366718528 -> 140597366718912
	140597366718528 [label=TBackward0]
	140597366719344 -> 140597366718528
	140597566637776 [label="ct.encoder.layers.12.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597566637776 -> 140597366719344
	140597366719344 [label=AccumulateGrad]
	140597366673312 -> 140597366673264
	140597566638496 [label="ct.encoder.layers.12.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597566638496 -> 140597366673312
	140597366673312 [label=AccumulateGrad]
	140597366672880 -> 140597366672832
	140597366672880 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597366673216 -> 140597366672880
	140597366673216 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597366672976 -> 140597366673216
	140597366672976 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366718864 -> 140597366672976
	140597366718864 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366719056 -> 140597366718864
	140597366719056 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366719392 -> 140597366719056
	140597366719392 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366719488 -> 140597366719392
	140597366719488 -> 140597367189312 [dir=none]
	140597367189312 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366719488 -> 140597367189392 [dir=none]
	140597367189392 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366719488 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366719584 -> 140597366719488
	140597566638016 [label="ct.encoder.layers.12.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597566638016 -> 140597366719584
	140597366719584 [label=AccumulateGrad]
	140597366719536 -> 140597366719488
	140597366719536 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366719104 -> 140597366719536
	140597366718768 -> 140597366719488
	140597366718768 [label=TBackward0]
	140597366719776 -> 140597366718768
	140597566637936 [label="ct.encoder.layers.12.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597566637936 -> 140597366719776
	140597366719776 [label=AccumulateGrad]
	140597366672640 -> 140597366672592
	140597366672640 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597366673120 -> 140597366672640
	140597366673120 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366673024 -> 140597366673120
	140597366673024 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366719440 -> 140597366673024
	140597366719440 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366719152 -> 140597366719440
	140597366719152 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597366719728 -> 140597366719152
	140597366719728 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597366719824 -> 140597366719728
	140597366719824 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597366719920 -> 140597366719824
	140597366719920 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597366720016 -> 140597366719920
	140597366720016 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597366720112 -> 140597366720016
	140597366720112 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597366720208 -> 140597366720112
	140597366720208 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597366720304 -> 140597366720208
	140597366720304 -> 140597367189072 [dir=none]
	140597367189072 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597366720304 -> 140597367188992 [dir=none]
	140597367188992 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366720304 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366720400 -> 140597366720304
	140597366720400 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366720544 -> 140597366720400
	140597366720544 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366720640 -> 140597366720544
	140597366720640 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366720736 -> 140597366720640
	140597366720736 [label="AddBackward0
------------
alpha: 1"]
	140597366673360 -> 140597366720736
	140597366720832 -> 140597366720736
	140597566638576 [label="ct.encoder.layers.12.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597566638576 -> 140597366720832
	140597366720832 [label=AccumulateGrad]
	140597366720352 -> 140597366720304
	140597366720352 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366720688 -> 140597366720352
	140597366720688 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366720928 -> 140597366720688
	140597366720928 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366720976 -> 140597366720928
	140597366720976 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366721024 -> 140597366720976
	140597366721024 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597366721168 -> 140597366721024
	140597366721168 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597366721264 -> 140597366721168
	140597366721264 -> 140597366735152 [dir=none]
	140597366735152 [label="self
 (251, 512)" fillcolor=orange]
	140597366721264 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597366721360 -> 140597366721264
	140597366721360 [label=TBackward0]
	140597366721456 -> 140597366721360
	140597566638416 [label="ct.encoder.layers.12.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597566638416 -> 140597366721456
	140597366721456 [label=AccumulateGrad]
	140597366671824 -> 140597366671776
	140597366671824 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366672160 -> 140597366671824
	140597366672160 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366672352 -> 140597366672160
	140597366672352 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366672544 -> 140597366672352
	140597366672544 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366672736 -> 140597366672544
	140597366672736 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366672784 -> 140597366672736
	140597366672784 -> 140597366734992 [dir=none]
	140597366734992 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366672784 -> 140597366735392 [dir=none]
	140597366735392 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366672784 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366719632 -> 140597366672784
	140597566638176 [label="ct.encoder.layers.12.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597566638176 -> 140597366719632
	140597366719632 [label=AccumulateGrad]
	140597366719200 -> 140597366672784
	140597366719200 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366719104 -> 140597366719200
	140597366718672 -> 140597366672784
	140597366718672 [label=TBackward0]
	140597366720064 -> 140597366718672
	140597566638096 [label="ct.encoder.layers.12.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597566638096 -> 140597366720064
	140597366720064 [label=AccumulateGrad]
	140597366669952 -> 140597480028336
	140597366669952 [label=TBackward0]
	140597366671536 -> 140597366669952
	140597566638256 [label="ct.encoder.layers.12.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597566638256 -> 140597366671536
	140597366671536 [label=AccumulateGrad]
	140597480028432 -> 140597480029488
	140597480028432 -> 140597366735472 [dir=none]
	140597366735472 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480028432 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597480028864 -> 140597480028432
	140597480028864 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366671440 -> 140597480028864
	140597366671440 -> 140597480538416 [dir=none]
	140597480538416 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366671440 -> 140597566637456 [dir=none]
	140597566637456 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597366671440 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366671632 -> 140597366671440
	140597366671632 -> 140597480538656 [dir=none]
	140597480538656 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597366671632 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366672256 -> 140597366671632
	140597366672256 -> 140597480539056 [dir=none]
	140597480539056 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366672256 -> 140597366735232 [dir=none]
	140597366735232 [label="result1
 (512)" fillcolor=orange]
	140597366672256 -> 140597366735552 [dir=none]
	140597366735552 [label="result2
 (512)" fillcolor=orange]
	140597366672256 -> 140597366735072 [dir=none]
	140597366735072 [label="result3
 (0)" fillcolor=orange]
	140597366672256 -> 140597855179136 [dir=none]
	140597855179136 [label="running_mean
 (512)" fillcolor=orange]
	140597366672256 -> 140597854177376 [dir=none]
	140597854177376 [label="running_var
 (512)" fillcolor=orange]
	140597366672256 -> 140597566637056 [dir=none]
	140597566637056 [label="weight
 (512)" fillcolor=orange]
	140597366672256 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597366671920 -> 140597366672256
	140597366671920 -> 140597480538736 [dir=none]
	140597480538736 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366671920 -> 140597566636896 [dir=none]
	140597566636896 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597366671920 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366720256 -> 140597366671920
	140597366720256 -> 140597480538896 [dir=none]
	140597480538896 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597366720256 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366720592 -> 140597366720256
	140597366720592 -> 140597480538976 [dir=none]
	140597480538976 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597366720592 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597366720448 -> 140597366720592
	140597366720448 -> 140597480539616 [dir=none]
	140597480539616 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366720448 -> 140597566636736 [dir=none]
	140597566636736 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597366720448 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366721120 -> 140597366720448
	140597366721120 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366721408 -> 140597366721120
	140597366721408 -> 140597566636656 [dir=none]
	140597566636656 [label="bias
 (512)" fillcolor=orange]
	140597366721408 -> 140597480539136 [dir=none]
	140597480539136 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366721408 -> 140597366735952 [dir=none]
	140597366735952 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366721408 -> 140597366735632 [dir=none]
	140597366735632 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366721408 -> 140597566636576 [dir=none]
	140597566636576 [label="weight
 (512)" fillcolor=orange]
	140597366721408 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480028192 -> 140597366721408
	140597366721504 -> 140597366721408
	140597566636576 [label="ct.encoder.layers.12.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597566636576 -> 140597366721504
	140597366721504 [label=AccumulateGrad]
	140597366721552 -> 140597366721408
	140597566636656 [label="ct.encoder.layers.12.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597566636656 -> 140597366721552
	140597366721552 [label=AccumulateGrad]
	140597366720880 -> 140597366720448
	140597566636736 [label="ct.encoder.layers.12.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597566636736 -> 140597366720880
	140597366720880 [label=AccumulateGrad]
	140597366719680 -> 140597366720448
	140597566636816 [label="ct.encoder.layers.12.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597566636816 -> 140597366719680
	140597366719680 [label=AccumulateGrad]
	140597366719968 -> 140597366671920
	140597566636896 [label="ct.encoder.layers.12.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597566636896 -> 140597366719968
	140597366719968 [label=AccumulateGrad]
	140597366719872 -> 140597366671920
	140597566636976 [label="ct.encoder.layers.12.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597566636976 -> 140597366719872
	140597366719872 [label=AccumulateGrad]
	140597366672448 -> 140597366672256
	140597566637056 [label="ct.encoder.layers.12.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597566637056 -> 140597366672448
	140597366672448 [label=AccumulateGrad]
	140597366671488 -> 140597366672256
	140597566637136 [label="ct.encoder.layers.12.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597566637136 -> 140597366671488
	140597366671488 [label=AccumulateGrad]
	140597366671728 -> 140597366671440
	140597566637456 [label="ct.encoder.layers.12.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597566637456 -> 140597366671728
	140597366671728 [label=AccumulateGrad]
	140597366669568 -> 140597366671440
	140597566637536 [label="ct.encoder.layers.12.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597566637536 -> 140597366669568
	140597366669568 [label=AccumulateGrad]
	140597480029152 -> 140597480026560
	140597480029152 -> 140597366736112 [dir=none]
	140597366736112 [label="other
 ()" fillcolor=orange]
	140597480029152 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480029008 -> 140597480029152
	140597480029008 -> 140597366736032 [dir=none]
	140597366736032 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480029008 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366672064 -> 140597480029008
	140597366672064 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366671968 -> 140597366672064
	140597366671968 -> 140597366736272 [dir=none]
	140597366736272 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597366671968 -> 140597366735312 [dir=none]
	140597366735312 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597366671968 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366720784 -> 140597366671968
	140597566639056 [label="ct.encoder.layers.12.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597566639056 -> 140597366720784
	140597366720784 [label=AccumulateGrad]
	140597366718576 -> 140597366671968
	140597366718576 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366721312 -> 140597366718576
	140597366721312 -> 140597366735872 [dir=none]
	140597366735872 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366721312 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366721648 -> 140597366721312
	140597366721648 -> 140597480538496 [dir=none]
	140597480538496 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366721648 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366721744 -> 140597366721648
	140597366721744 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366721840 -> 140597366721744
	140597366721840 -> 140597366735712 [dir=none]
	140597366735712 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366721840 -> 140597366736352 [dir=none]
	140597366736352 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366721840 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366721936 -> 140597366721840
	140597566638896 [label="ct.encoder.layers.12.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597566638896 -> 140597366721936
	140597366721936 [label=AccumulateGrad]
	140597366721888 -> 140597366721840
	140597366721888 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366722032 -> 140597366721888
	140597366722032 -> 140597566638736 [dir=none]
	140597566638736 [label="bias
 (512)" fillcolor=orange]
	140597366722032 -> 140597480539296 [dir=none]
	140597480539296 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366722032 -> 140597366736512 [dir=none]
	140597366736512 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366722032 -> 140597366736432 [dir=none]
	140597366736432 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366722032 -> 140597566638656 [dir=none]
	140597566638656 [label="weight
 (512)" fillcolor=orange]
	140597366722032 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480029488 -> 140597366722032
	140597366722224 -> 140597366722032
	140597566638656 [label="ct.encoder.layers.12.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597566638656 -> 140597366722224
	140597366722224 [label=AccumulateGrad]
	140597366722176 -> 140597366722032
	140597566638736 [label="ct.encoder.layers.12.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597566638736 -> 140597366722176
	140597366722176 [label=AccumulateGrad]
	140597366720496 -> 140597366721840
	140597366720496 [label=TBackward0]
	140597366722272 -> 140597366720496
	140597566638816 [label="ct.encoder.layers.12.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597566638816 -> 140597366722272
	140597366722272 [label=AccumulateGrad]
	140597366720160 -> 140597366671968
	140597366720160 [label=TBackward0]
	140597366721696 -> 140597366720160
	140597566638976 [label="ct.encoder.layers.12.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597566638976 -> 140597366721696
	140597366721696 [label=AccumulateGrad]
	140597480029296 -> 140597480028528
	140597566639136 [label="ct.encoder.layers.12.norm_out.weight
 (512)" fillcolor=lightblue]
	140597566639136 -> 140597480029296
	140597480029296 [label=AccumulateGrad]
	140597480029440 -> 140597480028528
	140597566639216 [label="ct.encoder.layers.12.norm_out.bias
 (512)" fillcolor=lightblue]
	140597566639216 -> 140597480029440
	140597480029440 [label=AccumulateGrad]
	140597480026512 -> 140597480026320
	140597480026512 -> 140597366736672 [dir=none]
	140597366736672 [label="other
 ()" fillcolor=orange]
	140597480026512 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480028480 -> 140597480026512
	140597480028480 -> 140597366736592 [dir=none]
	140597366736592 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480028480 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366670912 -> 140597480028480
	140597366670912 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597480028576 -> 140597366670912
	140597480028576 -> 140597366736832 [dir=none]
	140597366736832 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597480028576 -> 140597366735792 [dir=none]
	140597366735792 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597480028576 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366721792 -> 140597480028576
	140597566639696 [label="ct.encoder.layers.13.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597566639696 -> 140597366721792
	140597366721792 [label=AccumulateGrad]
	140597366721984 -> 140597480028576
	140597366721984 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366722128 -> 140597366721984
	140597366722128 -> 140597366736992 [dir=none]
	140597366736992 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366722128 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366722416 -> 140597366722128
	140597366722416 -> 140597480540016 [dir=none]
	140597480540016 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366722416 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366722512 -> 140597366722416
	140597366722512 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366722320 -> 140597366722512
	140597366722320 -> 140597366737072 [dir=none]
	140597366737072 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366722320 -> 140597366736912 [dir=none]
	140597366736912 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366722320 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366784208 -> 140597366722320
	140597566639536 [label="ct.encoder.layers.13.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597566639536 -> 140597366784208
	140597366784208 [label=AccumulateGrad]
	140597366784160 -> 140597366722320
	140597366784160 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366784304 -> 140597366784160
	140597366784304 -> 140597566639376 [dir=none]
	140597566639376 [label="bias
 (512)" fillcolor=orange]
	140597366784304 -> 140597480539936 [dir=none]
	140597480539936 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366784304 -> 140597366737232 [dir=none]
	140597366737232 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366784304 -> 140597366737152 [dir=none]
	140597366737152 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366784304 -> 140597566639296 [dir=none]
	140597566639296 [label="weight
 (512)" fillcolor=orange]
	140597366784304 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480028528 -> 140597366784304
	140597366784496 -> 140597366784304
	140597566639296 [label="ct.encoder.layers.13.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597566639296 -> 140597366784496
	140597366784496 [label=AccumulateGrad]
	140597366784448 -> 140597366784304
	140597566639376 [label="ct.encoder.layers.13.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597566639376 -> 140597366784448
	140597366784448 [label=AccumulateGrad]
	140597366784064 -> 140597366722320
	140597366784064 [label=TBackward0]
	140597366784544 -> 140597366784064
	140597566639456 [label="ct.encoder.layers.13.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597566639456 -> 140597366784544
	140597366784544 [label=AccumulateGrad]
	140597366721072 -> 140597480028576
	140597366721072 [label=TBackward0]
	140597366722464 -> 140597366721072
	140597566639616 [label="ct.encoder.layers.13.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597566639616 -> 140597366722464
	140597366722464 [label=AccumulateGrad]
	140597480026272 -> 140597480026368
	140597480026272 -> 140597366737392 [dir=none]
	140597366737392 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480026272 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366671248 -> 140597480026272
	140597366671248 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597480026224 -> 140597366671248
	140597480026224 -> 140597366737472 [dir=none]
	140597366737472 [label="mat1
 (126, 512)" fillcolor=orange]
	140597480026224 -> 140597366736752 [dir=none]
	140597366736752 [label="mat2
 (512, 512)" fillcolor=orange]
	140597480026224 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366722368 -> 140597480026224
	140597566764512 [label="ct.encoder.layers.13.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597566764512 -> 140597366722368
	140597366722368 [label=AccumulateGrad]
	140597366722080 -> 140597480026224
	140597366722080 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366784112 -> 140597366722080
	140597366784112 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597366784352 -> 140597366784112
	140597366784352 [label=CloneBackward0]
	140597366784736 -> 140597366784352
	140597366784736 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366784832 -> 140597366784736
	140597366784832 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597366784928 -> 140597366784832
	140597366784928 -> 140597366737632 [dir=none]
	140597366737632 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597366784928 -> 140597366737552 [dir=none]
	140597366737552 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597366784928 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366785024 -> 140597366784928
	140597366785024 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366785168 -> 140597366785024
	140597366785168 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366785264 -> 140597366785168
	140597366785264 -> 140597366737312 [dir=none]
	140597366737312 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597366785264 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366785360 -> 140597366785264
	140597366785360 -> 140597480541456 [dir=none]
	140597480541456 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366785360 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366785456 -> 140597366785360
	140597366785456 -> 140597366737952 [dir=none]
	140597366737952 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597366785456 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597366785552 -> 140597366785456
	140597366785552 -> 140597480541456 [dir=none]
	140597480541456 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366785552 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366785648 -> 140597366785552
	140597366785648 -> 140597366737872 [dir=none]
	140597366737872 [label="other
 ()" fillcolor=orange]
	140597366785648 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597366785744 -> 140597366785648
	140597366785744 [label="AddBackward0
------------
alpha: 1"]
	140597366785840 -> 140597366785744
	140597366785840 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597366785984 -> 140597366785840
	140597366785984 -> 140597366737792 [dir=none]
	140597366737792 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597366785984 -> 140597366737712 [dir=none]
	140597366737712 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366785984 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366786080 -> 140597366785984
	140597366786080 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366786224 -> 140597366786080
	140597366786224 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366786320 -> 140597366786224
	140597366786320 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366786416 -> 140597366786320
	140597366786416 [label="AddBackward0
------------
alpha: 1"]
	140597366786512 -> 140597366786416
	140597366786512 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366786656 -> 140597366786512
	140597366786656 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366786752 -> 140597366786656
	140597366786752 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366786848 -> 140597366786752
	140597366786848 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366786944 -> 140597366786848
	140597366786944 -> 140597366734912 [dir=none]
	140597366734912 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366786944 -> 140597366738192 [dir=none]
	140597366738192 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366786944 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366787040 -> 140597366786944
	140597566764032 [label="ct.encoder.layers.13.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597566764032 -> 140597366787040
	140597366787040 [label=AccumulateGrad]
	140597366786992 -> 140597366786944
	140597366786992 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366787136 -> 140597366786992
	140597366787136 -> 140597566763872 [dir=none]
	140597566763872 [label="bias
 (512)" fillcolor=orange]
	140597366787136 -> 140597480540256 [dir=none]
	140597480540256 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366787136 -> 140597366738272 [dir=none]
	140597366738272 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366787136 -> 140597366736192 [dir=none]
	140597366736192 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366787136 -> 140597566763792 [dir=none]
	140597566763792 [label="weight
 (512)" fillcolor=orange]
	140597366787136 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480026320 -> 140597366787136
	140597366787328 -> 140597366787136
	140597566763792 [label="ct.encoder.layers.13.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597566763792 -> 140597366787328
	140597366787328 [label=AccumulateGrad]
	140597366787280 -> 140597366787136
	140597566763872 [label="ct.encoder.layers.13.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597566763872 -> 140597366787280
	140597366787280 [label=AccumulateGrad]
	140597366786560 -> 140597366786944
	140597366786560 [label=TBackward0]
	140597366787376 -> 140597366786560
	140597566763952 [label="ct.encoder.layers.13.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597566763952 -> 140597366787376
	140597366787376 [label=AccumulateGrad]
	140597366786464 -> 140597366786416
	140597566764672 [label="ct.encoder.layers.13.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597566764672 -> 140597366786464
	140597366786464 [label=AccumulateGrad]
	140597366786032 -> 140597366785984
	140597366786032 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597366786368 -> 140597366786032
	140597366786368 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597366786800 -> 140597366786368
	140597366786800 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366786896 -> 140597366786800
	140597366786896 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366787088 -> 140597366786896
	140597366787088 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366787424 -> 140597366787088
	140597366787424 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366787520 -> 140597366787424
	140597366787520 -> 140597366738432 [dir=none]
	140597366738432 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366787520 -> 140597366738032 [dir=none]
	140597366738032 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366787520 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366787616 -> 140597366787520
	140597566764192 [label="ct.encoder.layers.13.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597566764192 -> 140597366787616
	140597366787616 [label=AccumulateGrad]
	140597366787568 -> 140597366787520
	140597366787568 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366787136 -> 140597366787568
	140597366786176 -> 140597366787520
	140597366786176 [label=TBackward0]
	140597366787808 -> 140597366786176
	140597566764112 [label="ct.encoder.layers.13.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597566764112 -> 140597366787808
	140597366787808 [label=AccumulateGrad]
	140597366785792 -> 140597366785744
	140597366785792 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597366786272 -> 140597366785792
	140597366786272 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366786608 -> 140597366786272
	140597366786608 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366787472 -> 140597366786608
	140597366787472 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366787184 -> 140597366787472
	140597366787184 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597366787760 -> 140597366787184
	140597366787760 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597366787856 -> 140597366787760
	140597366787856 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597366787952 -> 140597366787856
	140597366787952 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597366788048 -> 140597366787952
	140597366788048 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597366785936 -> 140597366788048
	140597366785936 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597366816976 -> 140597366785936
	140597366816976 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597366817072 -> 140597366816976
	140597366817072 -> 140597366738352 [dir=none]
	140597366738352 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597366817072 -> 140597366738672 [dir=none]
	140597366738672 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366817072 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366817168 -> 140597366817072
	140597366817168 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366817312 -> 140597366817168
	140597366817312 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366817408 -> 140597366817312
	140597366817408 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366817504 -> 140597366817408
	140597366817504 [label="AddBackward0
------------
alpha: 1"]
	140597366786512 -> 140597366817504
	140597366817600 -> 140597366817504
	140597566764752 [label="ct.encoder.layers.13.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597566764752 -> 140597366817600
	140597366817600 [label=AccumulateGrad]
	140597366817120 -> 140597366817072
	140597366817120 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366817456 -> 140597366817120
	140597366817456 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366817696 -> 140597366817456
	140597366817696 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366817744 -> 140597366817696
	140597366817744 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366817792 -> 140597366817744
	140597366817792 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597366817936 -> 140597366817792
	140597366817936 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597366818032 -> 140597366817936
	140597366818032 -> 140597366738512 [dir=none]
	140597366738512 [label="self
 (251, 512)" fillcolor=orange]
	140597366818032 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597366818128 -> 140597366818032
	140597366818128 [label=TBackward0]
	140597366818224 -> 140597366818128
	140597566764592 [label="ct.encoder.layers.13.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597566764592 -> 140597366818224
	140597366818224 [label=AccumulateGrad]
	140597366784976 -> 140597366784928
	140597366784976 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366785312 -> 140597366784976
	140597366785312 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366785504 -> 140597366785312
	140597366785504 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366785696 -> 140597366785504
	140597366785696 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366785888 -> 140597366785696
	140597366785888 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366786704 -> 140597366785888
	140597366786704 -> 140597366738112 [dir=none]
	140597366738112 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366786704 -> 140597366738752 [dir=none]
	140597366738752 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366786704 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366787664 -> 140597366786704
	140597566764352 [label="ct.encoder.layers.13.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597566764352 -> 140597366787664
	140597366787664 [label=AccumulateGrad]
	140597366787232 -> 140597366786704
	140597366787232 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366787136 -> 140597366787232
	140597366785120 -> 140597366786704
	140597366785120 [label=TBackward0]
	140597366788000 -> 140597366785120
	140597566764272 [label="ct.encoder.layers.13.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597566764272 -> 140597366788000
	140597366788000 [label=AccumulateGrad]
	140597366721600 -> 140597480026224
	140597366721600 [label=TBackward0]
	140597366784688 -> 140597366721600
	140597566764432 [label="ct.encoder.layers.13.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597566764432 -> 140597366784688
	140597366784688 [label=AccumulateGrad]
	140597480026464 -> 140597480026608
	140597480026464 -> 140597366738832 [dir=none]
	140597366738832 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480026464 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366721216 -> 140597480026464
	140597366721216 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597480029392 -> 140597366721216
	140597480029392 -> 140597480540496 [dir=none]
	140597480540496 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597480029392 -> 140597566763632 [dir=none]
	140597566763632 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597480029392 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366784784 -> 140597480029392
	140597366784784 -> 140597480540736 [dir=none]
	140597480540736 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597366784784 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366785408 -> 140597366784784
	140597366785408 -> 140597480541136 [dir=none]
	140597480541136 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366785408 -> 140597366738592 [dir=none]
	140597366738592 [label="result1
 (512)" fillcolor=orange]
	140597366785408 -> 140597366833216 [dir=none]
	140597366833216 [label="result2
 (512)" fillcolor=orange]
	140597366785408 -> 140597366833376 [dir=none]
	140597366833376 [label="result3
 (0)" fillcolor=orange]
	140597366785408 -> 140597854971040 [dir=none]
	140597854971040 [label="running_mean
 (512)" fillcolor=orange]
	140597366785408 -> 140597566637376 [dir=none]
	140597566637376 [label="running_var
 (512)" fillcolor=orange]
	140597366785408 -> 140597566763232 [dir=none]
	140597566763232 [label="weight
 (512)" fillcolor=orange]
	140597366785408 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597366785072 -> 140597366785408
	140597366785072 -> 140597480540816 [dir=none]
	140597480540816 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366785072 -> 140597566763072 [dir=none]
	140597566763072 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597366785072 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366787712 -> 140597366785072
	140597366787712 -> 140597480540976 [dir=none]
	140597480540976 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597366787712 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366817360 -> 140597366787712
	140597366817360 -> 140597480541056 [dir=none]
	140597480541056 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597366817360 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597366817216 -> 140597366817360
	140597366817216 -> 140597480541696 [dir=none]
	140597480541696 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366817216 -> 140597566639936 [dir=none]
	140597566639936 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597366817216 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366817888 -> 140597366817216
	140597366817888 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366818176 -> 140597366817888
	140597366818176 -> 140597566639856 [dir=none]
	140597566639856 [label="bias
 (512)" fillcolor=orange]
	140597366818176 -> 140597480541216 [dir=none]
	140597480541216 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366818176 -> 140597366833776 [dir=none]
	140597366833776 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366818176 -> 140597366833296 [dir=none]
	140597366833296 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366818176 -> 140597566639776 [dir=none]
	140597566639776 [label="weight
 (512)" fillcolor=orange]
	140597366818176 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480026368 -> 140597366818176
	140597366818272 -> 140597366818176
	140597566639776 [label="ct.encoder.layers.13.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597566639776 -> 140597366818272
	140597366818272 [label=AccumulateGrad]
	140597366818320 -> 140597366818176
	140597566639856 [label="ct.encoder.layers.13.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597566639856 -> 140597366818320
	140597366818320 [label=AccumulateGrad]
	140597366817648 -> 140597366817216
	140597566639936 [label="ct.encoder.layers.13.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597566639936 -> 140597366817648
	140597366817648 [label=AccumulateGrad]
	140597366816880 -> 140597366817216
	140597566640016 [label="ct.encoder.layers.13.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597566640016 -> 140597366816880
	140597366816880 [label=AccumulateGrad]
	140597366787904 -> 140597366785072
	140597566763072 [label="ct.encoder.layers.13.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597566763072 -> 140597366787904
	140597366787904 [label=AccumulateGrad]
	140597366817024 -> 140597366785072
	140597566763152 [label="ct.encoder.layers.13.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597566763152 -> 140597366817024
	140597366817024 [label=AccumulateGrad]
	140597366785600 -> 140597366785408
	140597566763232 [label="ct.encoder.layers.13.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597566763232 -> 140597366785600
	140597366785600 [label=AccumulateGrad]
	140597366784640 -> 140597366785408
	140597566763312 [label="ct.encoder.layers.13.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597566763312 -> 140597366784640
	140597366784640 [label=AccumulateGrad]
	140597366784880 -> 140597480029392
	140597566763632 [label="ct.encoder.layers.13.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597566763632 -> 140597366784880
	140597366784880 [label=AccumulateGrad]
	140597366784256 -> 140597480029392
	140597566763712 [label="ct.encoder.layers.13.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597566763712 -> 140597366784256
	140597366784256 [label=AccumulateGrad]
	140597480026176 -> 140597480377648
	140597480026176 -> 140597366833936 [dir=none]
	140597366833936 [label="other
 ()" fillcolor=orange]
	140597480026176 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480028384 -> 140597480026176
	140597480028384 -> 140597366833856 [dir=none]
	140597366833856 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480028384 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366785216 -> 140597480028384
	140597366785216 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366786128 -> 140597366785216
	140597366786128 -> 140597366834176 [dir=none]
	140597366834176 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597366786128 -> 140597366833696 [dir=none]
	140597366833696 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597366786128 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366817552 -> 140597366786128
	140597566765232 [label="ct.encoder.layers.13.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597566765232 -> 140597366817552
	140597366817552 [label=AccumulateGrad]
	140597366816832 -> 140597366786128
	140597366816832 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366818080 -> 140597366816832
	140597366818080 -> 140597366833616 [dir=none]
	140597366833616 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366818080 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366818416 -> 140597366818080
	140597366818416 -> 140597480540576 [dir=none]
	140597480540576 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366818416 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366818512 -> 140597366818416
	140597366818512 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366818608 -> 140597366818512
	140597366818608 -> 140597366833456 [dir=none]
	140597366833456 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366818608 -> 140597366834256 [dir=none]
	140597366834256 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366818608 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366818704 -> 140597366818608
	140597566765072 [label="ct.encoder.layers.13.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597566765072 -> 140597366818704
	140597366818704 [label=AccumulateGrad]
	140597366818656 -> 140597366818608
	140597366818656 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366818800 -> 140597366818656
	140597366818800 -> 140597566764912 [dir=none]
	140597566764912 [label="bias
 (512)" fillcolor=orange]
	140597366818800 -> 140597480541376 [dir=none]
	140597480541376 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366818800 -> 140597366834416 [dir=none]
	140597366834416 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366818800 -> 140597366834336 [dir=none]
	140597366834336 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366818800 -> 140597566764832 [dir=none]
	140597566764832 [label="weight
 (512)" fillcolor=orange]
	140597366818800 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480026608 -> 140597366818800
	140597366818992 -> 140597366818800
	140597566764832 [label="ct.encoder.layers.13.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597566764832 -> 140597366818992
	140597366818992 [label=AccumulateGrad]
	140597366818944 -> 140597366818800
	140597566764912 [label="ct.encoder.layers.13.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597566764912 -> 140597366818944
	140597366818944 [label=AccumulateGrad]
	140597366817264 -> 140597366818608
	140597366817264 [label=TBackward0]
	140597366819040 -> 140597366817264
	140597566764992 [label="ct.encoder.layers.13.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597566764992 -> 140597366819040
	140597366819040 [label=AccumulateGrad]
	140597366816928 -> 140597366786128
	140597366816928 [label=TBackward0]
	140597366818464 -> 140597366816928
	140597566765152 [label="ct.encoder.layers.13.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597566765152 -> 140597366818464
	140597366818464 [label=AccumulateGrad]
	140597480377408 -> 140597480377984
	140597566765312 [label="ct.encoder.layers.13.norm_out.weight
 (512)" fillcolor=lightblue]
	140597566765312 -> 140597480377408
	140597480377408 [label=AccumulateGrad]
	140597480377504 -> 140597480377984
	140597566765392 [label="ct.encoder.layers.13.norm_out.bias
 (512)" fillcolor=lightblue]
	140597566765392 -> 140597480377504
	140597480377504 [label=AccumulateGrad]
	140597480378032 -> 140597480377888
	140597480378032 -> 140597366834576 [dir=none]
	140597366834576 [label="other
 ()" fillcolor=orange]
	140597480378032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480026416 -> 140597480378032
	140597480026416 -> 140597366834496 [dir=none]
	140597366834496 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480026416 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366784592 -> 140597480026416
	140597366784592 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597480124576 -> 140597366784592
	140597480124576 -> 140597366834736 [dir=none]
	140597366834736 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597480124576 -> 140597366833536 [dir=none]
	140597366833536 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597480124576 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366818560 -> 140597480124576
	140597566765872 [label="ct.encoder.layers.14.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597566765872 -> 140597366818560
	140597366818560 [label=AccumulateGrad]
	140597366818752 -> 140597480124576
	140597366818752 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366818896 -> 140597366818752
	140597366818896 -> 140597366834896 [dir=none]
	140597366834896 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366818896 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366819184 -> 140597366818896
	140597366819184 -> 140597480542096 [dir=none]
	140597480542096 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366819184 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366819280 -> 140597366819184
	140597366819280 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366819376 -> 140597366819280
	140597366819376 -> 140597366834976 [dir=none]
	140597366834976 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366819376 -> 140597366834816 [dir=none]
	140597366834816 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366819376 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366819472 -> 140597366819376
	140597566765712 [label="ct.encoder.layers.14.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597566765712 -> 140597366819472
	140597366819472 [label=AccumulateGrad]
	140597366819424 -> 140597366819376
	140597366819424 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366819568 -> 140597366819424
	140597366819568 -> 140597566765552 [dir=none]
	140597566765552 [label="bias
 (512)" fillcolor=orange]
	140597366819568 -> 140597480542016 [dir=none]
	140597480542016 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366819568 -> 140597366835136 [dir=none]
	140597366835136 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366819568 -> 140597366835056 [dir=none]
	140597366835056 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366819568 -> 140597566765472 [dir=none]
	140597566765472 [label="weight
 (512)" fillcolor=orange]
	140597366819568 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480377984 -> 140597366819568
	140597366819760 -> 140597366819568
	140597566765472 [label="ct.encoder.layers.14.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597566765472 -> 140597366819760
	140597366819760 [label=AccumulateGrad]
	140597366819712 -> 140597366819568
	140597566765552 [label="ct.encoder.layers.14.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597566765552 -> 140597366819712
	140597366819712 [label=AccumulateGrad]
	140597366819088 -> 140597366819376
	140597366819088 [label=TBackward0]
	140597366819808 -> 140597366819088
	140597566765632 [label="ct.encoder.layers.14.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597566765632 -> 140597366819808
	140597366819808 [label=AccumulateGrad]
	140597366817840 -> 140597480124576
	140597366817840 [label=TBackward0]
	140597366819232 -> 140597366817840
	140597566765792 [label="ct.encoder.layers.14.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597566765792 -> 140597366819232
	140597366819232 [label=AccumulateGrad]
	140597480377600 -> 140597480377840
	140597480377600 -> 140597366835296 [dir=none]
	140597366835296 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480377600 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366784400 -> 140597480377600
	140597366784400 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597480376160 -> 140597366784400
	140597480376160 -> 140597366835376 [dir=none]
	140597366835376 [label="mat1
 (126, 512)" fillcolor=orange]
	140597480376160 -> 140597366834656 [dir=none]
	140597366834656 [label="mat2
 (512, 512)" fillcolor=orange]
	140597480376160 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366819520 -> 140597480376160
	140597520167616 [label="ct.encoder.layers.14.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597520167616 -> 140597366819520
	140597366819520 [label=AccumulateGrad]
	140597366818848 -> 140597480376160
	140597366818848 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366819136 -> 140597366818848
	140597366819136 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597366819616 -> 140597366819136
	140597366819616 [label=CloneBackward0]
	140597366820000 -> 140597366819616
	140597366820000 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366820096 -> 140597366820000
	140597366820096 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597366820192 -> 140597366820096
	140597366820192 -> 140597366835536 [dir=none]
	140597366835536 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597366820192 -> 140597366835456 [dir=none]
	140597366835456 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597366820192 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366820288 -> 140597366820192
	140597366820288 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366820432 -> 140597366820288
	140597366820432 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366820528 -> 140597366820432
	140597366820528 -> 140597366835216 [dir=none]
	140597366835216 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597366820528 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366820624 -> 140597366820528
	140597366820624 -> 140597480543632 [dir=none]
	140597480543632 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366820624 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366820720 -> 140597366820624
	140597366820720 -> 140597366835856 [dir=none]
	140597366835856 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597366820720 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597366820816 -> 140597366820720
	140597366820816 -> 140597480543632 [dir=none]
	140597480543632 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366820816 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366820336 -> 140597366820816
	140597366820336 -> 140597366835776 [dir=none]
	140597366835776 [label="other
 ()" fillcolor=orange]
	140597366820336 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597366878416 -> 140597366820336
	140597366878416 [label="AddBackward0
------------
alpha: 1"]
	140597366878512 -> 140597366878416
	140597366878512 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597366878656 -> 140597366878512
	140597366878656 -> 140597366835696 [dir=none]
	140597366835696 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597366878656 -> 140597366835616 [dir=none]
	140597366835616 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366878656 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366878752 -> 140597366878656
	140597366878752 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366878896 -> 140597366878752
	140597366878896 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366878992 -> 140597366878896
	140597366878992 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366879088 -> 140597366878992
	140597366879088 [label="AddBackward0
------------
alpha: 1"]
	140597366879184 -> 140597366879088
	140597366879184 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366879328 -> 140597366879184
	140597366879328 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366879424 -> 140597366879328
	140597366879424 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366879520 -> 140597366879424
	140597366879520 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366879616 -> 140597366879520
	140597366879616 -> 140597366834016 [dir=none]
	140597366834016 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366879616 -> 140597366836096 [dir=none]
	140597366836096 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366879616 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366879712 -> 140597366879616
	140597520167136 [label="ct.encoder.layers.14.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597520167136 -> 140597366879712
	140597366879712 [label=AccumulateGrad]
	140597366879664 -> 140597366879616
	140597366879664 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366879808 -> 140597366879664
	140597366879808 -> 140597520166976 [dir=none]
	140597520166976 [label="bias
 (512)" fillcolor=orange]
	140597366879808 -> 140597480542432 [dir=none]
	140597480542432 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366879808 -> 140597366836176 [dir=none]
	140597366836176 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366879808 -> 140597366834096 [dir=none]
	140597366834096 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366879808 -> 140597566766992 [dir=none]
	140597566766992 [label="weight
 (512)" fillcolor=orange]
	140597366879808 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480377888 -> 140597366879808
	140597366880000 -> 140597366879808
	140597566766992 [label="ct.encoder.layers.14.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597566766992 -> 140597366880000
	140597366880000 [label=AccumulateGrad]
	140597366879952 -> 140597366879808
	140597520166976 [label="ct.encoder.layers.14.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597520166976 -> 140597366879952
	140597366879952 [label=AccumulateGrad]
	140597366879232 -> 140597366879616
	140597366879232 [label=TBackward0]
	140597366880048 -> 140597366879232
	140597520167056 [label="ct.encoder.layers.14.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597520167056 -> 140597366880048
	140597366880048 [label=AccumulateGrad]
	140597366879136 -> 140597366879088
	140597520167776 [label="ct.encoder.layers.14.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597520167776 -> 140597366879136
	140597366879136 [label=AccumulateGrad]
	140597366878704 -> 140597366878656
	140597366878704 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597366879040 -> 140597366878704
	140597366879040 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597366879472 -> 140597366879040
	140597366879472 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366879568 -> 140597366879472
	140597366879568 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366879760 -> 140597366879568
	140597366879760 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366880096 -> 140597366879760
	140597366880096 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366880192 -> 140597366880096
	140597366880192 -> 140597366836336 [dir=none]
	140597366836336 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366880192 -> 140597366835936 [dir=none]
	140597366835936 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366880192 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366880288 -> 140597366880192
	140597520167296 [label="ct.encoder.layers.14.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597520167296 -> 140597366880288
	140597366880288 [label=AccumulateGrad]
	140597366880240 -> 140597366880192
	140597366880240 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366879808 -> 140597366880240
	140597366878848 -> 140597366880192
	140597366878848 [label=TBackward0]
	140597366880480 -> 140597366878848
	140597520167216 [label="ct.encoder.layers.14.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597520167216 -> 140597366880480
	140597366880480 [label=AccumulateGrad]
	140597366878464 -> 140597366878416
	140597366878464 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597366878944 -> 140597366878464
	140597366878944 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366879280 -> 140597366878944
	140597366879280 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366880144 -> 140597366879280
	140597366880144 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366879856 -> 140597366880144
	140597366879856 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597366880432 -> 140597366879856
	140597366880432 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597366880528 -> 140597366880432
	140597366880528 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597366880624 -> 140597366880528
	140597366880624 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597366880720 -> 140597366880624
	140597366880720 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597366880816 -> 140597366880720
	140597366880816 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597366880912 -> 140597366880816
	140597366880912 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597366881008 -> 140597366880912
	140597366881008 -> 140597366836896 [dir=none]
	140597366836896 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597366881008 -> 140597366836016 [dir=none]
	140597366836016 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366881008 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366881104 -> 140597366881008
	140597366881104 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366881248 -> 140597366881104
	140597366881248 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366881344 -> 140597366881248
	140597366881344 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366881440 -> 140597366881344
	140597366881440 [label="AddBackward0
------------
alpha: 1"]
	140597366879184 -> 140597366881440
	140597366881536 -> 140597366881440
	140597520167856 [label="ct.encoder.layers.14.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597520167856 -> 140597366881536
	140597366881536 [label=AccumulateGrad]
	140597366881056 -> 140597366881008
	140597366881056 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366881392 -> 140597366881056
	140597366881392 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366881632 -> 140597366881392
	140597366881632 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366881680 -> 140597366881632
	140597366881680 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366881728 -> 140597366881680
	140597366881728 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597366881872 -> 140597366881728
	140597366881872 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597366881968 -> 140597366881872
	140597366881968 -> 140597366836416 [dir=none]
	140597366836416 [label="self
 (251, 512)" fillcolor=orange]
	140597366881968 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597366882064 -> 140597366881968
	140597366882064 [label=TBackward0]
	140597366882160 -> 140597366882064
	140597520167696 [label="ct.encoder.layers.14.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597520167696 -> 140597366882160
	140597366882160 [label=AccumulateGrad]
	140597366820240 -> 140597366820192
	140597366820240 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366820576 -> 140597366820240
	140597366820576 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366820768 -> 140597366820576
	140597366820768 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366820384 -> 140597366820768
	140597366820384 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366878560 -> 140597366820384
	140597366878560 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366879376 -> 140597366878560
	140597366879376 -> 140597366836736 [dir=none]
	140597366836736 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366879376 -> 140597366836656 [dir=none]
	140597366836656 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366879376 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366880336 -> 140597366879376
	140597520167456 [label="ct.encoder.layers.14.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597520167456 -> 140597366880336
	140597366880336 [label=AccumulateGrad]
	140597366879904 -> 140597366879376
	140597366879904 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366879808 -> 140597366879904
	140597366878272 -> 140597366879376
	140597366878272 [label=TBackward0]
	140597366880768 -> 140597366878272
	140597520167376 [label="ct.encoder.layers.14.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597520167376 -> 140597366880768
	140597366880768 [label=AccumulateGrad]
	140597366818368 -> 140597480376160
	140597366818368 [label=TBackward0]
	140597366819952 -> 140597366818368
	140597520167536 [label="ct.encoder.layers.14.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597520167536 -> 140597366819952
	140597366819952 [label=AccumulateGrad]
	140597480375728 -> 140597480376400
	140597480375728 -> 140597366836576 [dir=none]
	140597366836576 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480375728 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597480029584 -> 140597480375728
	140597480029584 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366819856 -> 140597480029584
	140597366819856 -> 140597480542672 [dir=none]
	140597480542672 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366819856 -> 140597566766832 [dir=none]
	140597566766832 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597366819856 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366820048 -> 140597366819856
	140597366820048 -> 140597480542912 [dir=none]
	140597480542912 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597366820048 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366820672 -> 140597366820048
	140597366820672 -> 140597480543312 [dir=none]
	140597480543312 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366820672 -> 140597366836256 [dir=none]
	140597366836256 [label="result1
 (512)" fillcolor=orange]
	140597366820672 -> 140597366837056 [dir=none]
	140597366837056 [label="result2
 (512)" fillcolor=orange]
	140597366820672 -> 140597366836496 [dir=none]
	140597366836496 [label="result3
 (0)" fillcolor=orange]
	140597366820672 -> 140597854055216 [dir=none]
	140597854055216 [label="running_mean
 (512)" fillcolor=orange]
	140597366820672 -> 140597566763552 [dir=none]
	140597566763552 [label="running_var
 (512)" fillcolor=orange]
	140597366820672 -> 140597566766432 [dir=none]
	140597566766432 [label="weight
 (512)" fillcolor=orange]
	140597366820672 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597366819904 -> 140597366820672
	140597366819904 -> 140597480542992 [dir=none]
	140597480542992 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366819904 -> 140597566766272 [dir=none]
	140597566766272 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597366819904 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366880960 -> 140597366819904
	140597366880960 -> 140597480543152 [dir=none]
	140597480543152 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597366880960 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366881296 -> 140597366880960
	140597366881296 -> 140597480543232 [dir=none]
	140597480543232 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597366881296 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597366881152 -> 140597366881296
	140597366881152 -> 140597480543872 [dir=none]
	140597480543872 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366881152 -> 140597566766112 [dir=none]
	140597566766112 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597366881152 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366881824 -> 140597366881152
	140597366881824 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366882112 -> 140597366881824
	140597366882112 -> 140597566766032 [dir=none]
	140597566766032 [label="bias
 (512)" fillcolor=orange]
	140597366882112 -> 140597480543392 [dir=none]
	140597480543392 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366882112 -> 140597366836816 [dir=none]
	140597366836816 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366882112 -> 140597366836976 [dir=none]
	140597366836976 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366882112 -> 140597566765952 [dir=none]
	140597566765952 [label="weight
 (512)" fillcolor=orange]
	140597366882112 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480377840 -> 140597366882112
	140597366882208 -> 140597366882112
	140597566765952 [label="ct.encoder.layers.14.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597566765952 -> 140597366882208
	140597366882208 [label=AccumulateGrad]
	140597366882256 -> 140597366882112
	140597566766032 [label="ct.encoder.layers.14.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597566766032 -> 140597366882256
	140597366882256 [label=AccumulateGrad]
	140597366881584 -> 140597366881152
	140597566766112 [label="ct.encoder.layers.14.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597566766112 -> 140597366881584
	140597366881584 [label=AccumulateGrad]
	140597366880384 -> 140597366881152
	140597566766192 [label="ct.encoder.layers.14.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597566766192 -> 140597366880384
	140597366880384 [label=AccumulateGrad]
	140597366880672 -> 140597366819904
	140597566766272 [label="ct.encoder.layers.14.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597566766272 -> 140597366880672
	140597366880672 [label=AccumulateGrad]
	140597366880576 -> 140597366819904
	140597566766352 [label="ct.encoder.layers.14.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597566766352 -> 140597366880576
	140597366880576 [label=AccumulateGrad]
	140597366878320 -> 140597366820672
	140597566766432 [label="ct.encoder.layers.14.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597566766432 -> 140597366878320
	140597366878320 [label=AccumulateGrad]
	140597366878368 -> 140597366820672
	140597566766512 [label="ct.encoder.layers.14.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597566766512 -> 140597366878368
	140597366878368 [label=AccumulateGrad]
	140597366820144 -> 140597366819856
	140597566766832 [label="ct.encoder.layers.14.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597566766832 -> 140597366820144
	140597366820144 [label=AccumulateGrad]
	140597366817984 -> 140597366819856
	140597566766912 [label="ct.encoder.layers.14.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597566766912 -> 140597366817984
	140597366817984 [label=AccumulateGrad]
	140597480375392 -> 140597480376592
	140597480375392 -> 140597366837136 [dir=none]
	140597366837136 [label="other
 ()" fillcolor=orange]
	140597480375392 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480377552 -> 140597480375392
	140597480377552 -> 140597366915136 [dir=none]
	140597366915136 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597480377552 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366820480 -> 140597480377552
	140597366820480 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366819328 -> 140597366820480
	140597366819328 -> 140597366915456 [dir=none]
	140597366915456 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597366819328 -> 140597366915376 [dir=none]
	140597366915376 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597366819328 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366881488 -> 140597366819328
	140597520168336 [label="ct.encoder.layers.14.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597520168336 -> 140597366881488
	140597366881488 [label=AccumulateGrad]
	140597366878608 -> 140597366819328
	140597366878608 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366882016 -> 140597366878608
	140597366882016 -> 140597366915696 [dir=none]
	140597366915696 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366882016 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366881200 -> 140597366882016
	140597366881200 -> 140597480542752 [dir=none]
	140597480542752 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366881200 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366407376 -> 140597366881200
	140597366407376 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366407472 -> 140597366407376
	140597366407472 -> 140597366915776 [dir=none]
	140597366915776 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366407472 -> 140597366915536 [dir=none]
	140597366915536 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366407472 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366407568 -> 140597366407472
	140597520168176 [label="ct.encoder.layers.14.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597520168176 -> 140597366407568
	140597366407568 [label=AccumulateGrad]
	140597366407520 -> 140597366407472
	140597366407520 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366407664 -> 140597366407520
	140597366407664 -> 140597520168016 [dir=none]
	140597520168016 [label="bias
 (512)" fillcolor=orange]
	140597366407664 -> 140597480543552 [dir=none]
	140597480543552 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366407664 -> 140597366915936 [dir=none]
	140597366915936 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366407664 -> 140597366915856 [dir=none]
	140597366915856 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366407664 -> 140597520167936 [dir=none]
	140597520167936 [label="weight
 (512)" fillcolor=orange]
	140597366407664 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480376400 -> 140597366407664
	140597366407856 -> 140597366407664
	140597520167936 [label="ct.encoder.layers.14.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597520167936 -> 140597366407856
	140597366407856 [label=AccumulateGrad]
	140597366407808 -> 140597366407664
	140597520168016 [label="ct.encoder.layers.14.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597520168016 -> 140597366407808
	140597366407808 [label=AccumulateGrad]
	140597366407280 -> 140597366407472
	140597366407280 [label=TBackward0]
	140597366407904 -> 140597366407280
	140597520168096 [label="ct.encoder.layers.14.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597520168096 -> 140597366407904
	140597366407904 [label=AccumulateGrad]
	140597366878800 -> 140597366819328
	140597366878800 [label=TBackward0]
	140597366881920 -> 140597366878800
	140597520168256 [label="ct.encoder.layers.14.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597520168256 -> 140597366881920
	140597366881920 [label=AccumulateGrad]
	140597480376688 -> 140597480376832
	140597520168416 [label="ct.encoder.layers.14.norm_out.weight
 (512)" fillcolor=lightblue]
	140597520168416 -> 140597480376688
	140597480376688 [label=AccumulateGrad]
	140597480376880 -> 140597480376832
	140597520168496 [label="ct.encoder.layers.14.norm_out.bias
 (512)" fillcolor=lightblue]
	140597520168496 -> 140597480376880
	140597480376880 [label=AccumulateGrad]
	140597480376544 -> 140597480503952
	140597480376544 [label=TBackward0]
	140597480375920 -> 140597480376544
	140597776161984 [label="acc_classifier.fc1.weight
 (1024, 512)" fillcolor=lightblue]
	140597776161984 -> 140597480375920
	140597480375920 [label=AccumulateGrad]
	140597480504720 -> 140597480503280
	140597480504720 [label=TBackward0]
	140597480502608 -> 140597480504720
	140597776162224 [label="acc_classifier.fc2.weight
 (256, 1024)" fillcolor=lightblue]
	140597776162224 -> 140597480502608
	140597480502608 [label=AccumulateGrad]
	140597480503184 -> 140597480505008
	140597480503184 [label=TBackward0]
	140597366819664 -> 140597480503184
	140597776162384 [label="acc_classifier.fc3.weight
 (3, 256)" fillcolor=lightblue]
	140597776162384 -> 140597366819664
	140597366819664 [label=AccumulateGrad]
	140597480502992 -> 140597480504432
	140597480502992 -> 140597366916096 [dir=none]
	140597366916096 [label="other
 ()" fillcolor=orange]
	140597480502992 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597480503328 -> 140597480502992
	140597480503328 [label="MeanBackward0
----------------
self_numel:    1
self_sizes: (1,)"]
	140597480503808 -> 140597480503328
	140597480503808 [label="CatBackward0
------------
dim: 0"]
	140597481151360 -> 140597480503808
	140597481151360 [label=_RNNTNumbaBackward]
	140597480504240 -> 140597481151360
	140597480504240 [label="AddBackward0
------------
alpha: 1"]
	140597480503376 -> 140597480504240
	140597480503376 [label="UnsafeViewBackward0
------------------------
self_sizes: (3906, 1025)"]
	140597480504048 -> 140597480503376
	140597480504048 -> 140597371177056 [dir=none]
	140597371177056 [label="mat2
 (640, 1025)" fillcolor=orange]
	140597480504048 -> 140597366915616 [dir=none]
	140597366915616 [label="self
 (3906, 640)" fillcolor=orange]
	140597480504048 [label="MmBackward0
----------------------------
mat2        : [saved tensor]
mat2_sizes  :    (640, 1025)
mat2_strides:       (1, 640)
self        : [saved tensor]
self_sizes  :    (3906, 640)
self_strides:       (640, 1)"]
	140597480502704 -> 140597480504048
	140597480502704 [label="ReshapeAliasBackward0
-----------------------------
self_sizes: (1, 126, 31, 640)"]
	140597480503232 -> 140597480502704
	140597480503232 -> 140597366915216 [dir=none]
	140597366915216 [label="result1
 (1, 126, 31, 640)" fillcolor=orange]
	140597480503232 [label="NativeDropoutBackward0
-----------------------
p      :            0.2
result1: [saved tensor]"]
	140597480376736 -> 140597480503232
	140597480376736 -> 140597366916256 [dir=none]
	140597366916256 [label="result
 (1, 126, 31, 640)" fillcolor=orange]
	140597480376736 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140597366881776 -> 140597480376736
	140597366881776 [label="AddBackward0
------------
alpha: 1"]
	140597480374768 -> 140597366881776
	140597480374768 [label="UnsqueezeBackward1
------------------
dim: 2"]
	140597366407760 -> 140597480374768
	140597366407760 [label="ViewBackward0
----------------------
self_sizes: (126, 640)"]
	140597366407952 -> 140597366407760
	140597366407952 -> 140597366916336 [dir=none]
	140597366916336 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366407952 -> 140597366916016 [dir=none]
	140597366916016 [label="mat2
 (512, 640)" fillcolor=orange]
	140597366407952 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 640)
mat2_strides:       (1, 512)"]
	140597366408048 -> 140597366407952
	140597390576192 [label="ct.joint.enc.bias
 (640)" fillcolor=lightblue]
	140597390576192 -> 140597366408048
	140597366408048 [label=AccumulateGrad]
	140597366407712 -> 140597366407952
	140597366407712 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366408144 -> 140597366407712
	140597366408144 [label="SliceBackward0
-------------------------
dim       :             0
end       :             1
self_sizes: (1, 126, 512)
start     :             0
step      :             1"]
	140597366408336 -> 140597366408144
	140597366408336 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366408432 -> 140597366408336
	140597366408432 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366408528 -> 140597366408432
	140597366408528 -> 140597520293696 [dir=none]
	140597520293696 [label="bias
 (512)" fillcolor=orange]
	140597366408528 -> 140597480548528 [dir=none]
	140597480548528 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366408528 -> 140597366915296 [dir=none]
	140597366915296 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366408528 -> 140597366916576 [dir=none]
	140597366916576 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366408528 -> 140597520293616 [dir=none]
	140597520293616 [label="weight
 (512)" fillcolor=orange]
	140597366408528 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597366408624 -> 140597366408528
	140597366408624 [label="AddBackward0
------------
alpha: 1"]
	140597366408816 -> 140597366408624
	140597366408816 [label="AddBackward0
------------
alpha: 1"]
	140597366408960 -> 140597366408816
	140597366408960 [label="AddBackward0
------------
alpha: 1"]
	140597366409104 -> 140597366408960
	140597366409104 [label="AddBackward0
------------
alpha: 1"]
	140597366409248 -> 140597366409104
	140597366409248 -> 140597520290496 [dir=none]
	140597520290496 [label="bias
 (512)" fillcolor=orange]
	140597366409248 -> 140597480546448 [dir=none]
	140597480546448 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366409248 -> 140597366916736 [dir=none]
	140597366916736 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366409248 -> 140597366916416 [dir=none]
	140597366916416 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366409248 -> 140597520290416 [dir=none]
	140597520290416 [label="weight
 (512)" fillcolor=orange]
	140597366409248 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597366409392 -> 140597366409248
	140597366409392 [label="AddBackward0
------------
alpha: 1"]
	140597366409584 -> 140597366409392
	140597366409584 [label="AddBackward0
------------
alpha: 1"]
	140597366409728 -> 140597366409584
	140597366409728 [label="AddBackward0
------------
alpha: 1"]
	140597366409872 -> 140597366409728
	140597366409872 [label="AddBackward0
------------
alpha: 1"]
	140597480376832 -> 140597366409872
	140597366410016 -> 140597366409872
	140597366410016 -> 140597366916176 [dir=none]
	140597366916176 [label="other
 ()" fillcolor=orange]
	140597366410016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597366410112 -> 140597366410016
	140597366410112 -> 140597366916816 [dir=none]
	140597366916816 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597366410112 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366410208 -> 140597366410112
	140597366410208 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366410304 -> 140597366410208
	140597366410304 -> 140597366916496 [dir=none]
	140597366916496 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597366410304 -> 140597366916656 [dir=none]
	140597366916656 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597366410304 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366410400 -> 140597366410304
	140597520168896 [label="ct.encoder.layers.15.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597520168896 -> 140597366410400
	140597366410400 [label=AccumulateGrad]
	140597366410352 -> 140597366410304
	140597366410352 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366410496 -> 140597366410352
	140597366410496 -> 140597366917216 [dir=none]
	140597366917216 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366410496 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366410688 -> 140597366410496
	140597366410688 -> 140597480544272 [dir=none]
	140597480544272 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366410688 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366410784 -> 140597366410688
	140597366410784 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366410880 -> 140597366410784
	140597366410880 -> 140597366917296 [dir=none]
	140597366917296 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366410880 -> 140597366917056 [dir=none]
	140597366917056 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366410880 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366410976 -> 140597366410880
	140597520168736 [label="ct.encoder.layers.15.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597520168736 -> 140597366410976
	140597366410976 [label=AccumulateGrad]
	140597366410928 -> 140597366410880
	140597366410928 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366411072 -> 140597366410928
	140597366411072 -> 140597520168576 [dir=none]
	140597520168576 [label="bias
 (512)" fillcolor=orange]
	140597366411072 -> 140597480544192 [dir=none]
	140597480544192 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366411072 -> 140597366917456 [dir=none]
	140597366917456 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366411072 -> 140597366917376 [dir=none]
	140597366917376 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366411072 -> 140597566766752 [dir=none]
	140597566766752 [label="weight
 (512)" fillcolor=orange]
	140597366411072 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597480376832 -> 140597366411072
	140597366411216 -> 140597366411072
	140597566766752 [label="ct.encoder.layers.15.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597566766752 -> 140597366411216
	140597366411216 [label=AccumulateGrad]
	140597366411168 -> 140597366411072
	140597520168576 [label="ct.encoder.layers.15.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597520168576 -> 140597366411168
	140597366411168 [label=AccumulateGrad]
	140597366410592 -> 140597366410880
	140597366410592 [label=TBackward0]
	140597366411120 -> 140597366410592
	140597520168656 [label="ct.encoder.layers.15.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597520168656 -> 140597366411120
	140597366411120 [label=AccumulateGrad]
	140597366409920 -> 140597366410304
	140597366409920 [label=TBackward0]
	140597366410736 -> 140597366409920
	140597520168816 [label="ct.encoder.layers.15.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597520168816 -> 140597366410736
	140597366410736 [label=AccumulateGrad]
	140597366409824 -> 140597366409728
	140597366409824 -> 140597366917616 [dir=none]
	140597366917616 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597366409824 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366410160 -> 140597366409824
	140597366410160 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366410448 -> 140597366410160
	140597366410448 -> 140597366917696 [dir=none]
	140597366917696 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366410448 -> 140597366916976 [dir=none]
	140597366916976 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366410448 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366411024 -> 140597366410448
	140597520170736 [label="ct.encoder.layers.15.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597520170736 -> 140597366411024
	140597366411024 [label=AccumulateGrad]
	140597366410640 -> 140597366410448
	140597366410640 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366410544 -> 140597366410640
	140597366410544 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597366440144 -> 140597366410544
	140597366440144 [label=CloneBackward0]
	140597366440240 -> 140597366440144
	140597366440240 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366440336 -> 140597366440240
	140597366440336 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597366440432 -> 140597366440336
	140597366440432 -> 140597366917856 [dir=none]
	140597366917856 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597366440432 -> 140597366917776 [dir=none]
	140597366917776 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597366440432 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366440528 -> 140597366440432
	140597366440528 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366440672 -> 140597366440528
	140597366440672 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366440768 -> 140597366440672
	140597366440768 -> 140597366917536 [dir=none]
	140597366917536 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597366440768 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366440864 -> 140597366440768
	140597366440864 -> 140597480545872 [dir=none]
	140597480545872 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366440864 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366440960 -> 140597366440864
	140597366440960 -> 140597366918176 [dir=none]
	140597366918176 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597366440960 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597366441056 -> 140597366440960
	140597366441056 -> 140597480545872 [dir=none]
	140597480545872 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366441056 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366441152 -> 140597366441056
	140597366441152 -> 140597366918096 [dir=none]
	140597366918096 [label="other
 ()" fillcolor=orange]
	140597366441152 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597366441248 -> 140597366441152
	140597366441248 [label="AddBackward0
------------
alpha: 1"]
	140597366441344 -> 140597366441248
	140597366441344 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597366441488 -> 140597366441344
	140597366441488 -> 140597366918016 [dir=none]
	140597366918016 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597366441488 -> 140597366917936 [dir=none]
	140597366917936 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366441488 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366441584 -> 140597366441488
	140597366441584 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366441728 -> 140597366441584
	140597366441728 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366441824 -> 140597366441728
	140597366441824 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366441920 -> 140597366441824
	140597366441920 [label="AddBackward0
------------
alpha: 1"]
	140597366442016 -> 140597366441920
	140597366442016 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366442160 -> 140597366442016
	140597366442160 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366442256 -> 140597366442160
	140597366442256 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366442352 -> 140597366442256
	140597366442352 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366442448 -> 140597366442352
	140597366442448 -> 140597366917136 [dir=none]
	140597366917136 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366442448 -> 140597366918416 [dir=none]
	140597366918416 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366442448 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366442544 -> 140597366442448
	140597520170256 [label="ct.encoder.layers.15.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597520170256 -> 140597366442544
	140597366442544 [label=AccumulateGrad]
	140597366442496 -> 140597366442448
	140597366442496 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366442640 -> 140597366442496
	140597366442640 -> 140597520170096 [dir=none]
	140597520170096 [label="bias
 (512)" fillcolor=orange]
	140597366442640 -> 140597480544672 [dir=none]
	140597480544672 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366442640 -> 140597366918496 [dir=none]
	140597366918496 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366442640 -> 140597366916896 [dir=none]
	140597366916896 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366442640 -> 140597520170016 [dir=none]
	140597520170016 [label="weight
 (512)" fillcolor=orange]
	140597366442640 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597366409872 -> 140597366442640
	140597366442832 -> 140597366442640
	140597520170016 [label="ct.encoder.layers.15.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597520170016 -> 140597366442832
	140597366442832 [label=AccumulateGrad]
	140597366442784 -> 140597366442640
	140597520170096 [label="ct.encoder.layers.15.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597520170096 -> 140597366442784
	140597366442784 [label=AccumulateGrad]
	140597366442064 -> 140597366442448
	140597366442064 [label=TBackward0]
	140597366442880 -> 140597366442064
	140597520170176 [label="ct.encoder.layers.15.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597520170176 -> 140597366442880
	140597366442880 [label=AccumulateGrad]
	140597366441968 -> 140597366441920
	140597520170896 [label="ct.encoder.layers.15.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597520170896 -> 140597366441968
	140597366441968 [label=AccumulateGrad]
	140597366441536 -> 140597366441488
	140597366441536 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597366441872 -> 140597366441536
	140597366441872 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597366442304 -> 140597366441872
	140597366442304 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366442400 -> 140597366442304
	140597366442400 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366442592 -> 140597366442400
	140597366442592 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366442928 -> 140597366442592
	140597366442928 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366443024 -> 140597366442928
	140597366443024 -> 140597366918656 [dir=none]
	140597366918656 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366443024 -> 140597366918256 [dir=none]
	140597366918256 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366443024 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366443120 -> 140597366443024
	140597520170416 [label="ct.encoder.layers.15.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597520170416 -> 140597366443120
	140597366443120 [label=AccumulateGrad]
	140597366443072 -> 140597366443024
	140597366443072 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366442640 -> 140597366443072
	140597366441680 -> 140597366443024
	140597366441680 [label=TBackward0]
	140597366443312 -> 140597366441680
	140597520170336 [label="ct.encoder.layers.15.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597520170336 -> 140597366443312
	140597366443312 [label=AccumulateGrad]
	140597366441296 -> 140597366441248
	140597366441296 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597366441776 -> 140597366441296
	140597366441776 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366442112 -> 140597366441776
	140597366442112 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366442976 -> 140597366442112
	140597366442976 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366442688 -> 140597366442976
	140597366442688 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597366443264 -> 140597366442688
	140597366443264 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597366443360 -> 140597366443264
	140597366443360 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597366443456 -> 140597366443360
	140597366443456 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597366443552 -> 140597366443456
	140597366443552 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597366443648 -> 140597366443552
	140597366443648 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597366443744 -> 140597366443648
	140597366443744 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597366443840 -> 140597366443744
	140597366443840 -> 140597366918576 [dir=none]
	140597366918576 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597366443840 -> 140597366918896 [dir=none]
	140597366918896 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366443840 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366443936 -> 140597366443840
	140597366443936 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366443984 -> 140597366443936
	140597366443984 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366472912 -> 140597366443984
	140597366472912 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366473008 -> 140597366472912
	140597366473008 [label="AddBackward0
------------
alpha: 1"]
	140597366442016 -> 140597366473008
	140597366473104 -> 140597366473008
	140597520289856 [label="ct.encoder.layers.15.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597520289856 -> 140597366473104
	140597366473104 [label=AccumulateGrad]
	140597366443888 -> 140597366443840
	140597366443888 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366472960 -> 140597366443888
	140597366472960 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366473200 -> 140597366472960
	140597366473200 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366473248 -> 140597366473200
	140597366473248 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366473296 -> 140597366473248
	140597366473296 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597366473440 -> 140597366473296
	140597366473440 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597366473536 -> 140597366473440
	140597366473536 -> 140597366918736 [dir=none]
	140597366918736 [label="self
 (251, 512)" fillcolor=orange]
	140597366473536 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597366473632 -> 140597366473536
	140597366473632 [label=TBackward0]
	140597366473728 -> 140597366473632
	140597520170816 [label="ct.encoder.layers.15.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597520170816 -> 140597366473728
	140597366473728 [label=AccumulateGrad]
	140597366440480 -> 140597366440432
	140597366440480 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366440816 -> 140597366440480
	140597366440816 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366441008 -> 140597366440816
	140597366441008 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366441200 -> 140597366441008
	140597366441200 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366441392 -> 140597366441200
	140597366441392 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366442208 -> 140597366441392
	140597366442208 -> 140597366918336 [dir=none]
	140597366918336 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366442208 -> 140597366918976 [dir=none]
	140597366918976 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366442208 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366443168 -> 140597366442208
	140597520170576 [label="ct.encoder.layers.15.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597520170576 -> 140597366443168
	140597366443168 [label=AccumulateGrad]
	140597366442736 -> 140597366442208
	140597366442736 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366442640 -> 140597366442736
	140597366440624 -> 140597366442208
	140597366440624 [label=TBackward0]
	140597366443600 -> 140597366440624
	140597520170496 [label="ct.encoder.layers.15.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597520170496 -> 140597366443600
	140597366443600 [label=AccumulateGrad]
	140597366409968 -> 140597366410448
	140597366409968 [label=TBackward0]
	140597366440192 -> 140597366409968
	140597520170656 [label="ct.encoder.layers.15.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597520170656 -> 140597366440192
	140597366440192 [label=AccumulateGrad]
	140597366409680 -> 140597366409584
	140597366409680 -> 140597366919056 [dir=none]
	140597366919056 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597366409680 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366410256 -> 140597366409680
	140597366410256 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366410832 -> 140597366410256
	140597366410832 -> 140597480544912 [dir=none]
	140597480544912 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366410832 -> 140597520169856 [dir=none]
	140597520169856 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597366410832 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366440288 -> 140597366410832
	140597366440288 -> 140597480545152 [dir=none]
	140597480545152 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597366440288 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366440912 -> 140597366440288
	140597366440912 -> 140597480545552 [dir=none]
	140597480545552 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366440912 -> 140597366918816 [dir=none]
	140597366918816 [label="result1
 (512)" fillcolor=orange]
	140597366440912 -> 140597366485136 [dir=none]
	140597366485136 [label="result2
 (512)" fillcolor=orange]
	140597366440912 -> 140597366485216 [dir=none]
	140597366485216 [label="result3
 (0)" fillcolor=orange]
	140597366440912 -> 140597854177296 [dir=none]
	140597854177296 [label="running_mean
 (512)" fillcolor=orange]
	140597366440912 -> 140597875561312 [dir=none]
	140597875561312 [label="running_var
 (512)" fillcolor=orange]
	140597366440912 -> 140597520169456 [dir=none]
	140597520169456 [label="weight
 (512)" fillcolor=orange]
	140597366440912 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597366440576 -> 140597366440912
	140597366440576 -> 140597480545232 [dir=none]
	140597480545232 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366440576 -> 140597520169296 [dir=none]
	140597520169296 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597366440576 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366443792 -> 140597366440576
	140597366443792 -> 140597480545392 [dir=none]
	140597480545392 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597366443792 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366441440 -> 140597366443792
	140597366441440 -> 140597480545472 [dir=none]
	140597480545472 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597366441440 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597366472768 -> 140597366441440
	140597366472768 -> 140597480546112 [dir=none]
	140597480546112 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366472768 -> 140597520169136 [dir=none]
	140597520169136 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597366472768 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366473392 -> 140597366472768
	140597366473392 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366473680 -> 140597366473392
	140597366473680 -> 140597520169056 [dir=none]
	140597520169056 [label="bias
 (512)" fillcolor=orange]
	140597366473680 -> 140597480545632 [dir=none]
	140597480545632 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366473680 -> 140597366485616 [dir=none]
	140597366485616 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366473680 -> 140597366485056 [dir=none]
	140597366485056 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366473680 -> 140597520168976 [dir=none]
	140597520168976 [label="weight
 (512)" fillcolor=orange]
	140597366473680 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597366409728 -> 140597366473680
	140597366473776 -> 140597366473680
	140597520168976 [label="ct.encoder.layers.15.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597520168976 -> 140597366473776
	140597366473776 [label=AccumulateGrad]
	140597366473824 -> 140597366473680
	140597520169056 [label="ct.encoder.layers.15.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597520169056 -> 140597366473824
	140597366473824 [label=AccumulateGrad]
	140597366473152 -> 140597366472768
	140597520169136 [label="ct.encoder.layers.15.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597520169136 -> 140597366473152
	140597366473152 [label=AccumulateGrad]
	140597366472864 -> 140597366472768
	140597520169216 [label="ct.encoder.layers.15.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597520169216 -> 140597366472864
	140597366472864 [label=AccumulateGrad]
	140597366443504 -> 140597366440576
	140597520169296 [label="ct.encoder.layers.15.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597520169296 -> 140597366443504
	140597366443504 [label=AccumulateGrad]
	140597366443408 -> 140597366440576
	140597520169376 [label="ct.encoder.layers.15.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597520169376 -> 140597366443408
	140597366443408 [label=AccumulateGrad]
	140597366441104 -> 140597366440912
	140597520169456 [label="ct.encoder.layers.15.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597520169456 -> 140597366441104
	140597366441104 [label=AccumulateGrad]
	140597366440000 -> 140597366440912
	140597520169536 [label="ct.encoder.layers.15.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597520169536 -> 140597366440000
	140597366440000 [label=AccumulateGrad]
	140597366440384 -> 140597366410832
	140597520169856 [label="ct.encoder.layers.15.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597520169856 -> 140597366440384
	140597366440384 [label=AccumulateGrad]
	140597366440096 -> 140597366410832
	140597520169936 [label="ct.encoder.layers.15.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597520169936 -> 140597366440096
	140597366440096 [label=AccumulateGrad]
	140597366409536 -> 140597366409392
	140597366409536 -> 140597366485776 [dir=none]
	140597366485776 [label="other
 ()" fillcolor=orange]
	140597366409536 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597366409776 -> 140597366409536
	140597366409776 -> 140597366485696 [dir=none]
	140597366485696 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597366409776 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366409632 -> 140597366409776
	140597366409632 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366443696 -> 140597366409632
	140597366443696 -> 140597366486016 [dir=none]
	140597366486016 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597366443696 -> 140597366485536 [dir=none]
	140597366485536 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597366443696 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366443216 -> 140597366443696
	140597520290336 [label="ct.encoder.layers.15.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597520290336 -> 140597366443216
	140597366443216 [label=AccumulateGrad]
	140597366440048 -> 140597366443696
	140597366440048 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366473584 -> 140597366440048
	140597366473584 -> 140597366485456 [dir=none]
	140597366485456 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366473584 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366473920 -> 140597366473584
	140597366473920 -> 140597480544992 [dir=none]
	140597480544992 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366473920 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366474016 -> 140597366473920
	140597366474016 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366474112 -> 140597366474016
	140597366474112 -> 140597366485296 [dir=none]
	140597366485296 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366474112 -> 140597366486096 [dir=none]
	140597366486096 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366474112 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366474208 -> 140597366474112
	140597520290176 [label="ct.encoder.layers.15.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597520290176 -> 140597366474208
	140597366474208 [label=AccumulateGrad]
	140597366474160 -> 140597366474112
	140597366474160 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366474304 -> 140597366474160
	140597366474304 -> 140597520290016 [dir=none]
	140597520290016 [label="bias
 (512)" fillcolor=orange]
	140597366474304 -> 140597480545792 [dir=none]
	140597480545792 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366474304 -> 140597366486256 [dir=none]
	140597366486256 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366474304 -> 140597366486176 [dir=none]
	140597366486176 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366474304 -> 140597520289936 [dir=none]
	140597520289936 [label="weight
 (512)" fillcolor=orange]
	140597366474304 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597366409584 -> 140597366474304
	140597366474496 -> 140597366474304
	140597520289936 [label="ct.encoder.layers.15.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597520289936 -> 140597366474496
	140597366474496 [label=AccumulateGrad]
	140597366474448 -> 140597366474304
	140597520290016 [label="ct.encoder.layers.15.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597520290016 -> 140597366474448
	140597366474448 [label=AccumulateGrad]
	140597366472816 -> 140597366474112
	140597366472816 [label=TBackward0]
	140597366474544 -> 140597366472816
	140597520290096 [label="ct.encoder.layers.15.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597520290096 -> 140597366474544
	140597366474544 [label=AccumulateGrad]
	140597366473056 -> 140597366443696
	140597366473056 [label=TBackward0]
	140597366473968 -> 140597366473056
	140597520290256 [label="ct.encoder.layers.15.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597520290256 -> 140597366473968
	140597366473968 [label=AccumulateGrad]
	140597366409344 -> 140597366409248
	140597520290416 [label="ct.encoder.layers.15.norm_out.weight
 (512)" fillcolor=lightblue]
	140597520290416 -> 140597366409344
	140597366409344 [label=AccumulateGrad]
	140597366409296 -> 140597366409248
	140597520290496 [label="ct.encoder.layers.15.norm_out.bias
 (512)" fillcolor=lightblue]
	140597520290496 -> 140597366409296
	140597366409296 [label=AccumulateGrad]
	140597366409200 -> 140597366409104
	140597366409200 -> 140597366486416 [dir=none]
	140597366486416 [label="other
 ()" fillcolor=orange]
	140597366409200 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597366410064 -> 140597366409200
	140597366410064 -> 140597366486336 [dir=none]
	140597366486336 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597366410064 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366441632 -> 140597366410064
	140597366441632 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366409440 -> 140597366441632
	140597366409440 -> 140597366486576 [dir=none]
	140597366486576 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597366409440 -> 140597366485376 [dir=none]
	140597366485376 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597366409440 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366474064 -> 140597366409440
	140597520290976 [label="ct.encoder.layers.16.feed_forward1.linear2.bias
 (512)" fillcolor=lightblue]
	140597520290976 -> 140597366474064
	140597366474064 [label=AccumulateGrad]
	140597366474256 -> 140597366409440
	140597366474256 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366474400 -> 140597366474256
	140597366474400 -> 140597366486736 [dir=none]
	140597366486736 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366474400 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366474688 -> 140597366474400
	140597366474688 -> 140597480546608 [dir=none]
	140597480546608 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366474688 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366474784 -> 140597366474688
	140597366474784 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366474880 -> 140597366474784
	140597366474880 -> 140597366486816 [dir=none]
	140597366486816 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366474880 -> 140597366486656 [dir=none]
	140597366486656 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366474880 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366474976 -> 140597366474880
	140597520290816 [label="ct.encoder.layers.16.feed_forward1.linear1.bias
 (2048)" fillcolor=lightblue]
	140597520290816 -> 140597366474976
	140597366474976 [label=AccumulateGrad]
	140597366474928 -> 140597366474880
	140597366474928 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366475072 -> 140597366474928
	140597366475072 -> 140597520290656 [dir=none]
	140597520290656 [label="bias
 (512)" fillcolor=orange]
	140597366475072 -> 140597480546528 [dir=none]
	140597480546528 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366475072 -> 140597366486976 [dir=none]
	140597366486976 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366475072 -> 140597366486896 [dir=none]
	140597366486896 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366475072 -> 140597520290576 [dir=none]
	140597520290576 [label="weight
 (512)" fillcolor=orange]
	140597366475072 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597366409248 -> 140597366475072
	140597366475264 -> 140597366475072
	140597520290576 [label="ct.encoder.layers.16.norm_feed_forward1.weight
 (512)" fillcolor=lightblue]
	140597520290576 -> 140597366475264
	140597366475264 [label=AccumulateGrad]
	140597366475216 -> 140597366475072
	140597520290656 [label="ct.encoder.layers.16.norm_feed_forward1.bias
 (512)" fillcolor=lightblue]
	140597520290656 -> 140597366475216
	140597366475216 [label=AccumulateGrad]
	140597366474592 -> 140597366474880
	140597366474592 [label=TBackward0]
	140597366475312 -> 140597366474592
	140597520290736 [label="ct.encoder.layers.16.feed_forward1.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597520290736 -> 140597366475312
	140597366475312 [label=AccumulateGrad]
	140597366473344 -> 140597366409440
	140597366473344 [label=TBackward0]
	140597366474736 -> 140597366473344
	140597520290896 [label="ct.encoder.layers.16.feed_forward1.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597520290896 -> 140597366474736
	140597366474736 [label=AccumulateGrad]
	140597366409056 -> 140597366408960
	140597366409056 -> 140597366487136 [dir=none]
	140597366487136 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597366409056 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366440720 -> 140597366409056
	140597366440720 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366409152 -> 140597366440720
	140597366409152 -> 140597366487216 [dir=none]
	140597366487216 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366409152 -> 140597366486496 [dir=none]
	140597366486496 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366409152 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366475024 -> 140597366409152
	140597520292816 [label="ct.encoder.layers.16.self_attn.linear_out.bias
 (512)" fillcolor=lightblue]
	140597520292816 -> 140597366475024
	140597366475024 [label=AccumulateGrad]
	140597366474352 -> 140597366409152
	140597366474352 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366474640 -> 140597366474352
	140597366474640 [label="UnsafeViewBackward0
---------------------------
self_sizes: (1, 126, 8, 64)"]
	140597366475120 -> 140597366474640
	140597366475120 [label=CloneBackward0]
	140597366475504 -> 140597366475120
	140597366475504 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366475600 -> 140597366475504
	140597366475600 [label="UnsafeViewBackward0
------------------------
self_sizes: (8, 126, 64)"]
	140597366475696 -> 140597366475600
	140597366475696 -> 140597366487376 [dir=none]
	140597366487376 [label="mat2
 (8, 126, 64)" fillcolor=orange]
	140597366475696 -> 140597366487296 [dir=none]
	140597366487296 [label="self
 (8, 126, 126)" fillcolor=orange]
	140597366475696 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366475792 -> 140597366475696
	140597366475792 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366475936 -> 140597366475792
	140597366475936 [label="ExpandBackward0
----------------------------
self_sizes: (1, 8, 126, 126)"]
	140597366476032 -> 140597366475936
	140597366476032 -> 140597366487056 [dir=none]
	140597366487056 [label="result1
 (1, 8, 126, 126)" fillcolor=orange]
	140597366476032 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366476128 -> 140597366476032
	140597366476128 -> 140597480548048 [dir=none]
	140597480548048 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366476128 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366476224 -> 140597366476128
	140597366476224 -> 140597366487696 [dir=none]
	140597366487696 [label="result
 (1, 8, 126, 126)" fillcolor=orange]
	140597366476224 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140597366476320 -> 140597366476224
	140597366476320 -> 140597480548048 [dir=none]
	140597480548048 [label="mask
 (1, 1, 126, 126)" fillcolor=orange]
	140597366476320 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366476416 -> 140597366476320
	140597366476416 -> 140597366487616 [dir=none]
	140597366487616 [label="other
 ()" fillcolor=orange]
	140597366476416 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597366476512 -> 140597366476416
	140597366476512 [label="AddBackward0
------------
alpha: 1"]
	140597366476608 -> 140597366476512
	140597366476608 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 126)"]
	140597366476752 -> 140597366476608
	140597366476752 -> 140597366487536 [dir=none]
	140597366487536 [label="mat2
 (8, 64, 126)" fillcolor=orange]
	140597366476752 -> 140597366487456 [dir=none]
	140597366487456 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366476752 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366476656 -> 140597366476752
	140597366476656 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366522112 -> 140597366476656
	140597366522112 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366522208 -> 140597366522112
	140597366522208 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366522304 -> 140597366522208
	140597366522304 [label="AddBackward0
------------
alpha: 1"]
	140597366522400 -> 140597366522304
	140597366522400 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366522544 -> 140597366522400
	140597366522544 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366522640 -> 140597366522544
	140597366522640 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366522736 -> 140597366522640
	140597366522736 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366522832 -> 140597366522736
	140597366522832 -> 140597366485856 [dir=none]
	140597366485856 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366522832 -> 140597366487936 [dir=none]
	140597366487936 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366522832 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366522928 -> 140597366522832
	140597520292336 [label="ct.encoder.layers.16.self_attn.linear_q.bias
 (512)" fillcolor=lightblue]
	140597520292336 -> 140597366522928
	140597366522928 [label=AccumulateGrad]
	140597366522880 -> 140597366522832
	140597366522880 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366523024 -> 140597366522880
	140597366523024 -> 140597520292176 [dir=none]
	140597520292176 [label="bias
 (512)" fillcolor=orange]
	140597366523024 -> 140597480546848 [dir=none]
	140597480546848 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366523024 -> 140597366488016 [dir=none]
	140597366488016 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366523024 -> 140597366485936 [dir=none]
	140597366485936 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366523024 -> 140597520292096 [dir=none]
	140597520292096 [label="weight
 (512)" fillcolor=orange]
	140597366523024 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597366409104 -> 140597366523024
	140597366523216 -> 140597366523024
	140597520292096 [label="ct.encoder.layers.16.norm_self_att.weight
 (512)" fillcolor=lightblue]
	140597520292096 -> 140597366523216
	140597366523216 [label=AccumulateGrad]
	140597366523168 -> 140597366523024
	140597520292176 [label="ct.encoder.layers.16.norm_self_att.bias
 (512)" fillcolor=lightblue]
	140597520292176 -> 140597366523168
	140597366523168 [label=AccumulateGrad]
	140597366522448 -> 140597366522832
	140597366522448 [label=TBackward0]
	140597366523264 -> 140597366522448
	140597520292256 [label="ct.encoder.layers.16.self_attn.linear_q.weight
 (512, 512)" fillcolor=lightblue]
	140597520292256 -> 140597366523264
	140597366523264 [label=AccumulateGrad]
	140597366522352 -> 140597366522304
	140597520292976 [label="ct.encoder.layers.16.self_attn.pos_bias_u
 (8, 64)" fillcolor=lightblue]
	140597520292976 -> 140597366522352
	140597366522352 [label=AccumulateGrad]
	140597366521968 -> 140597366476752
	140597366521968 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597366522256 -> 140597366521968
	140597366522256 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 126)"]
	140597366522688 -> 140597366522256
	140597366522688 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366522784 -> 140597366522688
	140597366522784 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366522976 -> 140597366522784
	140597366522976 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366523312 -> 140597366522976
	140597366523312 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366523408 -> 140597366523312
	140597366523408 -> 140597366488176 [dir=none]
	140597366488176 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366523408 -> 140597366487776 [dir=none]
	140597366487776 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366523408 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366523504 -> 140597366523408
	140597520292496 [label="ct.encoder.layers.16.self_attn.linear_k.bias
 (512)" fillcolor=lightblue]
	140597520292496 -> 140597366523504
	140597366523504 [label=AccumulateGrad]
	140597366523456 -> 140597366523408
	140597366523456 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366523024 -> 140597366523456
	140597366522064 -> 140597366523408
	140597366522064 [label=TBackward0]
	140597366523696 -> 140597366522064
	140597520292416 [label="ct.encoder.layers.16.self_attn.linear_k.weight
 (512, 512)" fillcolor=lightblue]
	140597520292416 -> 140597366523696
	140597366523696 [label=AccumulateGrad]
	140597366476560 -> 140597366476512
	140597366476560 [label="SliceBackward0
----------------------------
dim       :                3
end       :              126
self_sizes: (1, 8, 126, 251)
start     :                0
step      :                1"]
	140597366476704 -> 140597366476560
	140597366476704 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366522496 -> 140597366476704
	140597366522496 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366523360 -> 140597366522496
	140597366523360 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 126, 251)
start     :                   0
step      :                   1"]
	140597366523072 -> 140597366523360
	140597366523072 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 251, 126)"]
	140597366523648 -> 140597366523072
	140597366523648 [label="SliceBackward0
-------------------------------
dim       :                   2
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   1
step      :                   1"]
	140597366523744 -> 140597366523648
	140597366523744 [label="SliceBackward0
-------------------------------
dim       :                   1
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597366523840 -> 140597366523744
	140597366523840 [label="SliceBackward0
-------------------------------
dim       :                   0
end       : 9223372036854775807
self_sizes:    (1, 8, 252, 126)
start     :                   0
step      :                   1"]
	140597366523936 -> 140597366523840
	140597366523936 [label="ViewBackward0
----------------------------
self_sizes: (1, 8, 126, 252)"]
	140597366524032 -> 140597366523936
	140597366524032 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	140597366524128 -> 140597366524032
	140597366524128 [label="UnsafeViewBackward0
-------------------------
self_sizes: (8, 126, 251)"]
	140597366524224 -> 140597366524128
	140597366524224 -> 140597366488736 [dir=none]
	140597366488736 [label="mat2
 (8, 64, 251)" fillcolor=orange]
	140597366524224 -> 140597366487856 [dir=none]
	140597366487856 [label="self
 (8, 126, 64)" fillcolor=orange]
	140597366524224 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140597366524320 -> 140597366524224
	140597366524320 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366524464 -> 140597366524320
	140597366524464 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366524560 -> 140597366524464
	140597366524560 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366524656 -> 140597366524560
	140597366524656 [label="AddBackward0
------------
alpha: 1"]
	140597366522400 -> 140597366524656
	140597366524752 -> 140597366524656
	140597520293056 [label="ct.encoder.layers.16.self_attn.pos_bias_v
 (8, 64)" fillcolor=lightblue]
	140597520293056 -> 140597366524752
	140597366524752 [label=AccumulateGrad]
	140597366524272 -> 140597366524224
	140597366524272 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366524608 -> 140597366524272
	140597366524608 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 64, 251)"]
	140597366524848 -> 140597366524608
	140597366524848 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140597366524896 -> 140597366524848
	140597366524896 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366524944 -> 140597366524896
	140597366524944 [label="ViewBackward0
-------------------------
self_sizes: (1, 251, 512)"]
	140597366525088 -> 140597366524944
	140597366525088 [label="UnsafeViewBackward0
----------------------
self_sizes: (251, 512)"]
	140597366525184 -> 140597366525088
	140597366525184 -> 140597366488256 [dir=none]
	140597366488256 [label="self
 (251, 512)" fillcolor=orange]
	140597366525184 [label="MmBackward0
----------------------------
mat2        :           None
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)
self        : [saved tensor]
self_sizes  :     (251, 512)
self_strides:             ()"]
	140597366525280 -> 140597366525184
	140597366525280 [label=TBackward0]
	140597366525376 -> 140597366525280
	140597520292896 [label="ct.encoder.layers.16.self_attn.linear_pos.weight
 (512, 512)" fillcolor=lightblue]
	140597520292896 -> 140597366525376
	140597366525376 [label=AccumulateGrad]
	140597366475744 -> 140597366475696
	140597366475744 [label="ReshapeAliasBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366476080 -> 140597366475744
	140597366476080 [label="ExpandBackward0
---------------------------
self_sizes: (1, 8, 126, 64)"]
	140597366476272 -> 140597366476080
	140597366476272 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366476464 -> 140597366476272
	140597366476464 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366475840 -> 140597366476464
	140597366475840 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366522592 -> 140597366475840
	140597366522592 -> 140597366488576 [dir=none]
	140597366488576 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366522592 -> 140597366488496 [dir=none]
	140597366488496 [label="mat2
 (512, 512)" fillcolor=orange]
	140597366522592 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :     (512, 512)
mat2_strides:       (1, 512)"]
	140597366523552 -> 140597366522592
	140597520292656 [label="ct.encoder.layers.16.self_attn.linear_v.bias
 (512)" fillcolor=lightblue]
	140597520292656 -> 140597366523552
	140597366523552 [label=AccumulateGrad]
	140597366523120 -> 140597366522592
	140597366523120 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366523024 -> 140597366523120
	140597366522160 -> 140597366522592
	140597366522160 [label=TBackward0]
	140597366523984 -> 140597366522160
	140597520292576 [label="ct.encoder.layers.16.self_attn.linear_v.weight
 (512, 512)" fillcolor=lightblue]
	140597520292576 -> 140597366523984
	140597366523984 [label=AccumulateGrad]
	140597366473872 -> 140597366409152
	140597366473872 [label=TBackward0]
	140597366475456 -> 140597366473872
	140597520292736 [label="ct.encoder.layers.16.self_attn.linear_out.weight
 (512, 512)" fillcolor=lightblue]
	140597520292736 -> 140597366475456
	140597366475456 [label=AccumulateGrad]
	140597366408912 -> 140597366408816
	140597366408912 -> 140597366488416 [dir=none]
	140597366488416 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597366408912 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366409488 -> 140597366408912
	140597366409488 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366475360 -> 140597366409488
	140597366475360 -> 140597480547088 [dir=none]
	140597480547088 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366475360 -> 140597520291936 [dir=none]
	140597520291936 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140597366475360 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366475552 -> 140597366475360
	140597366475552 -> 140597480547328 [dir=none]
	140597480547328 [label="self
 (1, 512, 126)" fillcolor=orange]
	140597366475552 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366476176 -> 140597366475552
	140597366476176 -> 140597480547728 [dir=none]
	140597480547728 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366476176 -> 140597366488096 [dir=none]
	140597366488096 [label="result1
 (512)" fillcolor=orange]
	140597366476176 -> 140597366488896 [dir=none]
	140597366488896 [label="result2
 (512)" fillcolor=orange]
	140597366476176 -> 140597366488336 [dir=none]
	140597366488336 [label="result3
 (0)" fillcolor=orange]
	140597366476176 -> 140597566766672 [dir=none]
	140597566766672 [label="running_mean
 (512)" fillcolor=orange]
	140597366476176 -> 140597520169776 [dir=none]
	140597520169776 [label="running_var
 (512)" fillcolor=orange]
	140597366476176 -> 140597520291536 [dir=none]
	140597520291536 [label="weight
 (512)" fillcolor=orange]
	140597366476176 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	140597366475888 -> 140597366476176
	140597366475888 -> 140597480547408 [dir=none]
	140597480547408 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366475888 -> 140597520291376 [dir=none]
	140597520291376 [label="weight
 (512, 1, 31)" fillcolor=orange]
	140597366475888 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (512,)
dilation      :           (1,)
groups        :            512
input         : [saved tensor]
output_padding:           (0,)
padding       :          (15,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366524176 -> 140597366475888
	140597366524176 -> 140597480547568 [dir=none]
	140597480547568 [label="mask
 (1, 1, 126)" fillcolor=orange]
	140597366524176 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140597366524512 -> 140597366524176
	140597366524512 -> 140597480547648 [dir=none]
	140597480547648 [label="self
 (1, 1024, 126)" fillcolor=orange]
	140597366524512 [label="GluBackward0
--------------------
dim :              1
self: [saved tensor]"]
	140597366524368 -> 140597366524512
	140597366524368 -> 140597480548288 [dir=none]
	140597480548288 [label="input
 (1, 512, 126)" fillcolor=orange]
	140597366524368 -> 140597520291216 [dir=none]
	140597520291216 [label="weight
 (1024, 512, 1)" fillcolor=orange]
	140597366524368 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:        (1024,)
dilation      :           (1,)
groups        :              1
input         : [saved tensor]
output_padding:           (0,)
padding       :           (0,)
stride        :           (1,)
transposed    :          False
weight        : [saved tensor]"]
	140597366525040 -> 140597366524368
	140597366525040 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366525328 -> 140597366525040
	140597366525328 -> 140597520291136 [dir=none]
	140597520291136 [label="bias
 (512)" fillcolor=orange]
	140597366525328 -> 140597480547808 [dir=none]
	140597480547808 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366525328 -> 140597366488656 [dir=none]
	140597366488656 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366525328 -> 140597366488816 [dir=none]
	140597366488816 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366525328 -> 140597520291056 [dir=none]
	140597520291056 [label="weight
 (512)" fillcolor=orange]
	140597366525328 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597366408960 -> 140597366525328
	140597366525424 -> 140597366525328
	140597520291056 [label="ct.encoder.layers.16.norm_conv.weight
 (512)" fillcolor=lightblue]
	140597520291056 -> 140597366525424
	140597366525424 [label=AccumulateGrad]
	140597366525472 -> 140597366525328
	140597520291136 [label="ct.encoder.layers.16.norm_conv.bias
 (512)" fillcolor=lightblue]
	140597520291136 -> 140597366525472
	140597366525472 [label=AccumulateGrad]
	140597366524800 -> 140597366524368
	140597520291216 [label="ct.encoder.layers.16.conv.pointwise_conv1.weight
 (1024, 512, 1)" fillcolor=lightblue]
	140597520291216 -> 140597366524800
	140597366524800 [label=AccumulateGrad]
	140597366523600 -> 140597366524368
	140597520291296 [label="ct.encoder.layers.16.conv.pointwise_conv1.bias
 (1024)" fillcolor=lightblue]
	140597520291296 -> 140597366523600
	140597366523600 [label=AccumulateGrad]
	140597366523888 -> 140597366475888
	140597520291376 [label="ct.encoder.layers.16.conv.depthwise_conv.weight
 (512, 1, 31)" fillcolor=lightblue]
	140597520291376 -> 140597366523888
	140597366523888 [label=AccumulateGrad]
	140597366523792 -> 140597366475888
	140597520291456 [label="ct.encoder.layers.16.conv.depthwise_conv.bias
 (512)" fillcolor=lightblue]
	140597520291456 -> 140597366523792
	140597366523792 [label=AccumulateGrad]
	140597366476368 -> 140597366476176
	140597520291536 [label="ct.encoder.layers.16.conv.batch_norm.weight
 (512)" fillcolor=lightblue]
	140597520291536 -> 140597366476368
	140597366476368 [label=AccumulateGrad]
	140597366475408 -> 140597366476176
	140597520291616 [label="ct.encoder.layers.16.conv.batch_norm.bias
 (512)" fillcolor=lightblue]
	140597520291616 -> 140597366475408
	140597366475408 [label=AccumulateGrad]
	140597366475648 -> 140597366475360
	140597520291936 [label="ct.encoder.layers.16.conv.pointwise_conv2.weight
 (512, 512, 1)" fillcolor=lightblue]
	140597520291936 -> 140597366475648
	140597366475648 [label=AccumulateGrad]
	140597366473488 -> 140597366475360
	140597520292016 [label="ct.encoder.layers.16.conv.pointwise_conv2.bias
 (512)" fillcolor=lightblue]
	140597520292016 -> 140597366473488
	140597366473488 [label=AccumulateGrad]
	140597366408768 -> 140597366408624
	140597366408768 -> 140597366488976 [dir=none]
	140597366488976 [label="other
 ()" fillcolor=orange]
	140597366408768 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140597366409008 -> 140597366408768
	140597366409008 -> 140597366554688 [dir=none]
	140597366554688 [label="result1
 (1, 126, 512)" fillcolor=orange]
	140597366409008 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366475984 -> 140597366409008
	140597366475984 [label="ViewBackward0
----------------------
self_sizes: (126, 512)"]
	140597366474832 -> 140597366475984
	140597366474832 -> 140597366555008 [dir=none]
	140597366555008 [label="mat1
 (126, 2048)" fillcolor=orange]
	140597366474832 -> 140597366554768 [dir=none]
	140597366554768 [label="mat2
 (2048, 512)" fillcolor=orange]
	140597366474832 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (126, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 512)
mat2_strides:      (1, 2048)"]
	140597366524704 -> 140597366474832
	140597520293536 [label="ct.encoder.layers.16.feed_forward2.linear2.bias
 (512)" fillcolor=lightblue]
	140597520293536 -> 140597366524704
	140597366524704 [label=AccumulateGrad]
	140597366521920 -> 140597366474832
	140597366521920 [label="ViewBackward0
--------------------------
self_sizes: (1, 126, 2048)"]
	140597366525232 -> 140597366521920
	140597366525232 -> 140597366555248 [dir=none]
	140597366555248 [label="result1
 (1, 126, 2048)" fillcolor=orange]
	140597366525232 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140597366525568 -> 140597366525232
	140597366525568 -> 140597480547168 [dir=none]
	140597480547168 [label="self
 (1, 126, 2048)" fillcolor=orange]
	140597366525568 [label="SiluBackward0
--------------------
self: [saved tensor]"]
	140597366525664 -> 140597366525568
	140597366525664 [label="ViewBackward0
-----------------------
self_sizes: (126, 2048)"]
	140597366525760 -> 140597366525664
	140597366525760 -> 140597366555328 [dir=none]
	140597366555328 [label="mat1
 (126, 512)" fillcolor=orange]
	140597366525760 -> 140597366555088 [dir=none]
	140597366555088 [label="mat2
 (512, 2048)" fillcolor=orange]
	140597366525760 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (126, 512)
mat1_strides:       (512, 1)
mat2        : [saved tensor]
mat2_sizes  :    (512, 2048)
mat2_strides:       (1, 512)"]
	140597366525856 -> 140597366525760
	140597520293376 [label="ct.encoder.layers.16.feed_forward2.linear1.bias
 (2048)" fillcolor=lightblue]
	140597520293376 -> 140597366525856
	140597366525856 [label=AccumulateGrad]
	140597366525808 -> 140597366525760
	140597366525808 [label="ViewBackward0
-------------------------
self_sizes: (1, 126, 512)"]
	140597366562880 -> 140597366525808
	140597366562880 -> 140597520293216 [dir=none]
	140597520293216 [label="bias
 (512)" fillcolor=orange]
	140597366562880 -> 140597480547968 [dir=none]
	140597480547968 [label="input
 (1, 126, 512)" fillcolor=orange]
	140597366562880 -> 140597366555488 [dir=none]
	140597366555488 [label="result1
 (1, 126, 1)" fillcolor=orange]
	140597366562880 -> 140597366555408 [dir=none]
	140597366555408 [label="result2
 (1, 126, 1)" fillcolor=orange]
	140597366562880 -> 140597520293136 [dir=none]
	140597520293136 [label="weight
 (512)" fillcolor=orange]
	140597366562880 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (512,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140597366408816 -> 140597366562880
	140597366563072 -> 140597366562880
	140597520293136 [label="ct.encoder.layers.16.norm_feed_forward2.weight
 (512)" fillcolor=lightblue]
	140597520293136 -> 140597366563072
	140597366563072 [label=AccumulateGrad]
	140597366563024 -> 140597366562880
	140597520293216 [label="ct.encoder.layers.16.norm_feed_forward2.bias
 (512)" fillcolor=lightblue]
	140597520293216 -> 140597366563024
	140597366563024 [label=AccumulateGrad]
	140597366524416 -> 140597366525760
	140597366524416 [label=TBackward0]
	140597366563120 -> 140597366524416
	140597520293296 [label="ct.encoder.layers.16.feed_forward2.linear1.weight
 (2048, 512)" fillcolor=lightblue]
	140597520293296 -> 140597366563120
	140597366563120 [label=AccumulateGrad]
	140597366522016 -> 140597366474832
	140597366522016 [label=TBackward0]
	140597366525616 -> 140597366522016
	140597520293456 [label="ct.encoder.layers.16.feed_forward2.linear2.weight
 (512, 2048)" fillcolor=lightblue]
	140597520293456 -> 140597366525616
	140597366525616 [label=AccumulateGrad]
	140597366408576 -> 140597366408528
	140597520293616 [label="ct.encoder.layers.16.norm_out.weight
 (512)" fillcolor=lightblue]
	140597520293616 -> 140597366408576
	140597366408576 [label=AccumulateGrad]
	140597366408240 -> 140597366408528
	140597520293696 [label="ct.encoder.layers.16.norm_out.bias
 (512)" fillcolor=lightblue]
	140597520293696 -> 140597366408240
	140597366408240 [label=AccumulateGrad]
	140597366407424 -> 140597366407952
	140597366407424 [label=TBackward0]
	140597366408384 -> 140597366407424
	140597390576112 [label="ct.joint.enc.weight
 (640, 512)" fillcolor=lightblue]
	140597390576112 -> 140597366408384
	140597366408384 [label=AccumulateGrad]
	140597366407616 -> 140597366881776
	140597366407616 [label="UnsqueezeBackward1
------------------
dim: 1"]
	140597366408096 -> 140597366407616
	140597366408096 [label="ViewBackward0
---------------------
self_sizes: (31, 640)"]
	140597366408672 -> 140597366408096
	140597366408672 -> 140597366555168 [dir=none]
	140597366555168 [label="mat1
 (31, 640)" fillcolor=orange]
	140597366408672 -> 140597366555648 [dir=none]
	140597366555648 [label="mat2
 (640, 640)" fillcolor=orange]
	140597366408672 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :      (31, 640)
mat1_strides:       (640, 1)
mat2        : [saved tensor]
mat2_sizes  :     (640, 640)
mat2_strides:       (1, 640)"]
	140597366408192 -> 140597366408672
	140597390576032 [label="ct.joint.pred.bias
 (640)" fillcolor=lightblue]
	140597390576032 -> 140597366408192
	140597366408192 [label=AccumulateGrad]
	140597366408480 -> 140597366408672
	140597366408480 [label="ViewBackward0
------------------------
self_sizes: (1, 31, 640)"]
	140597366408864 -> 140597366408480
	140597366408864 [label="SliceBackward0
------------------------
dim       :            0
end       :            1
self_sizes: (1, 31, 640)
start     :            0
step      :            1"]
	140597366525520 -> 140597366408864
	140597366525520 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366525712 -> 140597366525520
	140597366525712 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140597366525136 -> 140597366525712
	140597366525136 [label="TransposeBackward0
------------------
dim0: 0
dim1: 1"]
	140597366563168 -> 140597366525136
	140597366563168 -> 140597366555568 [dir=none]
	140597366555568 [label="result1
 (31, 1, 640)" fillcolor=orange]
	140597366563168 [label="NativeDropoutBackward0
-----------------------
p      :            0.2
result1: [saved tensor]"]
	140597366563264 -> 140597366563168
	140597366563264 -> 140597480549008 [dir=none]
	140597480549008 [label="cx
 (1, 1, 640)" fillcolor=orange]
	140597366563264 -> 140597366555808 [dir=none]
	140597366555808 [label="dropout_state
 (393216)" fillcolor=orange]
	140597366563264 -> 140597480548688 [dir=none]
	140597480548688 [label="hx
 (1, 1, 640)" fillcolor=orange]
	140597366563264 -> 140597480454560 [dir=none]
	140597480454560 [label="input
 (31, 1, 640)" fillcolor=orange]
	140597366563264 -> 140597366556048 [dir=none]
	140597366556048 [label="result0
 (31, 1, 640)" fillcolor=orange]
	140597366563264 -> 140597366555968 [dir=none]
	140597366555968 [label="result3
 (396816)" fillcolor=orange]
	140597366563264 -> 140597366555888 [dir=none]
	140597366555888 [label="result4
 (3281920)" fillcolor=orange]
	140597366563264 -> 140597478144496 [dir=none]
	140597478144496 [label="weight[0]
 (2560, 640)" fillcolor=orange]
	140597366563264 -> 140597478075504 [dir=none]
	140597478075504 [label="weight[1]
 (2560, 640)" fillcolor=orange]
	140597366563264 -> 140597478075584 [dir=none]
	140597478075584 [label="weight[2]
 (2560)" fillcolor=orange]
	140597366563264 -> 140597478075664 [dir=none]
	140597478075664 [label="weight[3]
 (2560)" fillcolor=orange]
	140597366563264 [label="CudnnRnnBackward0
-------------------------------
batch_first   :           False
batch_sizes   :              ()
bidirectional :           False
cx            :  [saved tensor]
dropout       :             0.2
dropout_state :  [saved tensor]
hidden_size   :             640
hx            :  [saved tensor]
input         :  [saved tensor]
mode          :               2
num_layers    :               1
proj_size     :               0
result0       :  [saved tensor]
result3       :  [saved tensor]
result4       :  [saved tensor]
train         :            True
weight        : [saved tensors]
weight_stride0:               4"]
	140597366563360 -> 140597366563264
	140597366563360 [label="TransposeBackward0
------------------
dim0: 0
dim1: 1"]
	140597366563600 -> 140597366563360
	140597366563600 [label="CatBackward0
------------
dim: 1"]
	140597366563744 -> 140597366563600
	140597366563744 -> 140597480453040 [dir=none]
	140597480453040 [label="indices
 (1, 30)" fillcolor=orange]
	140597366563744 [label="EmbeddingBackward0
----------------------------------
indices           : [saved tensor]
padding_idx       :           1024
scale_grad_by_freq:          False
sparse            :          False
weight_argsize_0  :           1025"]
	140597366563840 -> 140597366563744
	140597876323328 [label="ct.decoder.prediction.embed.weight
 (1025, 640)" fillcolor=lightblue]
	140597876323328 -> 140597366563840
	140597366563840 [label=AccumulateGrad]
	140597366563312 -> 140597366563264
	140597478144496 [label="ct.decoder.prediction.dec_rnn.lstm.weight_ih_l0
 (2560, 640)" fillcolor=lightblue]
	140597478144496 -> 140597366563312
	140597366563312 [label=AccumulateGrad]
	140597366562976 -> 140597366563264
	140597478075504 [label="ct.decoder.prediction.dec_rnn.lstm.weight_hh_l0
 (2560, 640)" fillcolor=lightblue]
	140597478075504 -> 140597366562976
	140597366562976 [label=AccumulateGrad]
	140597366563408 -> 140597366563264
	140597478075584 [label="ct.decoder.prediction.dec_rnn.lstm.bias_ih_l0
 (2560)" fillcolor=lightblue]
	140597478075584 -> 140597366563408
	140597366563408 [label=AccumulateGrad]
	140597366563456 -> 140597366563264
	140597478075664 [label="ct.decoder.prediction.dec_rnn.lstm.bias_hh_l0
 (2560)" fillcolor=lightblue]
	140597478075664 -> 140597366563456
	140597366563456 [label=AccumulateGrad]
	140597366407232 -> 140597366408672
	140597366407232 [label=TBackward0]
	140597366475168 -> 140597366407232
	140597390575952 [label="ct.joint.pred.weight
 (640, 640)" fillcolor=lightblue]
	140597390575952 -> 140597366475168
	140597366475168 [label=AccumulateGrad]
	140597480504672 -> 140597480504048
	140597480504672 [label=TBackward0]
	140597366880864 -> 140597480504672
	140597390576272 [label="ct.joint.joint_net.2.weight
 (1025, 640)" fillcolor=lightblue]
	140597390576272 -> 140597366880864
	140597366880864 [label=AccumulateGrad]
	140597480504528 -> 140597480504240
	140597390576352 [label="ct.joint.joint_net.2.bias
 (1025)" fillcolor=lightblue]
	140597390576352 -> 140597480504528
	140597480504528 [label=AccumulateGrad]
	140597480504432 -> 140597480547008
}
