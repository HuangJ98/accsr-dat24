trainer:
  max_epochs: 2
  accumulate_grad_batches: 1
  limit_train_batches: 1.0 # how much of training dataset to use (pctg as float or n_batches as int)
  limit_val_batches: 50 # how much of validation dataset to use (pctg as float or n_batches as int)

optim: # adamw optimizer
  lr: 1e-05 # maximal learning rate
  eps: 1e-08
  weight_decay: 1e-02
  ac_args:
    lr: 1e-02
  sched:
    warmup_steps: 500
    max_steps: true # computed at runtime
